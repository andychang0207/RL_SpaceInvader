{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2de45b",
   "metadata": {},
   "source": [
    "## Package\n",
    "匯入所有所需套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c8e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d431178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9029ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d14338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357b3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b84d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635979f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f61345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d300e3",
   "metadata": {},
   "source": [
    "## Useful function\n",
    "用來呈現遊玩畫面<br>如果是用伺服器跑，env.render無法正常執行，利用 matplotlib渲染圖片在jupyter中，並用ipython的display.clear_output將舊的圖片刪掉，形成動圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f7e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(obs, step=0, info=\"\"):\n",
    "    \"\"\"\n",
    "    show real time observation/state\n",
    "    \"\"\"\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(obs)\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816daf3",
   "metadata": {},
   "source": [
    "## DQN + CNN\n",
    "DQN架構如下\n",
    "![title](static/DQN.png)\n",
    "進入DQN前用CNN將高維度的圖片降維"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13005347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    DQN model with CNN layer processing image state\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim, in_channel=1):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channel, 32, 8, 4)\n",
    "        self.conv_2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv_3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.conv_1(state))\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016d605",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "**Using numpy reduce memory consumption and improve speed performance**<br>\n",
    "將玩過的經驗存入 replay buffer，訓練時從其中隨機挑取，以打破資料時間上的correlation<br>\n",
    "實作上用numpy array取代list以減少memory負擔和加速執行速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd1c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Buffer to store Experience Replay\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=75000):\n",
    "        self.state_mem = np.zeros((max_size,4,84,84))\n",
    "        self.next_state_mem = np.zeros((max_size,4,84,84))\n",
    "        self.action_mem = np.zeros((max_size,))\n",
    "        self.reward_mem = np.zeros((max_size,))\n",
    "        self.done_mem = np.zeros((max_size,))\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.current_size = 0\n",
    "    def __len__(self):\n",
    "        return self.current_size\n",
    "    def add(self,transition):\n",
    "        state, next_state, action, reward, done = transition\n",
    "        self.state_mem[self.ptr] = state\n",
    "        self.next_state_mem[self.ptr] = next_state\n",
    "        self.action_mem[self.ptr] = action\n",
    "        self.reward_mem[self.ptr] = reward\n",
    "        self.done_mem[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.current_size = self.current_size + 1 if self.current_size < self.max_size else self.max_size\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.current_size, size=batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = self.state_mem[ind], self.next_state_mem[ind], self.action_mem[ind], self.reward_mem[ind], self.done_mem[ind]\n",
    "\n",
    "        return batch_states, batch_next_states, batch_actions.reshape(-1, 1), batch_rewards.reshape(-1, 1), batch_dones.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96a623",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "參考paper，將圖片從(210,160,3) 轉成(84,84)，並對值進行正規化<br>\n",
    "實作方法參考https://skywalker0803r.medium.com/%E8%A8%93%E7%B7%B4dqn%E7%8E%A9atari-space-invaders-9bc0fc264f5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78851be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    \"\"\"\n",
    "    preprocess state from rgb image (210,160,3) to gray image (84,84) with normalization \n",
    "    \"\"\"\n",
    "    gray = rgb2gray(state)\n",
    "    cropped_image = gray[8:-12, 4:-12]\n",
    "    normalized_image = cropped_image / 255.0\n",
    "    processed_image = transform.resize(normalized_image, [84,84])\n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c05b4",
   "metadata": {},
   "source": [
    "## Agent\n",
    "agent功能:\n",
    "- 做動作\n",
    "- 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786fb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    RL agent contain one DQN network and one DQN target network\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim, in_channel, lr, device):\n",
    "        self.DQN_model = DQN(action_dim, in_channel).to(device)\n",
    "        self.DQN_target = DQN(action_dim, in_channel).to(device)\n",
    "        self.DQN_target.load_state_dict(self.DQN_model.state_dict())\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(params=self.DQN_model.parameters(),lr=lr)\n",
    "        self.device = device\n",
    "        self.learning_counter = 0\n",
    "        self.debug = []\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        select action while playing\n",
    "        use epsilon-greedy algo. (outside)\n",
    "        \"\"\"\n",
    "        # state -> np.array(4 ,84, 84), x -> Tensor(1, 4, 84, 84)\n",
    "        x = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Q_value -> tensor(1, action_dim)\n",
    "        Q_value = self.DQN_model(x)\n",
    "        \n",
    "        # action -> int\n",
    "        action = torch.max(Q_value, 1)[1].data.cpu().numpy()[0]\n",
    "        \n",
    "        # Q_value -> float\n",
    "        #Q_max = torch.max(Q_value, 1)[0].data.cpu().numpy()[0]\n",
    "        \n",
    "        return action, Q_value.cpu().detach().numpy()\n",
    "    \n",
    "    def train(self, replaybuffer, batch_size=100, discount=0.99, policy_freq=2):\n",
    "        \"\"\"\n",
    "        training a batch\n",
    "        replaybuffer -> object: containing experience\n",
    "        iteration -> int: current iteration\n",
    "        batch_size -> int: batch size, default 100\n",
    "        discount -> float: discount factor - gamma, default 0.99\n",
    "        policy_freq -> int: every policy_freq iterations update target network once, default 2\n",
    "        \"\"\"\n",
    "        # sample batch from experience replay\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replaybuffer.sample(batch_size)\n",
    "        \n",
    "        # batch_states -> np.array(batch, 4, 84, 84), states -> Tensor(batch, 4, 84, 84)\n",
    "        states = torch.Tensor(batch_states).to(self.device)\n",
    "        \n",
    "        # batch_next_states -> np.array(batch, 4, 84, 84), next_states -> Tensor(batch, 4, 84, 84)\n",
    "        next_states = torch.Tensor(batch_next_states).to(self.device)\n",
    "        \n",
    "        # batch_actions -> np.array(batch, 1), actions -> LongTensor(batch, 1), index must be int64\n",
    "        actions = torch.LongTensor(batch_actions).to(self.device)\n",
    "        \n",
    "        # batch_rewards -> np.array(batch, 1), rewards -> Tensor(batch, 1)\n",
    "        rewards = torch.Tensor(batch_rewards).to(self.device)\n",
    "        \n",
    "        # batch_dones -> np.array(batch, 1), dones -> Tensor(batch, 1)\n",
    "        dones = torch.Tensor(batch_dones).to(self.device)\n",
    "        \n",
    "        # next_states -> Tensor(batch, 4, 84, 84), Q_next -> Tensor(batch, action_dim)\n",
    "        # detach target to prevent gradient descent on target network\n",
    "        Q_next = self.DQN_target(next_states).detach()\n",
    "        \n",
    "        # self.DQN_model(states) -> Tensor(batch, action_dim), Q_now -> Tensor(batch, 1)\n",
    "        Q_now = self.DQN_model(states).gather(-1, actions)\n",
    "        \n",
    "        # Q(st, at) = rt + gamma * max_a': Q'(st+1, a') rewards 要加 s，幹 = =\n",
    "        Q_target = rewards +  discount * Q_next.max(-1)[0].unsqueeze(-1) #(1 - dones) *\n",
    "        loss = self.loss_func(Q_now, Q_target)\n",
    "#         self.debug.append(loss.item())\n",
    "#         a = list(self.DQN_model.parameters())[0].clone()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "#         b = list(self.DQN_model.parameters())[0].clone()\n",
    "#        self.debug.append(torch.equal(a.data, b.data))\n",
    "        \n",
    "        self.learning_counter += 1\n",
    "        if self.learning_counter % policy_freq == 0:\n",
    "            # update target network\n",
    "            self.DQN_target.load_state_dict(self.DQN_model.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.DQN_model.state_dict(), '%s/%s_DQN_CNN.pth' % (directory, filename))\n",
    "  \n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.DQN_model.load_state_dict(torch.load('%s/%s_DQN_CNN.pth' % (directory, filename)))\n",
    "        self.DQN_target.load_state_dict(self.DQN_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9a9ef",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "超參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7be9fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'SpaceInvaders-v0'\n",
    "seed = 0\n",
    "eval_freq = 5 # 5e3\n",
    "lr = 0.0002\n",
    "max_episode = 100 # 5e5\n",
    "max_timestep = 1e6\n",
    "isuseepisode = False\n",
    "save_models = True\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "policy_freq = 1e5\n",
    "memory_capacity = 5e4 # 2e3\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "decay_rate = 0.00001\n",
    "perform_clip = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c350b",
   "metadata": {},
   "source": [
    "## Env setup\n",
    "設定環境參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29271cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: DQN_SpaceInvaders-v0_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "filename = \"%s_%s_%s\" % (\"DQN\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (filename))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2055b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"./{filename}_results\"):\n",
    "    os.makedirs(f\"./{filename}_results\")\n",
    "if save_models and not os.path.exists(f\"./{filename}_pytorch_models\"):\n",
    "    os.makedirs(f\"./{filename}_pytorch_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0c08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f0b6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "080afc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de4b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim, 4, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da098f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaybuffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d6c68",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "驗證環節"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b4cb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, eval_episodes=10):\n",
    "    avg_reward = 0.\n",
    "    avg_Q_value = 0\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        state = deque([np.zeros((84, 84)) for i in range(4)],maxlen=4)\n",
    "        state.append(preprocess(obs))\n",
    "        for _ in range(3):\n",
    "            obs, _, _, _ = env.step(0)\n",
    "            state.append(preprocess(obs))\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            t += 1\n",
    "            action, Q_predict = agent.select_action(np.array(state))\n",
    "            avg_Q_value += Q_predict.mean()\n",
    "            for _ in range(4):\n",
    "                obs, reward, done, _ = env.step(action)\n",
    "                state.append(preprocess(obs))\n",
    "                if done: break\n",
    "            avg_reward += reward\n",
    "        avg_Q_value /= t\n",
    "    avg_reward /= eval_episodes\n",
    "    avg_Q_value /= eval_episodes\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward, avg_Q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae591d89",
   "metadata": {},
   "source": [
    "## Initializing variables\n",
    "初始化變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d03a55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_episode = 0\n",
    "current_timestep = 0\n",
    "total_loss = []\n",
    "reward_record = []\n",
    "eval_reward_record = []\n",
    "eval_Q_value_record = []\n",
    "training_epsilon = epsilon_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3ec56",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c9463",
   "metadata": {},
   "source": [
    "## Frame skipping\n",
    "訓練過程加入 frame skipping技術"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a28e2411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating Experience Play..../replaybuffer:211\n",
      "Accumulating Experience Play..../replaybuffer:371\n",
      "Accumulating Experience Play..../replaybuffer:470\n",
      "Accumulating Experience Play..../replaybuffer:644\n",
      "Accumulating Experience Play..../replaybuffer:766\n",
      "Accumulating Experience Play..../replaybuffer:920\n",
      "Accumulating Experience Play..../replaybuffer:1126\n",
      "Accumulating Experience Play..../replaybuffer:1392\n",
      "Accumulating Experience Play..../replaybuffer:1691\n",
      "Accumulating Experience Play..../replaybuffer:1840\n",
      "Accumulating Experience Play..../replaybuffer:2000\n",
      "Accumulating Experience Play..../replaybuffer:2171\n",
      "Accumulating Experience Play..../replaybuffer:2327\n",
      "Accumulating Experience Play..../replaybuffer:2456\n",
      "Accumulating Experience Play..../replaybuffer:2704\n",
      "Accumulating Experience Play..../replaybuffer:2840\n",
      "Accumulating Experience Play..../replaybuffer:2937\n",
      "Accumulating Experience Play..../replaybuffer:3050\n",
      "Accumulating Experience Play..../replaybuffer:3222\n",
      "Accumulating Experience Play..../replaybuffer:3492\n",
      "Accumulating Experience Play..../replaybuffer:3613\n",
      "Accumulating Experience Play..../replaybuffer:3773\n",
      "Accumulating Experience Play..../replaybuffer:4069\n",
      "Accumulating Experience Play..../replaybuffer:4244\n",
      "Accumulating Experience Play..../replaybuffer:4507\n",
      "Accumulating Experience Play..../replaybuffer:4675\n",
      "Accumulating Experience Play..../replaybuffer:4804\n",
      "Accumulating Experience Play..../replaybuffer:4964\n",
      "Accumulating Experience Play..../replaybuffer:5097\n",
      "Accumulating Experience Play..../replaybuffer:5268\n",
      "Accumulating Experience Play..../replaybuffer:5395\n",
      "Accumulating Experience Play..../replaybuffer:5609\n",
      "Accumulating Experience Play..../replaybuffer:5843\n",
      "Accumulating Experience Play..../replaybuffer:6122\n",
      "Accumulating Experience Play..../replaybuffer:6289\n",
      "Accumulating Experience Play..../replaybuffer:6506\n",
      "Accumulating Experience Play..../replaybuffer:6749\n",
      "Accumulating Experience Play..../replaybuffer:7021\n",
      "Accumulating Experience Play..../replaybuffer:7179\n",
      "Accumulating Experience Play..../replaybuffer:7336\n",
      "Accumulating Experience Play..../replaybuffer:7513\n",
      "Accumulating Experience Play..../replaybuffer:7660\n",
      "Accumulating Experience Play..../replaybuffer:7766\n",
      "Accumulating Experience Play..../replaybuffer:7941\n",
      "Accumulating Experience Play..../replaybuffer:8120\n",
      "Accumulating Experience Play..../replaybuffer:8329\n",
      "Accumulating Experience Play..../replaybuffer:8414\n",
      "Accumulating Experience Play..../replaybuffer:8585\n",
      "Accumulating Experience Play..../replaybuffer:8705\n",
      "Accumulating Experience Play..../replaybuffer:8834\n",
      "Accumulating Experience Play..../replaybuffer:9092\n",
      "Accumulating Experience Play..../replaybuffer:9260\n",
      "Accumulating Experience Play..../replaybuffer:9431\n",
      "Accumulating Experience Play..../replaybuffer:9737\n",
      "Accumulating Experience Play..../replaybuffer:9892\n",
      "Accumulating Experience Play..../replaybuffer:9975\n",
      "Accumulating Experience Play..../replaybuffer:10084\n",
      "Accumulating Experience Play..../replaybuffer:10304\n",
      "Accumulating Experience Play..../replaybuffer:10527\n",
      "Accumulating Experience Play..../replaybuffer:10623\n",
      "Accumulating Experience Play..../replaybuffer:10717\n",
      "Accumulating Experience Play..../replaybuffer:10896\n",
      "Accumulating Experience Play..../replaybuffer:11069\n",
      "Accumulating Experience Play..../replaybuffer:11301\n",
      "Accumulating Experience Play..../replaybuffer:11460\n",
      "Accumulating Experience Play..../replaybuffer:11768\n",
      "Accumulating Experience Play..../replaybuffer:12006\n",
      "Accumulating Experience Play..../replaybuffer:12176\n",
      "Accumulating Experience Play..../replaybuffer:12307\n",
      "Accumulating Experience Play..../replaybuffer:12471\n",
      "Accumulating Experience Play..../replaybuffer:12873\n",
      "Accumulating Experience Play..../replaybuffer:13036\n",
      "Accumulating Experience Play..../replaybuffer:13178\n",
      "Accumulating Experience Play..../replaybuffer:13369\n",
      "Accumulating Experience Play..../replaybuffer:13622\n",
      "Accumulating Experience Play..../replaybuffer:13796\n",
      "Accumulating Experience Play..../replaybuffer:14046\n",
      "Accumulating Experience Play..../replaybuffer:14274\n",
      "Accumulating Experience Play..../replaybuffer:14394\n",
      "Accumulating Experience Play..../replaybuffer:14638\n",
      "Accumulating Experience Play..../replaybuffer:14806\n",
      "Accumulating Experience Play..../replaybuffer:14933\n",
      "Accumulating Experience Play..../replaybuffer:15204\n",
      "Accumulating Experience Play..../replaybuffer:15325\n",
      "Accumulating Experience Play..../replaybuffer:15490\n",
      "Accumulating Experience Play..../replaybuffer:15630\n",
      "Accumulating Experience Play..../replaybuffer:15722\n",
      "Accumulating Experience Play..../replaybuffer:15883\n",
      "Accumulating Experience Play..../replaybuffer:16134\n",
      "Accumulating Experience Play..../replaybuffer:16305\n",
      "Accumulating Experience Play..../replaybuffer:16502\n",
      "Accumulating Experience Play..../replaybuffer:16691\n",
      "Accumulating Experience Play..../replaybuffer:16969\n",
      "Accumulating Experience Play..../replaybuffer:17068\n",
      "Accumulating Experience Play..../replaybuffer:17229\n",
      "Accumulating Experience Play..../replaybuffer:17474\n",
      "Accumulating Experience Play..../replaybuffer:17628\n",
      "Accumulating Experience Play..../replaybuffer:17934\n",
      "Accumulating Experience Play..../replaybuffer:18072\n",
      "Accumulating Experience Play..../replaybuffer:18308\n",
      "Accumulating Experience Play..../replaybuffer:18480\n",
      "Accumulating Experience Play..../replaybuffer:18594\n",
      "Accumulating Experience Play..../replaybuffer:18759\n",
      "Accumulating Experience Play..../replaybuffer:18892\n",
      "Accumulating Experience Play..../replaybuffer:19051\n",
      "Accumulating Experience Play..../replaybuffer:19177\n",
      "Accumulating Experience Play..../replaybuffer:19355\n",
      "Accumulating Experience Play..../replaybuffer:19617\n",
      "Accumulating Experience Play..../replaybuffer:19712\n",
      "Accumulating Experience Play..../replaybuffer:19873\n",
      "Accumulating Experience Play..../replaybuffer:19970\n",
      "Accumulating Experience Play..../replaybuffer:20251\n",
      "Accumulating Experience Play..../replaybuffer:20458\n",
      "Accumulating Experience Play..../replaybuffer:20587\n",
      "Accumulating Experience Play..../replaybuffer:20791\n",
      "Accumulating Experience Play..../replaybuffer:20956\n",
      "Accumulating Experience Play..../replaybuffer:21122\n",
      "Accumulating Experience Play..../replaybuffer:21374\n",
      "Accumulating Experience Play..../replaybuffer:21548\n",
      "Accumulating Experience Play..../replaybuffer:21709\n",
      "Accumulating Experience Play..../replaybuffer:21896\n",
      "Accumulating Experience Play..../replaybuffer:22131\n",
      "Accumulating Experience Play..../replaybuffer:22377\n",
      "Accumulating Experience Play..../replaybuffer:22471\n",
      "Accumulating Experience Play..../replaybuffer:22600\n",
      "Accumulating Experience Play..../replaybuffer:22704\n",
      "Accumulating Experience Play..../replaybuffer:22948\n",
      "Accumulating Experience Play..../replaybuffer:23105\n",
      "Accumulating Experience Play..../replaybuffer:23197\n",
      "Accumulating Experience Play..../replaybuffer:23381\n",
      "Accumulating Experience Play..../replaybuffer:23604\n",
      "Accumulating Experience Play..../replaybuffer:23774\n",
      "Accumulating Experience Play..../replaybuffer:24030\n",
      "Accumulating Experience Play..../replaybuffer:24177\n",
      "Accumulating Experience Play..../replaybuffer:24400\n",
      "Accumulating Experience Play..../replaybuffer:24690\n",
      "Accumulating Experience Play..../replaybuffer:24789\n",
      "Accumulating Experience Play..../replaybuffer:24972\n",
      "Accumulating Experience Play..../replaybuffer:25177\n",
      "Accumulating Experience Play..../replaybuffer:25269\n",
      "Accumulating Experience Play..../replaybuffer:25434\n",
      "Accumulating Experience Play..../replaybuffer:25541\n",
      "Accumulating Experience Play..../replaybuffer:25820\n",
      "Accumulating Experience Play..../replaybuffer:26145\n",
      "Accumulating Experience Play..../replaybuffer:26491\n",
      "Accumulating Experience Play..../replaybuffer:26608\n",
      "Accumulating Experience Play..../replaybuffer:26770\n",
      "Accumulating Experience Play..../replaybuffer:26997\n",
      "Accumulating Experience Play..../replaybuffer:27201\n",
      "Accumulating Experience Play..../replaybuffer:27378\n",
      "Accumulating Experience Play..../replaybuffer:27616\n",
      "Accumulating Experience Play..../replaybuffer:27732\n",
      "Accumulating Experience Play..../replaybuffer:27817\n",
      "Accumulating Experience Play..../replaybuffer:28013\n",
      "Accumulating Experience Play..../replaybuffer:28169\n",
      "Accumulating Experience Play..../replaybuffer:28360\n",
      "Accumulating Experience Play..../replaybuffer:28446\n",
      "Accumulating Experience Play..../replaybuffer:28605\n",
      "Accumulating Experience Play..../replaybuffer:28761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating Experience Play..../replaybuffer:29051\n",
      "Accumulating Experience Play..../replaybuffer:29277\n",
      "Accumulating Experience Play..../replaybuffer:29439\n",
      "Accumulating Experience Play..../replaybuffer:29717\n",
      "Accumulating Experience Play..../replaybuffer:29872\n",
      "Accumulating Experience Play..../replaybuffer:30010\n",
      "Accumulating Experience Play..../replaybuffer:30191\n",
      "Accumulating Experience Play..../replaybuffer:30352\n",
      "Accumulating Experience Play..../replaybuffer:30508\n",
      "Accumulating Experience Play..../replaybuffer:30660\n",
      "Accumulating Experience Play..../replaybuffer:30824\n",
      "Accumulating Experience Play..../replaybuffer:30921\n",
      "Accumulating Experience Play..../replaybuffer:31110\n",
      "Accumulating Experience Play..../replaybuffer:31206\n",
      "Accumulating Experience Play..../replaybuffer:31375\n",
      "Accumulating Experience Play..../replaybuffer:31557\n",
      "Accumulating Experience Play..../replaybuffer:31704\n",
      "Accumulating Experience Play..../replaybuffer:31848\n",
      "Accumulating Experience Play..../replaybuffer:32009\n",
      "Accumulating Experience Play..../replaybuffer:32158\n",
      "Accumulating Experience Play..../replaybuffer:32287\n",
      "Accumulating Experience Play..../replaybuffer:32422\n",
      "Accumulating Experience Play..../replaybuffer:32522\n",
      "Accumulating Experience Play..../replaybuffer:32733\n",
      "Accumulating Experience Play..../replaybuffer:32874\n",
      "Accumulating Experience Play..../replaybuffer:33124\n",
      "Accumulating Experience Play..../replaybuffer:33236\n",
      "Accumulating Experience Play..../replaybuffer:33380\n",
      "Accumulating Experience Play..../replaybuffer:33466\n",
      "Accumulating Experience Play..../replaybuffer:33557\n",
      "Accumulating Experience Play..../replaybuffer:33641\n",
      "Accumulating Experience Play..../replaybuffer:33929\n",
      "Accumulating Experience Play..../replaybuffer:34088\n",
      "Accumulating Experience Play..../replaybuffer:34338\n",
      "Accumulating Experience Play..../replaybuffer:34512\n",
      "Accumulating Experience Play..../replaybuffer:34673\n",
      "Accumulating Experience Play..../replaybuffer:34807\n",
      "Accumulating Experience Play..../replaybuffer:34965\n",
      "Accumulating Experience Play..../replaybuffer:35130\n",
      "Accumulating Experience Play..../replaybuffer:35329\n",
      "Accumulating Experience Play..../replaybuffer:35475\n",
      "Accumulating Experience Play..../replaybuffer:35640\n",
      "Accumulating Experience Play..../replaybuffer:35766\n",
      "Accumulating Experience Play..../replaybuffer:35904\n",
      "Accumulating Experience Play..../replaybuffer:36105\n",
      "Accumulating Experience Play..../replaybuffer:36228\n",
      "Accumulating Experience Play..../replaybuffer:36455\n",
      "Accumulating Experience Play..../replaybuffer:36739\n",
      "Accumulating Experience Play..../replaybuffer:36919\n",
      "Accumulating Experience Play..../replaybuffer:37121\n",
      "Accumulating Experience Play..../replaybuffer:37263\n",
      "Accumulating Experience Play..../replaybuffer:37384\n",
      "Accumulating Experience Play..../replaybuffer:37539\n",
      "Accumulating Experience Play..../replaybuffer:37710\n",
      "Accumulating Experience Play..../replaybuffer:37925\n",
      "Accumulating Experience Play..../replaybuffer:38056\n",
      "Accumulating Experience Play..../replaybuffer:38163\n",
      "Accumulating Experience Play..../replaybuffer:38258\n",
      "Accumulating Experience Play..../replaybuffer:38422\n",
      "Accumulating Experience Play..../replaybuffer:38683\n",
      "Accumulating Experience Play..../replaybuffer:39024\n",
      "Accumulating Experience Play..../replaybuffer:39184\n",
      "Accumulating Experience Play..../replaybuffer:39413\n",
      "Accumulating Experience Play..../replaybuffer:39623\n",
      "Accumulating Experience Play..../replaybuffer:39724\n",
      "Accumulating Experience Play..../replaybuffer:39983\n",
      "Accumulating Experience Play..../replaybuffer:40134\n",
      "Accumulating Experience Play..../replaybuffer:40264\n",
      "Accumulating Experience Play..../replaybuffer:40524\n",
      "Accumulating Experience Play..../replaybuffer:40693\n",
      "Accumulating Experience Play..../replaybuffer:40814\n",
      "Accumulating Experience Play..../replaybuffer:41005\n",
      "Accumulating Experience Play..../replaybuffer:41180\n",
      "Accumulating Experience Play..../replaybuffer:41309\n",
      "Accumulating Experience Play..../replaybuffer:41423\n",
      "Accumulating Experience Play..../replaybuffer:41626\n",
      "Accumulating Experience Play..../replaybuffer:41890\n",
      "Accumulating Experience Play..../replaybuffer:42019\n",
      "Accumulating Experience Play..../replaybuffer:42143\n",
      "Accumulating Experience Play..../replaybuffer:42297\n",
      "Accumulating Experience Play..../replaybuffer:42380\n",
      "Accumulating Experience Play..../replaybuffer:42694\n",
      "Accumulating Experience Play..../replaybuffer:42895\n",
      "Accumulating Experience Play..../replaybuffer:43093\n",
      "Accumulating Experience Play..../replaybuffer:43242\n",
      "Accumulating Experience Play..../replaybuffer:43424\n",
      "Accumulating Experience Play..../replaybuffer:43683\n",
      "Accumulating Experience Play..../replaybuffer:43856\n",
      "Accumulating Experience Play..../replaybuffer:44084\n",
      "Accumulating Experience Play..../replaybuffer:44195\n",
      "Accumulating Experience Play..../replaybuffer:44327\n",
      "Accumulating Experience Play..../replaybuffer:44532\n",
      "Accumulating Experience Play..../replaybuffer:44720\n",
      "Accumulating Experience Play..../replaybuffer:44962\n",
      "Accumulating Experience Play..../replaybuffer:45121\n",
      "Accumulating Experience Play..../replaybuffer:45378\n",
      "Accumulating Experience Play..../replaybuffer:45675\n",
      "Accumulating Experience Play..../replaybuffer:45830\n",
      "Accumulating Experience Play..../replaybuffer:46001\n",
      "Accumulating Experience Play..../replaybuffer:46212\n",
      "Accumulating Experience Play..../replaybuffer:46305\n",
      "Accumulating Experience Play..../replaybuffer:46437\n",
      "Accumulating Experience Play..../replaybuffer:46569\n",
      "Accumulating Experience Play..../replaybuffer:46681\n",
      "Accumulating Experience Play..../replaybuffer:46814\n",
      "Accumulating Experience Play..../replaybuffer:46948\n",
      "Accumulating Experience Play..../replaybuffer:47211\n",
      "Accumulating Experience Play..../replaybuffer:47380\n",
      "Accumulating Experience Play..../replaybuffer:47584\n",
      "Accumulating Experience Play..../replaybuffer:47794\n",
      "Accumulating Experience Play..../replaybuffer:48006\n",
      "Accumulating Experience Play..../replaybuffer:48152\n",
      "Accumulating Experience Play..../replaybuffer:48358\n",
      "Accumulating Experience Play..../replaybuffer:48513\n",
      "Accumulating Experience Play..../replaybuffer:48606\n",
      "Accumulating Experience Play..../replaybuffer:48769\n",
      "Accumulating Experience Play..../replaybuffer:48905\n",
      "Accumulating Experience Play..../replaybuffer:49050\n",
      "Accumulating Experience Play..../replaybuffer:49330\n",
      "Accumulating Experience Play..../replaybuffer:49522\n",
      "Accumulating Experience Play..../replaybuffer:49616\n",
      "Accumulating Experience Play..../replaybuffer:49748\n",
      "Accumulating Experience Play..../replaybuffer:49873\n",
      "Episode 1 finished after 141 timesteps, total rewards 1.0, mean loss 0.01947378277157744\n",
      "Episode 2 finished after 155 timesteps, total rewards 1.0, mean loss 0.011684714770020829\n",
      "Episode 3 finished after 141 timesteps, total rewards 1.0, mean loss 0.01388488548090731\n",
      "Episode 4 finished after 121 timesteps, total rewards 2.0, mean loss 0.01312433295369231\n",
      "Episode 5 finished after 100 timesteps, total rewards 2.0, mean loss 0.015486104826704832\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 6 finished after 127 timesteps, total rewards 1.0, mean loss 0.015944170082168672\n",
      "Episode 7 finished after 159 timesteps, total rewards 1.0, mean loss 0.010365043822830615\n",
      "Episode 8 finished after 256 timesteps, total rewards 2.0, mean loss 0.012964284094309164\n",
      "Episode 9 finished after 221 timesteps, total rewards 3.0, mean loss 0.014683434625384182\n",
      "Episode 10 finished after 146 timesteps, total rewards 2.0, mean loss 0.016888236712998463\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 11 finished after 223 timesteps, total rewards 6.0, mean loss 0.012495135547156156\n",
      "Episode 12 finished after 197 timesteps, total rewards 4.0, mean loss 0.014413900737100232\n",
      "Episode 13 finished after 157 timesteps, total rewards 0.0, mean loss 0.014754845012914743\n",
      "Episode 14 finished after 148 timesteps, total rewards 3.0, mean loss 0.012560753533565533\n",
      "Episode 15 finished after 151 timesteps, total rewards 1.0, mean loss 0.016383383191560335\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 16 finished after 189 timesteps, total rewards 1.0, mean loss 0.013722052589136178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17 finished after 221 timesteps, total rewards 3.0, mean loss 0.013713543048388972\n",
      "Episode 18 finished after 175 timesteps, total rewards 1.0, mean loss 0.013942189172459101\n",
      "Episode 19 finished after 93 timesteps, total rewards 0.0, mean loss 0.01686357809384028\n",
      "Episode 20 finished after 97 timesteps, total rewards 0.0, mean loss 0.017496297816888755\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 21 finished after 164 timesteps, total rewards 3.0, mean loss 0.012071086664413665\n",
      "Episode 22 finished after 182 timesteps, total rewards 5.0, mean loss 0.014927955747295458\n",
      "Episode 23 finished after 249 timesteps, total rewards 4.0, mean loss 0.013279151454463066\n",
      "Episode 24 finished after 207 timesteps, total rewards 1.0, mean loss 0.013857032803949058\n",
      "Episode 25 finished after 168 timesteps, total rewards 3.0, mean loss 0.015410252781770222\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 26 finished after 95 timesteps, total rewards 0.0, mean loss 0.014308174501255915\n",
      "Episode 27 finished after 204 timesteps, total rewards 4.0, mean loss 0.012123244439434903\n",
      "Episode 28 finished after 246 timesteps, total rewards 1.0, mean loss 0.014171348010699504\n",
      "Episode 29 finished after 179 timesteps, total rewards 2.0, mean loss 0.012953083722136119\n",
      "Episode 30 finished after 145 timesteps, total rewards 3.0, mean loss 0.012370299233990753\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 31 finished after 179 timesteps, total rewards 3.0, mean loss 0.015328145357635452\n",
      "Episode 32 finished after 93 timesteps, total rewards 1.0, mean loss 0.011318144059279353\n",
      "Episode 33 finished after 134 timesteps, total rewards 1.0, mean loss 0.016088445811154956\n",
      "Episode 34 finished after 99 timesteps, total rewards 1.0, mean loss 0.010032657749774205\n",
      "Episode 35 finished after 88 timesteps, total rewards 1.0, mean loss 0.011260970035188868\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 36 finished after 178 timesteps, total rewards 3.0, mean loss 0.01268202008573558\n",
      "Episode 37 finished after 328 timesteps, total rewards 7.0, mean loss 0.01438856471070433\n",
      "Episode 38 finished after 160 timesteps, total rewards 4.0, mean loss 0.013691234006728337\n",
      "Episode 39 finished after 173 timesteps, total rewards 1.0, mean loss 0.014791462480173328\n",
      "Episode 40 finished after 153 timesteps, total rewards 4.0, mean loss 0.01650856340568805\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 41 finished after 201 timesteps, total rewards 5.0, mean loss 0.012627593720940217\n",
      "Episode 42 finished after 205 timesteps, total rewards 7.0, mean loss 0.012670881008351698\n",
      "Episode 43 finished after 174 timesteps, total rewards 4.0, mean loss 0.015401708477953831\n",
      "Episode 44 finished after 125 timesteps, total rewards 1.0, mean loss 0.010180595544632524\n",
      "Episode 45 finished after 178 timesteps, total rewards 3.0, mean loss 0.013521719103276186\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 46 finished after 128 timesteps, total rewards 1.0, mean loss 0.013998455827504586\n",
      "Episode 47 finished after 114 timesteps, total rewards 3.0, mean loss 0.01672899525657525\n",
      "Episode 48 finished after 110 timesteps, total rewards 1.0, mean loss 0.013741737219904529\n",
      "Episode 49 finished after 240 timesteps, total rewards 1.0, mean loss 0.01437408454354833\n",
      "Episode 50 finished after 96 timesteps, total rewards 1.0, mean loss 0.01094232769211582\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 51 finished after 158 timesteps, total rewards 1.0, mean loss 0.011914956378019223\n",
      "Episode 52 finished after 256 timesteps, total rewards 4.0, mean loss 0.012439632411542334\n",
      "Episode 53 finished after 245 timesteps, total rewards 3.0, mean loss 0.013346346446558861\n",
      "Episode 54 finished after 160 timesteps, total rewards 0.0, mean loss 0.01368006384191176\n",
      "Episode 55 finished after 119 timesteps, total rewards 6.0, mean loss 0.014237688405000498\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 56 finished after 93 timesteps, total rewards 1.0, mean loss 0.009322869221984048\n",
      "Episode 57 finished after 135 timesteps, total rewards 1.0, mean loss 0.017308549478938975\n",
      "Episode 58 finished after 237 timesteps, total rewards 3.0, mean loss 0.012993858162009644\n",
      "Episode 59 finished after 213 timesteps, total rewards 5.0, mean loss 0.015230666271407057\n",
      "Episode 60 finished after 170 timesteps, total rewards 1.0, mean loss 0.0138084486764188\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 5.000000\n",
      "---------------------------------------\n",
      "Episode 61 finished after 290 timesteps, total rewards 1.0, mean loss 0.012414721225725551\n",
      "Episode 62 finished after 153 timesteps, total rewards 1.0, mean loss 0.014621560262467444\n",
      "Episode 63 finished after 125 timesteps, total rewards 1.0, mean loss 0.010419787594117224\n",
      "Episode 64 finished after 157 timesteps, total rewards 1.0, mean loss 0.013506079455909904\n",
      "Episode 65 finished after 105 timesteps, total rewards 1.0, mean loss 0.013982934213778381\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 66 finished after 175 timesteps, total rewards 2.0, mean loss 0.01097467810821919\n",
      "Episode 67 finished after 285 timesteps, total rewards 1.0, mean loss 0.013910358570737706\n",
      "Episode 68 finished after 212 timesteps, total rewards 2.0, mean loss 0.01476907096253373\n",
      "Episode 69 finished after 177 timesteps, total rewards 2.0, mean loss 0.015032338552090792\n",
      "Episode 70 finished after 154 timesteps, total rewards 2.0, mean loss 0.015150071056882848\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 8.333333\n",
      "---------------------------------------\n",
      "Episode 71 finished after 178 timesteps, total rewards 4.0, mean loss 0.015302386789903467\n",
      "Episode 72 finished after 179 timesteps, total rewards 3.0, mean loss 0.011210105468038738\n",
      "Episode 73 finished after 197 timesteps, total rewards 6.0, mean loss 0.01678353836798336\n",
      "Episode 74 finished after 229 timesteps, total rewards 5.0, mean loss 0.014235161913621877\n",
      "Episode 75 finished after 180 timesteps, total rewards 1.0, mean loss 0.013309870484428958\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 76 finished after 98 timesteps, total rewards 0.0, mean loss 0.01685455941169865\n",
      "Episode 77 finished after 150 timesteps, total rewards 0.0, mean loss 0.013523209443277059\n",
      "Episode 78 finished after 162 timesteps, total rewards 1.0, mean loss 0.018221374184958908\n",
      "Episode 79 finished after 92 timesteps, total rewards 0.0, mean loss 0.0182899356438176\n",
      "Episode 80 finished after 130 timesteps, total rewards 0.0, mean loss 0.010671519384329888\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 81 finished after 174 timesteps, total rewards 2.0, mean loss 0.01389655916011242\n",
      "Episode 82 finished after 133 timesteps, total rewards 1.0, mean loss 0.012724600908918922\n",
      "Episode 83 finished after 277 timesteps, total rewards 3.0, mean loss 0.012626905535113038\n",
      "Episode 84 finished after 223 timesteps, total rewards 3.0, mean loss 0.014086683872761414\n",
      "Episode 85 finished after 97 timesteps, total rewards 1.0, mean loss 0.013941823930171146\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 86 finished after 252 timesteps, total rewards 5.0, mean loss 0.011721790412810237\n",
      "Episode 87 finished after 200 timesteps, total rewards 2.0, mean loss 0.013361717725420021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 88 finished after 129 timesteps, total rewards 1.0, mean loss 0.013111599099568756\n",
      "Episode 89 finished after 90 timesteps, total rewards 1.0, mean loss 0.012978244614593374\n",
      "Episode 90 finished after 245 timesteps, total rewards 2.0, mean loss 0.011072146679317503\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 91 finished after 165 timesteps, total rewards 2.0, mean loss 0.013255352105118095\n",
      "Episode 92 finished after 198 timesteps, total rewards 7.0, mean loss 0.012547177491392681\n",
      "Episode 93 finished after 115 timesteps, total rewards 4.0, mean loss 0.011004905531919339\n",
      "Episode 94 finished after 280 timesteps, total rewards 8.0, mean loss 0.012659908271065693\n",
      "Episode 95 finished after 166 timesteps, total rewards 2.0, mean loss 0.01186036773862111\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 83.333333\n",
      "---------------------------------------\n",
      "Episode 96 finished after 266 timesteps, total rewards 5.0, mean loss 0.014776655989134494\n",
      "Episode 97 finished after 153 timesteps, total rewards 3.0, mean loss 0.012444662179488208\n",
      "Episode 98 finished after 126 timesteps, total rewards 2.0, mean loss 0.012443253410599446\n",
      "Episode 99 finished after 230 timesteps, total rewards 2.0, mean loss 0.014426834543258377\n",
      "Episode 100 finished after 180 timesteps, total rewards 6.0, mean loss 0.013678530075736085\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 101 finished after 161 timesteps, total rewards 1.0, mean loss 0.012219572253778333\n",
      "Episode 102 finished after 199 timesteps, total rewards 2.0, mean loss 0.015608379604719315\n",
      "Episode 103 finished after 222 timesteps, total rewards 6.0, mean loss 0.014655478471451698\n",
      "Episode 104 finished after 158 timesteps, total rewards 0.0, mean loss 0.0147419522008586\n",
      "Episode 105 finished after 131 timesteps, total rewards 0.0, mean loss 0.01799233437203429\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 106 finished after 160 timesteps, total rewards 0.0, mean loss 0.014587679996111547\n",
      "Episode 107 finished after 156 timesteps, total rewards 0.0, mean loss 0.015121280531811629\n",
      "Episode 108 finished after 274 timesteps, total rewards 2.0, mean loss 0.013133417488534443\n",
      "Episode 109 finished after 88 timesteps, total rewards 1.0, mean loss 0.010905314138075151\n",
      "Episode 110 finished after 131 timesteps, total rewards 1.0, mean loss 0.015208286372489272\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 111 finished after 100 timesteps, total rewards 1.0, mean loss 0.012918358737952076\n",
      "Episode 112 finished after 263 timesteps, total rewards 3.0, mean loss 0.01445258397718771\n",
      "Episode 113 finished after 162 timesteps, total rewards 3.0, mean loss 0.014401525988500065\n",
      "Episode 114 finished after 246 timesteps, total rewards 7.0, mean loss 0.010891771906614398\n",
      "Episode 115 finished after 141 timesteps, total rewards 1.0, mean loss 0.013246301859031029\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 116 finished after 106 timesteps, total rewards 1.0, mean loss 0.01734738142099473\n",
      "Episode 117 finished after 132 timesteps, total rewards 0.0, mean loss 0.015778297501542096\n",
      "Episode 118 finished after 157 timesteps, total rewards 4.0, mean loss 0.013450174105177919\n",
      "Episode 119 finished after 132 timesteps, total rewards 2.0, mean loss 0.013479416413302357\n",
      "Episode 120 finished after 228 timesteps, total rewards 5.0, mean loss 0.014259288034576968\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 121 finished after 216 timesteps, total rewards 6.0, mean loss 0.014087265062564122\n",
      "Episode 122 finished after 96 timesteps, total rewards 1.0, mean loss 0.013149573216651333\n",
      "Episode 123 finished after 172 timesteps, total rewards 2.0, mean loss 0.016391454192438856\n",
      "Episode 124 finished after 228 timesteps, total rewards 2.0, mean loss 0.013067864124247552\n",
      "Episode 125 finished after 135 timesteps, total rewards 4.0, mean loss 0.013227571125241444\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 126 finished after 104 timesteps, total rewards 2.0, mean loss 0.016414261336859245\n",
      "Episode 127 finished after 147 timesteps, total rewards 2.0, mean loss 0.010740343093944137\n",
      "Episode 128 finished after 234 timesteps, total rewards 0.0, mean loss 0.013408138928480962\n",
      "Episode 129 finished after 160 timesteps, total rewards 1.0, mean loss 0.014753022550303285\n",
      "Episode 130 finished after 138 timesteps, total rewards 1.0, mean loss 0.01249962330151955\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 131 finished after 95 timesteps, total rewards 3.0, mean loss 0.010387881985700101\n",
      "Episode 132 finished after 170 timesteps, total rewards 2.0, mean loss 0.014985170606301194\n",
      "Episode 133 finished after 191 timesteps, total rewards 6.0, mean loss 0.015171229813573394\n",
      "Episode 134 finished after 169 timesteps, total rewards 3.0, mean loss 0.015438573991538153\n",
      "Episode 135 finished after 143 timesteps, total rewards 2.0, mean loss 0.011863509937652738\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 136 finished after 163 timesteps, total rewards 2.0, mean loss 0.014471178766796393\n",
      "Episode 137 finished after 230 timesteps, total rewards 2.0, mean loss 0.011798408107215832\n",
      "Episode 138 finished after 131 timesteps, total rewards 0.0, mean loss 0.01640452717640615\n",
      "Episode 139 finished after 139 timesteps, total rewards 1.0, mean loss 0.012608224198577269\n",
      "Episode 140 finished after 97 timesteps, total rewards 1.0, mean loss 0.01644059429851303\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 141 finished after 153 timesteps, total rewards 1.0, mean loss 0.009480271347854712\n",
      "Episode 142 finished after 171 timesteps, total rewards 5.0, mean loss 0.011749434824010061\n",
      "Episode 143 finished after 249 timesteps, total rewards 7.0, mean loss 0.013205534857314408\n",
      "Episode 144 finished after 137 timesteps, total rewards 2.0, mean loss 0.011265776881446338\n",
      "Episode 145 finished after 114 timesteps, total rewards 0.0, mean loss 0.013721491526220026\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1.666667\n",
      "---------------------------------------\n",
      "Episode 146 finished after 177 timesteps, total rewards 1.0, mean loss 0.012658128486069821\n",
      "Episode 147 finished after 154 timesteps, total rewards 0.0, mean loss 0.011209983310094119\n",
      "Episode 148 finished after 120 timesteps, total rewards 3.0, mean loss 0.010770698602876412\n",
      "Episode 149 finished after 128 timesteps, total rewards 1.0, mean loss 0.011479996567175021\n",
      "Episode 150 finished after 138 timesteps, total rewards 4.0, mean loss 0.013536007017967717\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 151 finished after 167 timesteps, total rewards 1.0, mean loss 0.0137746355577036\n",
      "Episode 152 finished after 105 timesteps, total rewards 1.0, mean loss 0.012901230026008783\n",
      "Episode 153 finished after 287 timesteps, total rewards 3.0, mean loss 0.015155265710058213\n",
      "Episode 154 finished after 187 timesteps, total rewards 3.0, mean loss 0.0158803686329953\n",
      "Episode 155 finished after 181 timesteps, total rewards 1.0, mean loss 0.013715859242388207\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 156 finished after 234 timesteps, total rewards 1.0, mean loss 0.01301530548616394\n",
      "Episode 157 finished after 216 timesteps, total rewards 3.0, mean loss 0.012962790835242939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 158 finished after 154 timesteps, total rewards 2.0, mean loss 0.010426632598096916\n",
      "Episode 159 finished after 230 timesteps, total rewards 4.0, mean loss 0.012061520441499812\n",
      "Episode 160 finished after 230 timesteps, total rewards 1.0, mean loss 0.012703513583002621\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 161 finished after 222 timesteps, total rewards 7.0, mean loss 0.01422166584312878\n",
      "Episode 162 finished after 209 timesteps, total rewards 2.0, mean loss 0.014540298749534268\n",
      "Episode 163 finished after 157 timesteps, total rewards 2.0, mean loss 0.013100393170464538\n",
      "Episode 164 finished after 245 timesteps, total rewards 8.0, mean loss 0.013812624892143875\n",
      "Episode 165 finished after 210 timesteps, total rewards 1.0, mean loss 0.013131147697484786\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 136.666667\n",
      "---------------------------------------\n",
      "Episode 166 finished after 166 timesteps, total rewards 2.0, mean loss 0.011493461358167038\n",
      "Episode 167 finished after 94 timesteps, total rewards 1.0, mean loss 0.01592473432398773\n",
      "Episode 168 finished after 162 timesteps, total rewards 2.0, mean loss 0.013690083020422062\n",
      "Episode 169 finished after 232 timesteps, total rewards 2.0, mean loss 0.013769497526827136\n",
      "Episode 170 finished after 398 timesteps, total rewards 6.0, mean loss 0.014034655516937635\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1.666667\n",
      "---------------------------------------\n",
      "Episode 171 finished after 176 timesteps, total rewards 1.0, mean loss 0.013758978380131091\n",
      "Episode 172 finished after 90 timesteps, total rewards 1.0, mean loss 0.008605890136095695\n",
      "Episode 173 finished after 220 timesteps, total rewards 1.0, mean loss 0.01039141908831053\n",
      "Episode 174 finished after 168 timesteps, total rewards 3.0, mean loss 0.012815472202375878\n",
      "Episode 175 finished after 208 timesteps, total rewards 1.0, mean loss 0.012811702261842584\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 176 finished after 252 timesteps, total rewards 1.0, mean loss 0.013301571888448615\n",
      "Episode 177 finished after 155 timesteps, total rewards 3.0, mean loss 0.011901773870903848\n",
      "Episode 178 finished after 194 timesteps, total rewards 5.0, mean loss 0.011881757847056845\n",
      "Episode 179 finished after 154 timesteps, total rewards 2.0, mean loss 0.012359505808419342\n",
      "Episode 180 finished after 314 timesteps, total rewards 3.0, mean loss 0.014269498991084827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 181 finished after 155 timesteps, total rewards 1.0, mean loss 0.012476623743949007\n",
      "Episode 182 finished after 232 timesteps, total rewards 1.0, mean loss 0.013644972147160624\n",
      "Episode 183 finished after 101 timesteps, total rewards 1.0, mean loss 0.010649712165050545\n",
      "Episode 184 finished after 210 timesteps, total rewards 4.0, mean loss 0.01329456052037477\n",
      "Episode 185 finished after 215 timesteps, total rewards 2.0, mean loss 0.013439673273539886\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 186 finished after 240 timesteps, total rewards 5.0, mean loss 0.012445074995351508\n",
      "Episode 187 finished after 166 timesteps, total rewards 3.0, mean loss 0.014714312844338872\n",
      "Episode 188 finished after 283 timesteps, total rewards 6.0, mean loss 0.011657360717390956\n",
      "Episode 189 finished after 122 timesteps, total rewards 1.0, mean loss 0.013559910901915186\n",
      "Episode 190 finished after 157 timesteps, total rewards 0.0, mean loss 0.012335090530425634\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 191 finished after 112 timesteps, total rewards 1.0, mean loss 0.010986502068979982\n",
      "Episode 192 finished after 287 timesteps, total rewards 6.0, mean loss 0.012008814508476066\n",
      "Episode 193 finished after 218 timesteps, total rewards 4.0, mean loss 0.01368686602390964\n",
      "Episode 194 finished after 192 timesteps, total rewards 1.0, mean loss 0.011665784167651813\n",
      "Episode 195 finished after 109 timesteps, total rewards 1.0, mean loss 0.014073232207911884\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 196 finished after 164 timesteps, total rewards 3.0, mean loss 0.013612557686162579\n",
      "Episode 197 finished after 212 timesteps, total rewards 1.0, mean loss 0.013202802256719739\n",
      "Episode 198 finished after 83 timesteps, total rewards 0.0, mean loss 0.016985628756355068\n",
      "Episode 199 finished after 173 timesteps, total rewards 4.0, mean loss 0.012790617720436607\n",
      "Episode 200 finished after 89 timesteps, total rewards 2.0, mean loss 0.016236171116064867\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 201 finished after 99 timesteps, total rewards 0.0, mean loss 0.014233153552255556\n",
      "Episode 202 finished after 131 timesteps, total rewards 2.0, mean loss 0.01501353527552569\n",
      "Episode 203 finished after 84 timesteps, total rewards 0.0, mean loss 0.018490902941613013\n",
      "Episode 204 finished after 87 timesteps, total rewards 1.0, mean loss 0.009218518665206255\n",
      "Episode 205 finished after 223 timesteps, total rewards 2.0, mean loss 0.015089992110120475\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 206 finished after 137 timesteps, total rewards 3.0, mean loss 0.011227003872066559\n",
      "Episode 207 finished after 229 timesteps, total rewards 1.0, mean loss 0.014473225452077288\n",
      "Episode 208 finished after 182 timesteps, total rewards 3.0, mean loss 0.011827769638050366\n",
      "Episode 209 finished after 298 timesteps, total rewards 4.0, mean loss 0.01599830932844502\n",
      "Episode 210 finished after 235 timesteps, total rewards 4.0, mean loss 0.012433107960176595\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 211 finished after 128 timesteps, total rewards 0.0, mean loss 0.016699602200219488\n",
      "Episode 212 finished after 125 timesteps, total rewards 5.0, mean loss 0.011614650982141029\n",
      "Episode 213 finished after 190 timesteps, total rewards 2.0, mean loss 0.012906066688791677\n",
      "Episode 214 finished after 144 timesteps, total rewards 2.0, mean loss 0.014499216823828142\n",
      "Episode 215 finished after 221 timesteps, total rewards 0.0, mean loss 0.013611862369465622\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 101.666667\n",
      "---------------------------------------\n",
      "Episode 216 finished after 124 timesteps, total rewards 3.0, mean loss 0.012380358963127946\n",
      "Episode 217 finished after 129 timesteps, total rewards 3.0, mean loss 0.01686580114670336\n",
      "Episode 218 finished after 102 timesteps, total rewards 1.0, mean loss 0.016507192234641088\n",
      "Episode 219 finished after 331 timesteps, total rewards 4.0, mean loss 0.012850399634540939\n",
      "Episode 220 finished after 129 timesteps, total rewards 1.0, mean loss 0.011703790004848856\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 221 finished after 113 timesteps, total rewards 2.0, mean loss 0.012506060640235427\n",
      "Episode 222 finished after 234 timesteps, total rewards 5.0, mean loss 0.015177805705541251\n",
      "Episode 223 finished after 220 timesteps, total rewards 1.0, mean loss 0.011991849366545054\n",
      "Episode 224 finished after 192 timesteps, total rewards 2.0, mean loss 0.0142135873177646\n",
      "Episode 225 finished after 234 timesteps, total rewards 1.0, mean loss 0.010936598923693124\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 226 finished after 215 timesteps, total rewards 4.0, mean loss 0.013230507117208787\n",
      "Episode 227 finished after 203 timesteps, total rewards 1.0, mean loss 0.011942442525440558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 228 finished after 299 timesteps, total rewards 1.0, mean loss 0.010985575293686808\n",
      "Episode 229 finished after 160 timesteps, total rewards 4.0, mean loss 0.013003204559936421\n",
      "Episode 230 finished after 315 timesteps, total rewards 7.0, mean loss 0.01461993498355335\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 231 finished after 156 timesteps, total rewards 0.0, mean loss 0.012906476359766645\n",
      "Episode 232 finished after 282 timesteps, total rewards 5.0, mean loss 0.012703324015904962\n",
      "Episode 233 finished after 170 timesteps, total rewards 8.0, mean loss 0.013009951197754537\n",
      "Episode 234 finished after 91 timesteps, total rewards 1.0, mean loss 0.009876647043671295\n",
      "Episode 235 finished after 171 timesteps, total rewards 2.0, mean loss 0.011532911432443026\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 236 finished after 217 timesteps, total rewards 3.0, mean loss 0.012027528119365489\n",
      "Episode 237 finished after 85 timesteps, total rewards 1.0, mean loss 0.015800176950527683\n",
      "Episode 238 finished after 128 timesteps, total rewards 2.0, mean loss 0.015587475328743494\n",
      "Episode 239 finished after 159 timesteps, total rewards 1.0, mean loss 0.01215879664996693\n",
      "Episode 240 finished after 200 timesteps, total rewards 2.0, mean loss 0.015783407021808672\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 241 finished after 165 timesteps, total rewards 2.0, mean loss 0.015941531168540347\n",
      "Episode 242 finished after 241 timesteps, total rewards 6.0, mean loss 0.012252335484645376\n",
      "Episode 243 finished after 237 timesteps, total rewards 4.0, mean loss 0.012195730683681622\n",
      "Episode 244 finished after 233 timesteps, total rewards 4.0, mean loss 0.01495109442710155\n",
      "Episode 245 finished after 168 timesteps, total rewards 3.0, mean loss 0.01219234569214764\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 246 finished after 154 timesteps, total rewards 4.0, mean loss 0.01448796159622196\n",
      "Episode 247 finished after 178 timesteps, total rewards 2.0, mean loss 0.013063020809792934\n",
      "Episode 248 finished after 207 timesteps, total rewards 1.0, mean loss 0.013915853047488113\n",
      "Episode 249 finished after 177 timesteps, total rewards 2.0, mean loss 0.013699714616460641\n",
      "Episode 250 finished after 203 timesteps, total rewards 4.0, mean loss 0.013680047625196947\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 251 finished after 93 timesteps, total rewards 4.0, mean loss 0.01341992028317945\n",
      "Episode 252 finished after 133 timesteps, total rewards 0.0, mean loss 0.013353088965505551\n",
      "Episode 253 finished after 180 timesteps, total rewards 2.0, mean loss 0.012316791455345488\n",
      "Episode 254 finished after 151 timesteps, total rewards 6.0, mean loss 0.010198070715098265\n",
      "Episode 255 finished after 271 timesteps, total rewards 3.0, mean loss 0.013627799028161067\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 256 finished after 251 timesteps, total rewards 4.0, mean loss 0.015096130993994568\n",
      "Episode 257 finished after 165 timesteps, total rewards 2.0, mean loss 0.013722436962857363\n",
      "Episode 258 finished after 186 timesteps, total rewards 3.0, mean loss 0.011553961328548124\n",
      "Episode 259 finished after 168 timesteps, total rewards 9.0, mean loss 0.013531006603010437\n",
      "Episode 260 finished after 236 timesteps, total rewards 2.0, mean loss 0.016306289165109095\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1.666667\n",
      "---------------------------------------\n",
      "Episode 261 finished after 180 timesteps, total rewards 1.0, mean loss 0.014290311313460632\n",
      "Episode 262 finished after 98 timesteps, total rewards 1.0, mean loss 0.015544815642025075\n",
      "Episode 263 finished after 202 timesteps, total rewards 4.0, mean loss 0.013670026548118918\n",
      "Episode 264 finished after 268 timesteps, total rewards 5.0, mean loss 0.01213864967369895\n",
      "Episode 265 finished after 158 timesteps, total rewards 1.0, mean loss 0.013028681524275857\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 266 finished after 179 timesteps, total rewards 2.0, mean loss 0.01505943935874371\n",
      "Episode 267 finished after 186 timesteps, total rewards 0.0, mean loss 0.014058357477062933\n",
      "Episode 268 finished after 266 timesteps, total rewards 3.0, mean loss 0.015137629886606404\n",
      "Episode 269 finished after 236 timesteps, total rewards 5.0, mean loss 0.012014715420926962\n",
      "Episode 270 finished after 171 timesteps, total rewards 3.0, mean loss 0.014510475650102266\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 271 finished after 130 timesteps, total rewards 1.0, mean loss 0.010629173630821663\n",
      "Episode 272 finished after 173 timesteps, total rewards 2.0, mean loss 0.013153424760480842\n",
      "Episode 273 finished after 128 timesteps, total rewards 3.0, mean loss 0.014965853432329368\n",
      "Episode 274 finished after 170 timesteps, total rewards 2.0, mean loss 0.012261517797271474\n",
      "Episode 275 finished after 138 timesteps, total rewards 2.0, mean loss 0.011606515705827169\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 276 finished after 238 timesteps, total rewards 2.0, mean loss 0.014740581342587522\n",
      "Episode 277 finished after 164 timesteps, total rewards 1.0, mean loss 0.01178723417650894\n",
      "Episode 278 finished after 312 timesteps, total rewards 5.0, mean loss 0.012624570612224955\n",
      "Episode 279 finished after 152 timesteps, total rewards 1.0, mean loss 0.010759762080995546\n",
      "Episode 280 finished after 192 timesteps, total rewards 0.0, mean loss 0.011535361902967148\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 281 finished after 129 timesteps, total rewards 1.0, mean loss 0.01073965164512403\n",
      "Episode 282 finished after 202 timesteps, total rewards 6.0, mean loss 0.011499867211523901\n",
      "Episode 283 finished after 166 timesteps, total rewards 2.0, mean loss 0.010544320918696426\n",
      "Episode 284 finished after 156 timesteps, total rewards 0.0, mean loss 0.009707828665154804\n",
      "Episode 285 finished after 139 timesteps, total rewards 1.0, mean loss 0.01235402404662957\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 286 finished after 256 timesteps, total rewards 5.0, mean loss 0.01297964681373287\n",
      "Episode 287 finished after 163 timesteps, total rewards 3.0, mean loss 0.012980442015458457\n",
      "Episode 288 finished after 108 timesteps, total rewards 0.0, mean loss 0.013047243560625551\n",
      "Episode 289 finished after 192 timesteps, total rewards 5.0, mean loss 0.012321907289068198\n",
      "Episode 290 finished after 285 timesteps, total rewards 5.0, mean loss 0.013356354694169887\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 291 finished after 165 timesteps, total rewards 3.0, mean loss 0.012472051337306656\n",
      "Episode 292 finished after 166 timesteps, total rewards 5.0, mean loss 0.013273839205443854\n",
      "Episode 293 finished after 89 timesteps, total rewards 1.0, mean loss 0.01447891311414587\n",
      "Episode 294 finished after 130 timesteps, total rewards 0.0, mean loss 0.012078918891179805\n",
      "Episode 295 finished after 166 timesteps, total rewards 2.0, mean loss 0.011971723150974911\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 296 finished after 251 timesteps, total rewards 4.0, mean loss 0.013632695362245732\n",
      "Episode 297 finished after 88 timesteps, total rewards 0.0, mean loss 0.014597559738831478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 298 finished after 250 timesteps, total rewards 5.0, mean loss 0.013184807317156811\n",
      "Episode 299 finished after 155 timesteps, total rewards 2.0, mean loss 0.013434478622108262\n",
      "Episode 300 finished after 166 timesteps, total rewards 2.0, mean loss 0.014689585249832444\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 125.000000\n",
      "---------------------------------------\n",
      "Episode 301 finished after 265 timesteps, total rewards 6.0, mean loss 0.013159713094212764\n",
      "Episode 302 finished after 161 timesteps, total rewards 2.0, mean loss 0.012913884690211456\n",
      "Episode 303 finished after 86 timesteps, total rewards 0.0, mean loss 0.010423636184071277\n",
      "Episode 304 finished after 115 timesteps, total rewards 2.0, mean loss 0.01639821132133796\n",
      "Episode 305 finished after 174 timesteps, total rewards 2.0, mean loss 0.01391545140881783\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 306 finished after 232 timesteps, total rewards 3.0, mean loss 0.013704577130456214\n",
      "Episode 307 finished after 243 timesteps, total rewards 3.0, mean loss 0.01410415325987626\n",
      "Episode 308 finished after 171 timesteps, total rewards 0.0, mean loss 0.010940715965453388\n",
      "Episode 309 finished after 97 timesteps, total rewards 1.0, mean loss 0.013238254129243343\n",
      "Episode 310 finished after 162 timesteps, total rewards 3.0, mean loss 0.012475468733302679\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 311 finished after 130 timesteps, total rewards 1.0, mean loss 0.014136473896080413\n",
      "Episode 312 finished after 171 timesteps, total rewards 6.0, mean loss 0.013139266770418831\n",
      "Episode 313 finished after 330 timesteps, total rewards 5.0, mean loss 0.013279390224598694\n",
      "Episode 314 finished after 191 timesteps, total rewards 4.0, mean loss 0.013453830835305542\n",
      "Episode 315 finished after 176 timesteps, total rewards 4.0, mean loss 0.011486685787779359\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 316 finished after 242 timesteps, total rewards 4.0, mean loss 0.012440745750827473\n",
      "Episode 317 finished after 317 timesteps, total rewards 5.0, mean loss 0.012369565153283314\n",
      "Episode 318 finished after 132 timesteps, total rewards 2.0, mean loss 0.01438688530613559\n",
      "Episode 319 finished after 158 timesteps, total rewards 1.0, mean loss 0.011632926729085924\n",
      "Episode 320 finished after 257 timesteps, total rewards 7.0, mean loss 0.013811096392275452\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 321 finished after 125 timesteps, total rewards 1.0, mean loss 0.011483636687626132\n",
      "Episode 322 finished after 163 timesteps, total rewards 1.0, mean loss 0.01614199682696654\n",
      "Episode 323 finished after 167 timesteps, total rewards 2.0, mean loss 0.0143261959975478\n",
      "Episode 324 finished after 127 timesteps, total rewards 1.0, mean loss 0.011425932355529416\n",
      "Episode 325 finished after 246 timesteps, total rewards 5.0, mean loss 0.012591570157028179\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 326 finished after 251 timesteps, total rewards 4.0, mean loss 0.015091929858619072\n",
      "Episode 327 finished after 95 timesteps, total rewards 1.0, mean loss 0.010054056232452\n",
      "Episode 328 finished after 294 timesteps, total rewards 4.0, mean loss 0.012310364943085402\n",
      "Episode 329 finished after 149 timesteps, total rewards 2.0, mean loss 0.01745160420192015\n",
      "Episode 330 finished after 158 timesteps, total rewards 1.0, mean loss 0.014273066647627827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 331 finished after 98 timesteps, total rewards 0.0, mean loss 0.01637149644969269\n",
      "Episode 332 finished after 91 timesteps, total rewards 4.0, mean loss 0.011423516174545512\n",
      "Episode 333 finished after 193 timesteps, total rewards 3.0, mean loss 0.013233268937642487\n",
      "Episode 334 finished after 173 timesteps, total rewards 4.0, mean loss 0.016059516438783006\n",
      "Episode 335 finished after 261 timesteps, total rewards 6.0, mean loss 0.013386885140064865\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 336 finished after 177 timesteps, total rewards 6.0, mean loss 0.0134552740759945\n",
      "Episode 337 finished after 101 timesteps, total rewards 3.0, mean loss 0.013273372647115075\n",
      "Episode 338 finished after 151 timesteps, total rewards 3.0, mean loss 0.013391647133332468\n",
      "Episode 339 finished after 179 timesteps, total rewards 2.0, mean loss 0.015117346698531424\n",
      "Episode 340 finished after 259 timesteps, total rewards 3.0, mean loss 0.013435111809215144\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 341 finished after 109 timesteps, total rewards 0.0, mean loss 0.01179116761040609\n",
      "Episode 342 finished after 148 timesteps, total rewards 1.0, mean loss 0.012384235252652984\n",
      "Episode 343 finished after 200 timesteps, total rewards 6.0, mean loss 0.014962556357932043\n",
      "Episode 344 finished after 239 timesteps, total rewards 3.0, mean loss 0.011952120291931793\n",
      "Episode 345 finished after 164 timesteps, total rewards 2.0, mean loss 0.01605882352469496\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1.666667\n",
      "---------------------------------------\n",
      "Episode 346 finished after 174 timesteps, total rewards 6.0, mean loss 0.010935742729768322\n",
      "Episode 347 finished after 107 timesteps, total rewards 2.0, mean loss 0.012332741600267098\n",
      "Episode 348 finished after 125 timesteps, total rewards 1.0, mean loss 0.012533952861558647\n",
      "Episode 349 finished after 198 timesteps, total rewards 5.0, mean loss 0.01216001237899587\n",
      "Episode 350 finished after 238 timesteps, total rewards 2.0, mean loss 0.011792565479680422\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 351 finished after 229 timesteps, total rewards 3.0, mean loss 0.014084305586273006\n",
      "Episode 352 finished after 228 timesteps, total rewards 3.0, mean loss 0.012886251949678632\n",
      "Episode 353 finished after 133 timesteps, total rewards 1.0, mean loss 0.012928573566056084\n",
      "Episode 354 finished after 231 timesteps, total rewards 5.0, mean loss 0.016346006806399593\n",
      "Episode 355 finished after 113 timesteps, total rewards 1.0, mean loss 0.013858326226555272\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 356 finished after 176 timesteps, total rewards 4.0, mean loss 0.014690307891379978\n",
      "Episode 357 finished after 299 timesteps, total rewards 5.0, mean loss 0.013030919541599366\n",
      "Episode 358 finished after 200 timesteps, total rewards 1.0, mean loss 0.013366502845528885\n",
      "Episode 359 finished after 128 timesteps, total rewards 3.0, mean loss 0.010823059624442521\n",
      "Episode 360 finished after 160 timesteps, total rewards 1.0, mean loss 0.012656394010627991\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 361 finished after 203 timesteps, total rewards 2.0, mean loss 0.01298997061552332\n",
      "Episode 362 finished after 165 timesteps, total rewards 3.0, mean loss 0.012982002864847539\n",
      "Episode 363 finished after 155 timesteps, total rewards 3.0, mean loss 0.012262777247956217\n",
      "Episode 364 finished after 163 timesteps, total rewards 0.0, mean loss 0.016197745826993813\n",
      "Episode 365 finished after 204 timesteps, total rewards 2.0, mean loss 0.012963575585173186\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 366 finished after 158 timesteps, total rewards 1.0, mean loss 0.013765753775870405\n",
      "Episode 367 finished after 136 timesteps, total rewards 1.0, mean loss 0.013676504629021272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 368 finished after 93 timesteps, total rewards 0.0, mean loss 0.014799507886236433\n",
      "Episode 369 finished after 162 timesteps, total rewards 1.0, mean loss 0.014450377601333234\n",
      "Episode 370 finished after 164 timesteps, total rewards 3.0, mean loss 0.010689261181252688\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 123.333333\n",
      "---------------------------------------\n",
      "Episode 371 finished after 144 timesteps, total rewards 1.0, mean loss 0.011352205707276476\n",
      "Episode 372 finished after 237 timesteps, total rewards 6.0, mean loss 0.01111266264615935\n",
      "Episode 373 finished after 128 timesteps, total rewards 3.0, mean loss 0.012068385444990781\n",
      "Episode 374 finished after 107 timesteps, total rewards 5.0, mean loss 0.010645053577370002\n",
      "Episode 375 finished after 97 timesteps, total rewards 1.0, mean loss 0.015402398485412519\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 376 finished after 204 timesteps, total rewards 1.0, mean loss 0.014557503481495693\n",
      "Episode 377 finished after 97 timesteps, total rewards 1.0, mean loss 0.01854659495416179\n",
      "Episode 378 finished after 309 timesteps, total rewards 6.0, mean loss 0.014910501504334256\n",
      "Episode 379 finished after 316 timesteps, total rewards 6.0, mean loss 0.013380291926088968\n",
      "Episode 380 finished after 240 timesteps, total rewards 5.0, mean loss 0.013278229569126172\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 381 finished after 228 timesteps, total rewards 6.0, mean loss 0.014198410217910637\n",
      "Episode 382 finished after 137 timesteps, total rewards 1.0, mean loss 0.01235311295250626\n",
      "Episode 383 finished after 160 timesteps, total rewards 1.0, mean loss 0.01447916608608466\n",
      "Episode 384 finished after 196 timesteps, total rewards 2.0, mean loss 0.01392521930816025\n",
      "Episode 385 finished after 262 timesteps, total rewards 6.0, mean loss 0.015343538149440384\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 386 finished after 164 timesteps, total rewards 1.0, mean loss 0.015461627655984158\n",
      "Episode 387 finished after 219 timesteps, total rewards 4.0, mean loss 0.012542534975061772\n",
      "Episode 388 finished after 81 timesteps, total rewards 1.0, mean loss 0.016260032755927106\n",
      "Episode 389 finished after 174 timesteps, total rewards 4.0, mean loss 0.015236086461655581\n",
      "Episode 390 finished after 83 timesteps, total rewards 1.0, mean loss 0.01466450624656022\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 391 finished after 310 timesteps, total rewards 6.0, mean loss 0.013058076002057679\n",
      "Episode 392 finished after 174 timesteps, total rewards 3.0, mean loss 0.015595027608040924\n",
      "Episode 393 finished after 159 timesteps, total rewards 3.0, mean loss 0.012072515927632954\n",
      "Episode 394 finished after 79 timesteps, total rewards 1.0, mean loss 0.01012025947291684\n",
      "Episode 395 finished after 162 timesteps, total rewards 3.0, mean loss 0.013433675199152792\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 396 finished after 115 timesteps, total rewards 2.0, mean loss 0.014593562815697743\n",
      "Episode 397 finished after 91 timesteps, total rewards 1.0, mean loss 0.015077665203284846\n",
      "Episode 398 finished after 199 timesteps, total rewards 3.0, mean loss 0.015394940995070125\n",
      "Episode 399 finished after 211 timesteps, total rewards 5.0, mean loss 0.013000400045505114\n",
      "Episode 400 finished after 167 timesteps, total rewards 1.0, mean loss 0.011737333725936416\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 401 finished after 150 timesteps, total rewards 0.0, mean loss 0.012470611160630748\n",
      "Episode 402 finished after 214 timesteps, total rewards 5.0, mean loss 0.010675303797950482\n",
      "Episode 403 finished after 197 timesteps, total rewards 3.0, mean loss 0.01164659106413134\n",
      "Episode 404 finished after 247 timesteps, total rewards 5.0, mean loss 0.016566330959079202\n",
      "Episode 405 finished after 93 timesteps, total rewards 1.0, mean loss 0.014237747493078582\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 406 finished after 259 timesteps, total rewards 6.0, mean loss 0.017004711548116825\n",
      "Episode 407 finished after 262 timesteps, total rewards 5.0, mean loss 0.014388250439920558\n",
      "Episode 408 finished after 168 timesteps, total rewards 4.0, mean loss 0.01325912776302998\n",
      "Episode 409 finished after 131 timesteps, total rewards 3.0, mean loss 0.013549454963654067\n",
      "Episode 410 finished after 159 timesteps, total rewards 0.0, mean loss 0.01083621556375325\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 411 finished after 270 timesteps, total rewards 3.0, mean loss 0.014227737234614323\n",
      "Episode 412 finished after 258 timesteps, total rewards 7.0, mean loss 0.013652838485350637\n",
      "Episode 413 finished after 134 timesteps, total rewards 2.0, mean loss 0.010563672477541902\n",
      "Episode 414 finished after 212 timesteps, total rewards 4.0, mean loss 0.015910997665656085\n",
      "Episode 415 finished after 133 timesteps, total rewards 0.0, mean loss 0.014747591741418964\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 86.666667\n",
      "---------------------------------------\n",
      "Episode 416 finished after 216 timesteps, total rewards 5.0, mean loss 0.015872624250025384\n",
      "Episode 417 finished after 226 timesteps, total rewards 2.0, mean loss 0.012946763774637644\n",
      "Episode 418 finished after 165 timesteps, total rewards 2.0, mean loss 0.014263042054671765\n",
      "Episode 419 finished after 73 timesteps, total rewards 1.0, mean loss 0.00973623967280991\n",
      "Episode 420 finished after 178 timesteps, total rewards 2.0, mean loss 0.013638402482418191\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 421 finished after 148 timesteps, total rewards 2.0, mean loss 0.014829783026397234\n",
      "Episode 422 finished after 145 timesteps, total rewards 3.0, mean loss 0.013329643390647232\n",
      "Episode 423 finished after 121 timesteps, total rewards 2.0, mean loss 0.015372347276805023\n",
      "Episode 424 finished after 91 timesteps, total rewards 0.0, mean loss 0.011152399503811427\n",
      "Episode 425 finished after 250 timesteps, total rewards 6.0, mean loss 0.015040416941454169\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 426 finished after 155 timesteps, total rewards 4.0, mean loss 0.016958753776834198\n",
      "Episode 427 finished after 211 timesteps, total rewards 4.0, mean loss 0.012855833329073313\n",
      "Episode 428 finished after 231 timesteps, total rewards 3.0, mean loss 0.014016948529663778\n",
      "Episode 429 finished after 108 timesteps, total rewards 2.0, mean loss 0.014670433895204312\n",
      "Episode 430 finished after 167 timesteps, total rewards 2.0, mean loss 0.010712695128875533\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 150.000000\n",
      "---------------------------------------\n",
      "Episode 431 finished after 364 timesteps, total rewards 6.0, mean loss 0.014325651053285992\n",
      "Episode 432 finished after 86 timesteps, total rewards 2.0, mean loss 0.016394949383792726\n",
      "Episode 433 finished after 324 timesteps, total rewards 5.0, mean loss 0.013311471149656293\n",
      "Episode 434 finished after 96 timesteps, total rewards 0.0, mean loss 0.019325493010304246\n",
      "Episode 435 finished after 274 timesteps, total rewards 8.0, mean loss 0.014636557342590386\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 436 finished after 241 timesteps, total rewards 3.0, mean loss 0.015465427770813603\n",
      "Episode 437 finished after 257 timesteps, total rewards 2.0, mean loss 0.01566801559272202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 438 finished after 269 timesteps, total rewards 3.0, mean loss 0.014836646688623409\n",
      "Episode 439 finished after 83 timesteps, total rewards 0.0, mean loss 0.013876013987079118\n",
      "Episode 440 finished after 158 timesteps, total rewards 4.0, mean loss 0.013396736132609142\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 110.000000\n",
      "---------------------------------------\n",
      "Episode 441 finished after 165 timesteps, total rewards 3.0, mean loss 0.014664101175466468\n",
      "Episode 442 finished after 161 timesteps, total rewards 1.0, mean loss 0.013023247742940098\n",
      "Episode 443 finished after 160 timesteps, total rewards 3.0, mean loss 0.010732514691790129\n",
      "Episode 444 finished after 159 timesteps, total rewards 0.0, mean loss 0.014823914751551062\n",
      "Episode 445 finished after 150 timesteps, total rewards 1.0, mean loss 0.014515524609887507\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 446 finished after 180 timesteps, total rewards 2.0, mean loss 0.013164929192134878\n",
      "Episode 447 finished after 128 timesteps, total rewards 2.0, mean loss 0.013590340377618304\n",
      "Episode 448 finished after 100 timesteps, total rewards 0.0, mean loss 0.016477485312643692\n",
      "Episode 449 finished after 178 timesteps, total rewards 2.0, mean loss 0.013590770553021818\n",
      "Episode 450 finished after 131 timesteps, total rewards 2.0, mean loss 0.012414641645896953\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 451 finished after 169 timesteps, total rewards 0.0, mean loss 0.013653353093083144\n",
      "Episode 452 finished after 156 timesteps, total rewards 1.0, mean loss 0.015974323727091243\n",
      "Episode 453 finished after 164 timesteps, total rewards 2.0, mean loss 0.013117555026128241\n",
      "Episode 454 finished after 241 timesteps, total rewards 2.0, mean loss 0.013784963564649687\n",
      "Episode 455 finished after 86 timesteps, total rewards 2.0, mean loss 0.010719933832533994\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 456 finished after 160 timesteps, total rewards 5.0, mean loss 0.014756998334178206\n",
      "Episode 457 finished after 240 timesteps, total rewards 6.0, mean loss 0.011840537564451855\n",
      "Episode 458 finished after 208 timesteps, total rewards 2.0, mean loss 0.01753856485535065\n",
      "Episode 459 finished after 244 timesteps, total rewards 3.0, mean loss 0.014008243864885967\n",
      "Episode 460 finished after 239 timesteps, total rewards 5.0, mean loss 0.0125392781717656\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 10.000000\n",
      "---------------------------------------\n",
      "Episode 461 finished after 157 timesteps, total rewards 6.0, mean loss 0.014413474432035545\n",
      "Episode 462 finished after 187 timesteps, total rewards 4.0, mean loss 0.01130364920616674\n",
      "Episode 463 finished after 258 timesteps, total rewards 8.0, mean loss 0.01390037920022886\n",
      "Episode 464 finished after 93 timesteps, total rewards 0.0, mean loss 0.014673615369403745\n",
      "Episode 465 finished after 92 timesteps, total rewards 1.0, mean loss 0.015521990552097914\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 466 finished after 247 timesteps, total rewards 2.0, mean loss 0.014048680148283274\n",
      "Episode 467 finished after 231 timesteps, total rewards 4.0, mean loss 0.017078427050388176\n",
      "Episode 468 finished after 105 timesteps, total rewards 0.0, mean loss 0.012686890476089459\n",
      "Episode 469 finished after 218 timesteps, total rewards 9.0, mean loss 0.014164104278454635\n",
      "Episode 470 finished after 124 timesteps, total rewards 3.0, mean loss 0.016368666427649347\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 471 finished after 248 timesteps, total rewards 7.0, mean loss 0.013744545244633392\n",
      "Episode 472 finished after 168 timesteps, total rewards 2.0, mean loss 0.014015745158238653\n",
      "Episode 473 finished after 105 timesteps, total rewards 1.0, mean loss 0.011980763051126685\n",
      "Episode 474 finished after 155 timesteps, total rewards 3.0, mean loss 0.01424574141602798\n",
      "Episode 475 finished after 140 timesteps, total rewards 3.0, mean loss 0.011269786550890006\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 476 finished after 162 timesteps, total rewards 3.0, mean loss 0.012403636143174896\n",
      "Episode 477 finished after 214 timesteps, total rewards 3.0, mean loss 0.015056832827204429\n",
      "Episode 478 finished after 124 timesteps, total rewards 2.0, mean loss 0.016494892809972836\n",
      "Episode 479 finished after 227 timesteps, total rewards 8.0, mean loss 0.014604780269570808\n",
      "Episode 480 finished after 211 timesteps, total rewards 6.0, mean loss 0.015599341869790943\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 481 finished after 179 timesteps, total rewards 2.0, mean loss 0.012211621218149917\n",
      "Episode 482 finished after 363 timesteps, total rewards 13.0, mean loss 0.014927471244993771\n",
      "Episode 483 finished after 195 timesteps, total rewards 4.0, mean loss 0.014545545113968472\n",
      "Episode 484 finished after 272 timesteps, total rewards 7.0, mean loss 0.012943979347549912\n",
      "Episode 485 finished after 123 timesteps, total rewards 0.0, mean loss 0.013439234357390636\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 6.666667\n",
      "---------------------------------------\n",
      "Episode 486 finished after 303 timesteps, total rewards 5.0, mean loss 0.013413036212783457\n",
      "Episode 487 finished after 305 timesteps, total rewards 8.0, mean loss 0.014718672631071789\n",
      "Episode 488 finished after 171 timesteps, total rewards 5.0, mean loss 0.012899825445526791\n",
      "Episode 489 finished after 103 timesteps, total rewards 4.0, mean loss 0.01741961688504021\n",
      "Episode 490 finished after 192 timesteps, total rewards 5.0, mean loss 0.0162903392570873\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 491 finished after 269 timesteps, total rewards 4.0, mean loss 0.017317402129711543\n",
      "Episode 492 finished after 85 timesteps, total rewards 0.0, mean loss 0.021399980353068233\n",
      "Episode 493 finished after 242 timesteps, total rewards 3.0, mean loss 0.014273518028843201\n",
      "Episode 494 finished after 167 timesteps, total rewards 5.0, mean loss 0.013740744302153845\n",
      "Episode 495 finished after 161 timesteps, total rewards 2.0, mean loss 0.016136172755859265\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 496 finished after 209 timesteps, total rewards 4.0, mean loss 0.014345498272436715\n",
      "Episode 497 finished after 308 timesteps, total rewards 6.0, mean loss 0.01406884339789999\n",
      "Episode 498 finished after 133 timesteps, total rewards 2.0, mean loss 0.014444765171252325\n",
      "Episode 499 finished after 97 timesteps, total rewards 1.0, mean loss 0.014435180926907895\n",
      "Episode 500 finished after 120 timesteps, total rewards 2.0, mean loss 0.013234684803743827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 501 finished after 165 timesteps, total rewards 3.0, mean loss 0.015465197123756463\n",
      "Episode 502 finished after 175 timesteps, total rewards 1.0, mean loss 0.0107734285719094\n",
      "Episode 503 finished after 124 timesteps, total rewards 1.0, mean loss 0.01652847479049195\n",
      "Episode 504 finished after 95 timesteps, total rewards 1.0, mean loss 0.01645501932806311\n",
      "Episode 505 finished after 269 timesteps, total rewards 2.0, mean loss 0.013545957921654053\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 506 finished after 264 timesteps, total rewards 6.0, mean loss 0.013995528604578849\n",
      "Episode 507 finished after 130 timesteps, total rewards 2.0, mean loss 0.014552102108987478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 508 finished after 95 timesteps, total rewards 0.0, mean loss 0.0188147237714331\n",
      "Episode 509 finished after 154 timesteps, total rewards 0.0, mean loss 0.01411261495736476\n",
      "Episode 510 finished after 119 timesteps, total rewards 2.0, mean loss 0.012689687407581907\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 511 finished after 230 timesteps, total rewards 4.0, mean loss 0.013935721763213539\n",
      "Episode 512 finished after 244 timesteps, total rewards 4.0, mean loss 0.016633198951508422\n",
      "Episode 513 finished after 91 timesteps, total rewards 0.0, mean loss 0.014130330802220339\n",
      "Episode 514 finished after 248 timesteps, total rewards 4.0, mean loss 0.014787255071291104\n",
      "Episode 515 finished after 249 timesteps, total rewards 7.0, mean loss 0.014051066347513344\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 516 finished after 326 timesteps, total rewards 4.0, mean loss 0.014604789864329372\n",
      "Episode 517 finished after 88 timesteps, total rewards 1.0, mean loss 0.012207738981404955\n",
      "Episode 518 finished after 129 timesteps, total rewards 2.0, mean loss 0.014591286750104236\n",
      "Episode 519 finished after 193 timesteps, total rewards 4.0, mean loss 0.015075111011624665\n",
      "Episode 520 finished after 116 timesteps, total rewards 1.0, mean loss 0.012438841964843772\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 85.000000\n",
      "---------------------------------------\n",
      "Episode 521 finished after 274 timesteps, total rewards 7.0, mean loss 0.014528901061567012\n",
      "Episode 522 finished after 125 timesteps, total rewards 2.0, mean loss 0.017010136193712243\n",
      "Episode 523 finished after 290 timesteps, total rewards 4.0, mean loss 0.013859458325745875\n",
      "Episode 524 finished after 178 timesteps, total rewards 0.0, mean loss 0.01342253736424681\n",
      "Episode 525 finished after 158 timesteps, total rewards 0.0, mean loss 0.017635150495371368\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 526 finished after 177 timesteps, total rewards 4.0, mean loss 0.017379471630819595\n",
      "Episode 527 finished after 307 timesteps, total rewards 3.0, mean loss 0.01549097845813939\n",
      "Episode 528 finished after 204 timesteps, total rewards 6.0, mean loss 0.014031742156351125\n",
      "Episode 529 finished after 216 timesteps, total rewards 4.0, mean loss 0.017641584381568397\n",
      "Episode 530 finished after 210 timesteps, total rewards 4.0, mean loss 0.014995174536376171\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 531 finished after 160 timesteps, total rewards 2.0, mean loss 0.014128871549019095\n",
      "Episode 532 finished after 158 timesteps, total rewards 4.0, mean loss 0.01281717305009576\n",
      "Episode 533 finished after 222 timesteps, total rewards 2.0, mean loss 0.01571526710910991\n",
      "Episode 534 finished after 250 timesteps, total rewards 2.0, mean loss 0.016073724694550037\n",
      "Episode 535 finished after 240 timesteps, total rewards 2.0, mean loss 0.013751529526780359\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 536 finished after 165 timesteps, total rewards 2.0, mean loss 0.012139637043980077\n",
      "Episode 537 finished after 184 timesteps, total rewards 3.0, mean loss 0.01601365942287489\n",
      "Episode 538 finished after 198 timesteps, total rewards 4.0, mean loss 0.01734392918120703\n",
      "Episode 539 finished after 176 timesteps, total rewards 4.0, mean loss 0.014597204441832384\n",
      "Episode 540 finished after 141 timesteps, total rewards 1.0, mean loss 0.015514858456738029\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 541 finished after 183 timesteps, total rewards 0.0, mean loss 0.015916921759849347\n",
      "Episode 542 finished after 226 timesteps, total rewards 4.0, mean loss 0.014310609361010529\n",
      "Episode 543 finished after 84 timesteps, total rewards 3.0, mean loss 0.01974315094862721\n",
      "Episode 544 finished after 128 timesteps, total rewards 1.0, mean loss 0.014807373074177121\n",
      "Episode 545 finished after 92 timesteps, total rewards 1.0, mean loss 0.014299316271954804\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 546 finished after 176 timesteps, total rewards 4.0, mean loss 0.017058470551091887\n",
      "Episode 547 finished after 289 timesteps, total rewards 3.0, mean loss 0.014574975430074988\n",
      "Episode 548 finished after 89 timesteps, total rewards 1.0, mean loss 0.0137549630409431\n",
      "Episode 549 finished after 108 timesteps, total rewards 1.0, mean loss 0.013592378338082065\n",
      "Episode 550 finished after 216 timesteps, total rewards 3.0, mean loss 0.014260561293532967\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 551 finished after 288 timesteps, total rewards 6.0, mean loss 0.012761685197827118\n",
      "Episode 552 finished after 212 timesteps, total rewards 4.0, mean loss 0.013398138208153882\n",
      "Episode 553 finished after 162 timesteps, total rewards 4.0, mean loss 0.012494210998907133\n",
      "Episode 554 finished after 154 timesteps, total rewards 0.0, mean loss 0.012705684064334869\n",
      "Episode 555 finished after 200 timesteps, total rewards 4.0, mean loss 0.012042836661239563\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 556 finished after 162 timesteps, total rewards 3.0, mean loss 0.015064999314127806\n",
      "Episode 557 finished after 296 timesteps, total rewards 2.0, mean loss 0.016663155028647847\n",
      "Episode 558 finished after 98 timesteps, total rewards 0.0, mean loss 0.009999872202811554\n",
      "Episode 559 finished after 284 timesteps, total rewards 3.0, mean loss 0.013665185807529807\n",
      "Episode 560 finished after 159 timesteps, total rewards 4.0, mean loss 0.010750484552875793\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 561 finished after 170 timesteps, total rewards 3.0, mean loss 0.013969817686837119\n",
      "Episode 562 finished after 232 timesteps, total rewards 2.0, mean loss 0.015473930445860505\n",
      "Episode 563 finished after 95 timesteps, total rewards 3.0, mean loss 0.015500454895051294\n",
      "Episode 564 finished after 156 timesteps, total rewards 4.0, mean loss 0.013870125831789874\n",
      "Episode 565 finished after 282 timesteps, total rewards 4.0, mean loss 0.016075800075146193\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 566 finished after 201 timesteps, total rewards 2.0, mean loss 0.012437643406829282\n",
      "Episode 567 finished after 181 timesteps, total rewards 6.0, mean loss 0.017013238756373932\n",
      "Episode 568 finished after 161 timesteps, total rewards 5.0, mean loss 0.014790919079262364\n",
      "Episode 569 finished after 183 timesteps, total rewards 4.0, mean loss 0.015354571712883685\n",
      "Episode 570 finished after 156 timesteps, total rewards 5.0, mean loss 0.014628873034384107\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 571 finished after 168 timesteps, total rewards 4.0, mean loss 0.01332634533866353\n",
      "Episode 572 finished after 92 timesteps, total rewards 2.0, mean loss 0.01719027756252731\n",
      "Episode 573 finished after 84 timesteps, total rewards 0.0, mean loss 0.016966982446666362\n",
      "Episode 574 finished after 297 timesteps, total rewards 9.0, mean loss 0.014971535057006819\n",
      "Episode 575 finished after 160 timesteps, total rewards 3.0, mean loss 0.016529154109866794\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 576 finished after 96 timesteps, total rewards 2.0, mean loss 0.01470767649652771\n",
      "Episode 577 finished after 154 timesteps, total rewards 1.0, mean loss 0.01522281796392885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 578 finished after 203 timesteps, total rewards 3.0, mean loss 0.014517326192245612\n",
      "Episode 579 finished after 125 timesteps, total rewards 2.0, mean loss 0.017742414301726968\n",
      "Episode 580 finished after 93 timesteps, total rewards 1.0, mean loss 0.014984941378521223\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 581 finished after 218 timesteps, total rewards 10.0, mean loss 0.014767132437688624\n",
      "Episode 582 finished after 237 timesteps, total rewards 4.0, mean loss 0.01610769393610943\n",
      "Episode 583 finished after 194 timesteps, total rewards 4.0, mean loss 0.016992626219436396\n",
      "Episode 584 finished after 269 timesteps, total rewards 10.0, mean loss 0.015416319297451775\n",
      "Episode 585 finished after 89 timesteps, total rewards 0.0, mean loss 0.01377478411883571\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 586 finished after 376 timesteps, total rewards 5.0, mean loss 0.01519504776711364\n",
      "Episode 587 finished after 156 timesteps, total rewards 5.0, mean loss 0.015178317348890675\n",
      "Episode 588 finished after 166 timesteps, total rewards 1.0, mean loss 0.015669695846894222\n",
      "Episode 589 finished after 159 timesteps, total rewards 0.0, mean loss 0.014262284805856165\n",
      "Episode 590 finished after 282 timesteps, total rewards 8.0, mean loss 0.016658444992765694\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 591 finished after 260 timesteps, total rewards 5.0, mean loss 0.014819117908476842\n",
      "Episode 592 finished after 182 timesteps, total rewards 4.0, mean loss 0.016001732739303925\n",
      "Episode 593 finished after 329 timesteps, total rewards 6.0, mean loss 0.014650860899951596\n",
      "Episode 594 finished after 147 timesteps, total rewards 2.0, mean loss 0.013984709623215092\n",
      "Episode 595 finished after 159 timesteps, total rewards 2.0, mean loss 0.013571046636628028\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 175.000000\n",
      "---------------------------------------\n",
      "Episode 596 finished after 292 timesteps, total rewards 3.0, mean loss 0.01828354729070569\n",
      "Episode 597 finished after 161 timesteps, total rewards 4.0, mean loss 0.015622105770935998\n",
      "Episode 598 finished after 178 timesteps, total rewards 1.0, mean loss 0.01474460448073668\n",
      "Episode 599 finished after 291 timesteps, total rewards 5.0, mean loss 0.0163227158045129\n",
      "Episode 600 finished after 188 timesteps, total rewards 2.0, mean loss 0.017465117510623036\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 601 finished after 295 timesteps, total rewards 5.0, mean loss 0.015445665377724948\n",
      "Episode 602 finished after 163 timesteps, total rewards 3.0, mean loss 0.01438022726676084\n",
      "Episode 603 finished after 109 timesteps, total rewards 2.0, mean loss 0.014240649618248993\n",
      "Episode 604 finished after 229 timesteps, total rewards 6.0, mean loss 0.01445247963820452\n",
      "Episode 605 finished after 128 timesteps, total rewards 2.0, mean loss 0.018600379168674408\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 606 finished after 157 timesteps, total rewards 4.0, mean loss 0.014771265029565396\n",
      "Episode 607 finished after 111 timesteps, total rewards 1.0, mean loss 0.017400819734043046\n",
      "Episode 608 finished after 90 timesteps, total rewards 1.0, mean loss 0.012668716815824155\n",
      "Episode 609 finished after 90 timesteps, total rewards 1.0, mean loss 0.015190084211968092\n",
      "Episode 610 finished after 226 timesteps, total rewards 3.0, mean loss 0.014041089994250282\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 10.000000\n",
      "---------------------------------------\n",
      "Episode 611 finished after 149 timesteps, total rewards 3.0, mean loss 0.015451959485009206\n",
      "Episode 612 finished after 274 timesteps, total rewards 14.0, mean loss 0.015534254277136048\n",
      "Episode 613 finished after 158 timesteps, total rewards 3.0, mean loss 0.01666510731049613\n",
      "Episode 614 finished after 167 timesteps, total rewards 3.0, mean loss 0.015228885688040332\n",
      "Episode 615 finished after 142 timesteps, total rewards 1.0, mean loss 0.0174067092505628\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 616 finished after 156 timesteps, total rewards 3.0, mean loss 0.016999234691842828\n",
      "Episode 617 finished after 156 timesteps, total rewards 2.0, mean loss 0.012752696183713105\n",
      "Episode 618 finished after 209 timesteps, total rewards 3.0, mean loss 0.015539176758761968\n",
      "Episode 619 finished after 208 timesteps, total rewards 5.0, mean loss 0.014920692614186219\n",
      "Episode 620 finished after 204 timesteps, total rewards 5.0, mean loss 0.0160790073515433\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 621 finished after 272 timesteps, total rewards 4.0, mean loss 0.017267972738837296\n",
      "Episode 622 finished after 134 timesteps, total rewards 3.0, mean loss 0.014763798562066157\n",
      "Episode 623 finished after 327 timesteps, total rewards 4.0, mean loss 0.013554650173240928\n",
      "Episode 624 finished after 231 timesteps, total rewards 4.0, mean loss 0.018506728153770538\n",
      "Episode 625 finished after 312 timesteps, total rewards 9.0, mean loss 0.013736544357925595\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 626 finished after 87 timesteps, total rewards 0.0, mean loss 0.017966079513272056\n",
      "Episode 627 finished after 296 timesteps, total rewards 6.0, mean loss 0.013746631197923184\n",
      "Episode 628 finished after 114 timesteps, total rewards 1.0, mean loss 0.019555835593915584\n",
      "Episode 629 finished after 181 timesteps, total rewards 5.0, mean loss 0.014452280894433822\n",
      "Episode 630 finished after 264 timesteps, total rewards 3.0, mean loss 0.015895794128785685\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 125.000000\n",
      "---------------------------------------\n",
      "Episode 631 finished after 90 timesteps, total rewards 0.0, mean loss 0.018645818112418056\n",
      "Episode 632 finished after 100 timesteps, total rewards 2.0, mean loss 0.01468305863410933\n",
      "Episode 633 finished after 209 timesteps, total rewards 3.0, mean loss 0.014990350111631641\n",
      "Episode 634 finished after 116 timesteps, total rewards 0.0, mean loss 0.015338953733338838\n",
      "Episode 635 finished after 272 timesteps, total rewards 4.0, mean loss 0.015969538209152662\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 636 finished after 194 timesteps, total rewards 3.0, mean loss 0.01609373915217128\n",
      "Episode 637 finished after 136 timesteps, total rewards 2.0, mean loss 0.016013605617712635\n",
      "Episode 638 finished after 137 timesteps, total rewards 2.0, mean loss 0.019486522293109175\n",
      "Episode 639 finished after 225 timesteps, total rewards 3.0, mean loss 0.015621188595218377\n",
      "Episode 640 finished after 179 timesteps, total rewards 5.0, mean loss 0.015339781102586497\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 641 finished after 119 timesteps, total rewards 1.0, mean loss 0.018431599772370914\n",
      "Episode 642 finished after 130 timesteps, total rewards 4.0, mean loss 0.014746364444839016\n",
      "Episode 643 finished after 156 timesteps, total rewards 2.0, mean loss 0.0178949503435713\n",
      "Episode 644 finished after 289 timesteps, total rewards 4.0, mean loss 0.016016190840881222\n",
      "Episode 645 finished after 143 timesteps, total rewards 3.0, mean loss 0.015529427946043702\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 646 finished after 100 timesteps, total rewards 0.0, mean loss 0.014045178116793977\n",
      "Episode 647 finished after 244 timesteps, total rewards 4.0, mean loss 0.014205023824257803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 648 finished after 272 timesteps, total rewards 5.0, mean loss 0.01646186902301933\n",
      "Episode 649 finished after 182 timesteps, total rewards 4.0, mean loss 0.01574836746998359\n",
      "Episode 650 finished after 159 timesteps, total rewards 3.0, mean loss 0.014835137609299418\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 651 finished after 94 timesteps, total rewards 1.0, mean loss 0.015101171441083933\n",
      "Episode 652 finished after 150 timesteps, total rewards 1.0, mean loss 0.01717040714613783\n",
      "Episode 653 finished after 158 timesteps, total rewards 2.0, mean loss 0.01507767141889486\n",
      "Episode 654 finished after 298 timesteps, total rewards 1.0, mean loss 0.015971658628653882\n",
      "Episode 655 finished after 169 timesteps, total rewards 4.0, mean loss 0.014004351146269132\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 656 finished after 157 timesteps, total rewards 3.0, mean loss 0.01568077925027639\n",
      "Episode 657 finished after 186 timesteps, total rewards 6.0, mean loss 0.014105919550763823\n",
      "Episode 658 finished after 288 timesteps, total rewards 6.0, mean loss 0.014515806177567155\n",
      "Episode 659 finished after 204 timesteps, total rewards 3.0, mean loss 0.01796983280998093\n",
      "Episode 660 finished after 245 timesteps, total rewards 3.0, mean loss 0.015461344471942558\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 661 finished after 167 timesteps, total rewards 2.0, mean loss 0.013469157863250087\n",
      "Episode 662 finished after 175 timesteps, total rewards 3.0, mean loss 0.01755503968069596\n",
      "Episode 663 finished after 158 timesteps, total rewards 3.0, mean loss 0.01734795073064595\n",
      "Episode 664 finished after 86 timesteps, total rewards 0.0, mean loss 0.01627553314062615\n",
      "Episode 665 finished after 162 timesteps, total rewards 3.0, mean loss 0.015053319011600745\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 666 finished after 193 timesteps, total rewards 5.0, mean loss 0.015863333704038113\n",
      "Episode 667 finished after 95 timesteps, total rewards 1.0, mean loss 0.013210458518005907\n",
      "Episode 668 finished after 88 timesteps, total rewards 1.0, mean loss 0.015128314637133206\n",
      "Episode 669 finished after 229 timesteps, total rewards 1.0, mean loss 0.01616053161544036\n",
      "Episode 670 finished after 160 timesteps, total rewards 3.0, mean loss 0.013747849413357471\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 671 finished after 308 timesteps, total rewards 10.0, mean loss 0.01642341257802282\n",
      "Episode 672 finished after 126 timesteps, total rewards 1.0, mean loss 0.01592918903724427\n",
      "Episode 673 finished after 233 timesteps, total rewards 4.0, mean loss 0.016999548173818314\n",
      "Episode 674 finished after 338 timesteps, total rewards 5.0, mean loss 0.014149311535274293\n",
      "Episode 675 finished after 98 timesteps, total rewards 0.0, mean loss 0.012805190984051609\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 3.333333\n",
      "---------------------------------------\n",
      "Episode 676 finished after 156 timesteps, total rewards 2.0, mean loss 0.01646444163578473\n",
      "Episode 677 finished after 321 timesteps, total rewards 4.0, mean loss 0.014137281567092935\n",
      "Episode 678 finished after 267 timesteps, total rewards 8.0, mean loss 0.016123917045582727\n",
      "Episode 679 finished after 91 timesteps, total rewards 2.0, mean loss 0.017545763121413805\n",
      "Episode 680 finished after 88 timesteps, total rewards 1.0, mean loss 0.013493713306822676\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 681 finished after 215 timesteps, total rewards 3.0, mean loss 0.016489122823140648\n",
      "Episode 682 finished after 127 timesteps, total rewards 0.0, mean loss 0.01681459996245222\n",
      "Episode 683 finished after 285 timesteps, total rewards 4.0, mean loss 0.014196501385671773\n",
      "Episode 684 finished after 96 timesteps, total rewards 0.0, mean loss 0.016085513697059167\n",
      "Episode 685 finished after 149 timesteps, total rewards 0.0, mean loss 0.012485684143078917\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 686 finished after 293 timesteps, total rewards 5.0, mean loss 0.01668112346600128\n",
      "Episode 687 finished after 102 timesteps, total rewards 1.0, mean loss 0.01457468386013038\n",
      "Episode 688 finished after 241 timesteps, total rewards 3.0, mean loss 0.01637333273188176\n",
      "Episode 689 finished after 230 timesteps, total rewards 5.0, mean loss 0.014984057467359969\n",
      "Episode 690 finished after 263 timesteps, total rewards 3.0, mean loss 0.014227799516053474\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 691 finished after 143 timesteps, total rewards 2.0, mean loss 0.018863270216856732\n",
      "Episode 692 finished after 125 timesteps, total rewards 2.0, mean loss 0.01657936953706667\n",
      "Episode 693 finished after 194 timesteps, total rewards 1.0, mean loss 0.017257651826756832\n",
      "Episode 694 finished after 167 timesteps, total rewards 0.0, mean loss 0.014070124794975826\n",
      "Episode 695 finished after 234 timesteps, total rewards 2.0, mean loss 0.014304896557475261\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 696 finished after 211 timesteps, total rewards 5.0, mean loss 0.015141735442429097\n",
      "Episode 697 finished after 219 timesteps, total rewards 4.0, mean loss 0.016677663273222693\n",
      "Episode 698 finished after 282 timesteps, total rewards 9.0, mean loss 0.014429112345569334\n",
      "Episode 699 finished after 398 timesteps, total rewards 8.0, mean loss 0.017232664642871907\n",
      "Episode 700 finished after 101 timesteps, total rewards 3.0, mean loss 0.013182992435758933\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 71.666667\n",
      "---------------------------------------\n",
      "Episode 701 finished after 151 timesteps, total rewards 0.0, mean loss 0.017906896504789393\n",
      "Episode 702 finished after 206 timesteps, total rewards 7.0, mean loss 0.01623912926787879\n",
      "Episode 703 finished after 156 timesteps, total rewards 2.0, mean loss 0.01767562193447539\n",
      "Episode 704 finished after 225 timesteps, total rewards 5.0, mean loss 0.014497425830793671\n",
      "Episode 705 finished after 165 timesteps, total rewards 1.0, mean loss 0.018784516933550495\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 706 finished after 98 timesteps, total rewards 0.0, mean loss 0.014092990964335124\n",
      "Episode 707 finished after 168 timesteps, total rewards 3.0, mean loss 0.01609479482119371\n",
      "Episode 708 finished after 82 timesteps, total rewards 0.0, mean loss 0.016018716369616437\n",
      "Episode 709 finished after 137 timesteps, total rewards 2.0, mean loss 0.015858433618618984\n",
      "Episode 710 finished after 231 timesteps, total rewards 5.0, mean loss 0.015136039654071164\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 711 finished after 201 timesteps, total rewards 3.0, mean loss 0.01861908229905417\n",
      "Episode 712 finished after 161 timesteps, total rewards 1.0, mean loss 0.016618439528739704\n",
      "Episode 713 finished after 220 timesteps, total rewards 6.0, mean loss 0.016439229143659098\n",
      "Episode 714 finished after 203 timesteps, total rewards 6.0, mean loss 0.017162356088456947\n",
      "Episode 715 finished after 133 timesteps, total rewards 2.0, mean loss 0.017957591261763713\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 716 finished after 113 timesteps, total rewards 1.0, mean loss 0.016236937665756568\n",
      "Episode 717 finished after 204 timesteps, total rewards 7.0, mean loss 0.015687999007829658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 718 finished after 222 timesteps, total rewards 4.0, mean loss 0.018314099029101943\n",
      "Episode 719 finished after 165 timesteps, total rewards 1.0, mean loss 0.018242422001926707\n",
      "Episode 720 finished after 204 timesteps, total rewards 3.0, mean loss 0.017836640567539315\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 721 finished after 151 timesteps, total rewards 1.0, mean loss 0.015684593636106966\n",
      "Episode 722 finished after 157 timesteps, total rewards 0.0, mean loss 0.017884754797219075\n",
      "Episode 723 finished after 147 timesteps, total rewards 3.0, mean loss 0.018387286804440642\n",
      "Episode 724 finished after 170 timesteps, total rewards 5.0, mean loss 0.017321549297776073\n",
      "Episode 725 finished after 162 timesteps, total rewards 4.0, mean loss 0.016635612053056477\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 726 finished after 233 timesteps, total rewards 2.0, mean loss 0.01583418773347062\n",
      "Episode 727 finished after 119 timesteps, total rewards 3.0, mean loss 0.017974633340500763\n",
      "Episode 728 finished after 166 timesteps, total rewards 4.0, mean loss 0.015833513545439052\n",
      "Episode 729 finished after 200 timesteps, total rewards 5.0, mean loss 0.015288745752841351\n",
      "Episode 730 finished after 166 timesteps, total rewards 2.0, mean loss 0.01689436055154607\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 731 finished after 150 timesteps, total rewards 2.0, mean loss 0.018230886670256345\n",
      "Episode 732 finished after 139 timesteps, total rewards 1.0, mean loss 0.017243821508705067\n",
      "Episode 733 finished after 162 timesteps, total rewards 2.0, mean loss 0.014267759794300353\n",
      "Episode 734 finished after 158 timesteps, total rewards 1.0, mean loss 0.015969852451096906\n",
      "Episode 735 finished after 168 timesteps, total rewards 2.0, mean loss 0.017164100216350994\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 736 finished after 178 timesteps, total rewards 1.0, mean loss 0.01669878021918899\n",
      "Episode 737 finished after 158 timesteps, total rewards 4.0, mean loss 0.012370875687246312\n",
      "Episode 738 finished after 413 timesteps, total rewards 7.0, mean loss 0.015955447023724586\n",
      "Episode 739 finished after 130 timesteps, total rewards 1.0, mean loss 0.016420642200346964\n",
      "Episode 740 finished after 171 timesteps, total rewards 5.0, mean loss 0.015703875964476144\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 130.000000\n",
      "---------------------------------------\n",
      "Episode 741 finished after 223 timesteps, total rewards 3.0, mean loss 0.01784143809724031\n",
      "Episode 742 finished after 286 timesteps, total rewards 6.0, mean loss 0.015573112094370645\n",
      "Episode 743 finished after 153 timesteps, total rewards 3.0, mean loss 0.020135444186641562\n",
      "Episode 744 finished after 393 timesteps, total rewards 8.0, mean loss 0.015181084683171767\n",
      "Episode 745 finished after 88 timesteps, total rewards 0.0, mean loss 0.01814988435382309\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 746 finished after 93 timesteps, total rewards 1.0, mean loss 0.013753762533631607\n",
      "Episode 747 finished after 357 timesteps, total rewards 9.0, mean loss 0.01649863330289625\n",
      "Episode 748 finished after 164 timesteps, total rewards 2.0, mean loss 0.01866568135248111\n",
      "Episode 749 finished after 252 timesteps, total rewards 1.0, mean loss 0.016165579380622293\n",
      "Episode 750 finished after 179 timesteps, total rewards 0.0, mean loss 0.01636545498925001\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 751 finished after 261 timesteps, total rewards 4.0, mean loss 0.012549821555015\n",
      "Episode 752 finished after 91 timesteps, total rewards 1.0, mean loss 0.014775566761202823\n",
      "Episode 753 finished after 236 timesteps, total rewards 1.0, mean loss 0.01662493571582333\n",
      "Episode 754 finished after 232 timesteps, total rewards 3.0, mean loss 0.015532167229296113\n",
      "Episode 755 finished after 202 timesteps, total rewards 2.0, mean loss 0.01503253611339784\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 756 finished after 99 timesteps, total rewards 1.0, mean loss 0.016683168877433572\n",
      "Episode 757 finished after 305 timesteps, total rewards 4.0, mean loss 0.015703373824314932\n",
      "Episode 758 finished after 181 timesteps, total rewards 2.0, mean loss 0.014882826761668034\n",
      "Episode 759 finished after 150 timesteps, total rewards 2.0, mean loss 0.014805400632612873\n",
      "Episode 760 finished after 148 timesteps, total rewards 5.0, mean loss 0.01412300625304704\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 761 finished after 198 timesteps, total rewards 4.0, mean loss 0.015102823231677345\n",
      "Episode 762 finished after 237 timesteps, total rewards 7.0, mean loss 0.01648333854046169\n",
      "Episode 763 finished after 260 timesteps, total rewards 4.0, mean loss 0.015592721799755684\n",
      "Episode 764 finished after 208 timesteps, total rewards 6.0, mean loss 0.014827646436970099\n",
      "Episode 765 finished after 158 timesteps, total rewards 0.0, mean loss 0.01532751502558704\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 85.000000\n",
      "---------------------------------------\n",
      "Episode 766 finished after 213 timesteps, total rewards 5.0, mean loss 0.015228566355811171\n",
      "Episode 767 finished after 202 timesteps, total rewards 2.0, mean loss 0.015347466380064524\n",
      "Episode 768 finished after 129 timesteps, total rewards 2.0, mean loss 0.015473964890779192\n",
      "Episode 769 finished after 167 timesteps, total rewards 2.0, mean loss 0.015703528305281423\n",
      "Episode 770 finished after 199 timesteps, total rewards 4.0, mean loss 0.019613195285575473\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 771 finished after 193 timesteps, total rewards 5.0, mean loss 0.01739555378114247\n",
      "Episode 772 finished after 135 timesteps, total rewards 2.0, mean loss 0.016834585220742694\n",
      "Episode 773 finished after 193 timesteps, total rewards 4.0, mean loss 0.017266938824087762\n",
      "Episode 774 finished after 209 timesteps, total rewards 2.0, mean loss 0.0162931462532883\n",
      "Episode 775 finished after 178 timesteps, total rewards 4.0, mean loss 0.015399270442450482\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 776 finished after 277 timesteps, total rewards 3.0, mean loss 0.017938261771816694\n",
      "Episode 777 finished after 142 timesteps, total rewards 3.0, mean loss 0.014152219545765957\n",
      "Episode 778 finished after 275 timesteps, total rewards 5.0, mean loss 0.017325722128152848\n",
      "Episode 779 finished after 145 timesteps, total rewards 2.0, mean loss 0.01306358324654866\n",
      "Episode 780 finished after 175 timesteps, total rewards 5.0, mean loss 0.016051235991430334\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 781 finished after 194 timesteps, total rewards 2.0, mean loss 0.01396576541949414\n",
      "Episode 782 finished after 156 timesteps, total rewards 4.0, mean loss 0.015252152135732243\n",
      "Episode 783 finished after 173 timesteps, total rewards 2.0, mean loss 0.01472446218054241\n",
      "Episode 784 finished after 222 timesteps, total rewards 3.0, mean loss 0.016268848973276042\n",
      "Episode 785 finished after 169 timesteps, total rewards 1.0, mean loss 0.016271450097094697\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 786 finished after 170 timesteps, total rewards 0.0, mean loss 0.01808167943801142\n",
      "Episode 787 finished after 172 timesteps, total rewards 5.0, mean loss 0.015940380980007534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 788 finished after 160 timesteps, total rewards 2.0, mean loss 0.016052024726923263\n",
      "Episode 789 finished after 180 timesteps, total rewards 5.0, mean loss 0.018109996439630374\n",
      "Episode 790 finished after 273 timesteps, total rewards 8.0, mean loss 0.015280368847362254\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 791 finished after 299 timesteps, total rewards 4.0, mean loss 0.014360733376728312\n",
      "Episode 792 finished after 215 timesteps, total rewards 2.0, mean loss 0.015484940200545933\n",
      "Episode 793 finished after 237 timesteps, total rewards 4.0, mean loss 0.01545199783555014\n",
      "Episode 794 finished after 158 timesteps, total rewards 4.0, mean loss 0.015636756121699662\n",
      "Episode 795 finished after 166 timesteps, total rewards 2.0, mean loss 0.011095776510002731\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 796 finished after 133 timesteps, total rewards 1.0, mean loss 0.014160310493517821\n",
      "Episode 797 finished after 246 timesteps, total rewards 5.0, mean loss 0.01594771220076903\n",
      "Episode 798 finished after 119 timesteps, total rewards 1.0, mean loss 0.013837374172426815\n",
      "Episode 799 finished after 157 timesteps, total rewards 1.0, mean loss 0.013984787057837542\n",
      "Episode 800 finished after 166 timesteps, total rewards 1.0, mean loss 0.016790400148830126\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 801 finished after 182 timesteps, total rewards 3.0, mean loss 0.01632465783376784\n",
      "Episode 802 finished after 137 timesteps, total rewards 4.0, mean loss 0.012852674303193964\n",
      "Episode 803 finished after 130 timesteps, total rewards 4.0, mean loss 0.017544440044730436\n",
      "Episode 804 finished after 216 timesteps, total rewards 2.0, mean loss 0.01680299310660134\n",
      "Episode 805 finished after 171 timesteps, total rewards 4.0, mean loss 0.015287803972414995\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 806 finished after 76 timesteps, total rewards 1.0, mean loss 0.02183615472581311\n",
      "Episode 807 finished after 145 timesteps, total rewards 1.0, mean loss 0.016745335452664035\n",
      "Episode 808 finished after 106 timesteps, total rewards 4.0, mean loss 0.01500678641797386\n",
      "Episode 809 finished after 179 timesteps, total rewards 1.0, mean loss 0.016202137061829196\n",
      "Episode 810 finished after 330 timesteps, total rewards 7.0, mean loss 0.016667506902409843\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 811 finished after 225 timesteps, total rewards 7.0, mean loss 0.015975783654980154\n",
      "Episode 812 finished after 92 timesteps, total rewards 1.0, mean loss 0.01438534090361234\n",
      "Episode 813 finished after 232 timesteps, total rewards 5.0, mean loss 0.01670384672434011\n",
      "Episode 814 finished after 200 timesteps, total rewards 5.0, mean loss 0.01560698816410877\n",
      "Episode 815 finished after 220 timesteps, total rewards 6.0, mean loss 0.014829436221109724\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 105.000000\n",
      "---------------------------------------\n",
      "Episode 816 finished after 139 timesteps, total rewards 2.0, mean loss 0.016750264242408705\n",
      "Episode 817 finished after 285 timesteps, total rewards 6.0, mean loss 0.017280952453208893\n",
      "Episode 818 finished after 231 timesteps, total rewards 9.0, mean loss 0.014597230151361681\n",
      "Episode 819 finished after 158 timesteps, total rewards 3.0, mean loss 0.015930234512687762\n",
      "Episode 820 finished after 94 timesteps, total rewards 0.0, mean loss 0.017848464435875476\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 821 finished after 155 timesteps, total rewards 2.0, mean loss 0.016614315809584373\n",
      "Episode 822 finished after 298 timesteps, total rewards 5.0, mean loss 0.015608915280608926\n",
      "Episode 823 finished after 155 timesteps, total rewards 2.0, mean loss 0.014830689095035796\n",
      "Episode 824 finished after 286 timesteps, total rewards 6.0, mean loss 0.014187630967648579\n",
      "Episode 825 finished after 231 timesteps, total rewards 3.0, mean loss 0.01512145790099106\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 826 finished after 371 timesteps, total rewards 6.0, mean loss 0.014537763850682797\n",
      "Episode 827 finished after 267 timesteps, total rewards 6.0, mean loss 0.016164509173557214\n",
      "Episode 828 finished after 248 timesteps, total rewards 5.0, mean loss 0.015472112900639448\n",
      "Episode 829 finished after 154 timesteps, total rewards 4.0, mean loss 0.015042334449531921\n",
      "Episode 830 finished after 217 timesteps, total rewards 5.0, mean loss 0.016506597997882504\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 831 finished after 177 timesteps, total rewards 0.0, mean loss 0.01799754404885784\n",
      "Episode 832 finished after 174 timesteps, total rewards 4.0, mean loss 0.01651309963851637\n",
      "Episode 833 finished after 127 timesteps, total rewards 3.0, mean loss 0.016806921215845257\n",
      "Episode 834 finished after 94 timesteps, total rewards 1.0, mean loss 0.013461784831343585\n",
      "Episode 835 finished after 103 timesteps, total rewards 2.0, mean loss 0.01639177649732887\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 836 finished after 167 timesteps, total rewards 3.0, mean loss 0.017656991854369287\n",
      "Episode 837 finished after 237 timesteps, total rewards 8.0, mean loss 0.01750872567500369\n",
      "Episode 838 finished after 300 timesteps, total rewards 5.0, mean loss 0.015025686865556054\n",
      "Episode 839 finished after 120 timesteps, total rewards 2.0, mean loss 0.016114790144517124\n",
      "Episode 840 finished after 168 timesteps, total rewards 3.0, mean loss 0.01871111216765019\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 841 finished after 82 timesteps, total rewards 0.0, mean loss 0.022272201291521125\n",
      "Episode 842 finished after 129 timesteps, total rewards 2.0, mean loss 0.017781065094470618\n",
      "Episode 843 finished after 284 timesteps, total rewards 7.0, mean loss 0.018618026491693994\n",
      "Episode 844 finished after 235 timesteps, total rewards 4.0, mean loss 0.014735821162925121\n",
      "Episode 845 finished after 168 timesteps, total rewards 5.0, mean loss 0.017653639862406374\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 85.000000\n",
      "---------------------------------------\n",
      "Episode 846 finished after 173 timesteps, total rewards 1.0, mean loss 0.019512884529452837\n",
      "Episode 847 finished after 260 timesteps, total rewards 9.0, mean loss 0.01489098144307858\n",
      "Episode 848 finished after 201 timesteps, total rewards 4.0, mean loss 0.018130119954987627\n",
      "Episode 849 finished after 169 timesteps, total rewards 2.0, mean loss 0.015254540260593247\n",
      "Episode 850 finished after 85 timesteps, total rewards 0.0, mean loss 0.013145889326252097\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 81.666667\n",
      "---------------------------------------\n",
      "Episode 851 finished after 95 timesteps, total rewards 0.0, mean loss 0.015201415273814314\n",
      "Episode 852 finished after 104 timesteps, total rewards 2.0, mean loss 0.018489688972644778\n",
      "Episode 853 finished after 157 timesteps, total rewards 2.0, mean loss 0.01526940291127187\n",
      "Episode 854 finished after 144 timesteps, total rewards 3.0, mean loss 0.019817852738924557\n",
      "Episode 855 finished after 87 timesteps, total rewards 2.0, mean loss 0.019586633110750765\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 856 finished after 177 timesteps, total rewards 1.0, mean loss 0.017961767203691494\n",
      "Episode 857 finished after 126 timesteps, total rewards 4.0, mean loss 0.01512283383970729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 858 finished after 161 timesteps, total rewards 1.0, mean loss 0.016632489163373593\n",
      "Episode 859 finished after 95 timesteps, total rewards 2.0, mean loss 0.015529576479763675\n",
      "Episode 860 finished after 159 timesteps, total rewards 4.0, mean loss 0.018265389753673988\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 115.000000\n",
      "---------------------------------------\n",
      "Episode 861 finished after 80 timesteps, total rewards 1.0, mean loss 0.013422903030004818\n",
      "Episode 862 finished after 171 timesteps, total rewards 1.0, mean loss 0.017875708447638573\n",
      "Episode 863 finished after 207 timesteps, total rewards 5.0, mean loss 0.016443488094801024\n",
      "Episode 864 finished after 185 timesteps, total rewards 4.0, mean loss 0.017850671385609973\n",
      "Episode 865 finished after 277 timesteps, total rewards 6.0, mean loss 0.014342079881592553\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 866 finished after 171 timesteps, total rewards 1.0, mean loss 0.014514250483630056\n",
      "Episode 867 finished after 180 timesteps, total rewards 4.0, mean loss 0.018085608897995875\n",
      "Episode 868 finished after 273 timesteps, total rewards 6.0, mean loss 0.016392755348650284\n",
      "Episode 869 finished after 221 timesteps, total rewards 1.0, mean loss 0.01848857894321752\n",
      "Episode 870 finished after 226 timesteps, total rewards 4.0, mean loss 0.01791852916345161\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 871 finished after 171 timesteps, total rewards 2.0, mean loss 0.015056097008848197\n",
      "Episode 872 finished after 273 timesteps, total rewards 7.0, mean loss 0.01806096657628753\n",
      "Episode 873 finished after 245 timesteps, total rewards 5.0, mean loss 0.014704153436231332\n",
      "Episode 874 finished after 192 timesteps, total rewards 0.0, mean loss 0.013033208540302136\n",
      "Episode 875 finished after 250 timesteps, total rewards 4.0, mean loss 0.015273679837991948\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 876 finished after 283 timesteps, total rewards 8.0, mean loss 0.015120543313995103\n",
      "Episode 877 finished after 99 timesteps, total rewards 0.0, mean loss 0.01813220242602807\n",
      "Episode 878 finished after 125 timesteps, total rewards 3.0, mean loss 0.017462068121647464\n",
      "Episode 879 finished after 170 timesteps, total rewards 2.0, mean loss 0.016565179075319868\n",
      "Episode 880 finished after 186 timesteps, total rewards 2.0, mean loss 0.018454950539268784\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 881 finished after 208 timesteps, total rewards 4.0, mean loss 0.01523311780622932\n",
      "Episode 882 finished after 163 timesteps, total rewards 2.0, mean loss 0.017886869845668316\n",
      "Episode 883 finished after 213 timesteps, total rewards 2.0, mean loss 0.014757477074556552\n",
      "Episode 884 finished after 176 timesteps, total rewards 3.0, mean loss 0.013645596072902581\n",
      "Episode 885 finished after 284 timesteps, total rewards 6.0, mean loss 0.015906502952392158\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 81.666667\n",
      "---------------------------------------\n",
      "Episode 886 finished after 97 timesteps, total rewards 1.0, mean loss 0.013985611815947666\n",
      "Episode 887 finished after 270 timesteps, total rewards 4.0, mean loss 0.015006854794369752\n",
      "Episode 888 finished after 93 timesteps, total rewards 1.0, mean loss 0.013752683352645217\n",
      "Episode 889 finished after 156 timesteps, total rewards 2.0, mean loss 0.014292674318055969\n",
      "Episode 890 finished after 124 timesteps, total rewards 2.0, mean loss 0.0195033581382426\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 891 finished after 210 timesteps, total rewards 5.0, mean loss 0.016046558392588936\n",
      "Episode 892 finished after 191 timesteps, total rewards 1.0, mean loss 0.015532693186901626\n",
      "Episode 893 finished after 158 timesteps, total rewards 1.0, mean loss 0.015489386132501266\n",
      "Episode 894 finished after 194 timesteps, total rewards 2.0, mean loss 0.017133884248473757\n",
      "Episode 895 finished after 159 timesteps, total rewards 1.0, mean loss 0.01505648442143179\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 896 finished after 105 timesteps, total rewards 3.0, mean loss 0.014424680578078898\n",
      "Episode 897 finished after 156 timesteps, total rewards 3.0, mean loss 0.014446987344481152\n",
      "Episode 898 finished after 193 timesteps, total rewards 5.0, mean loss 0.013892090416886153\n",
      "Episode 899 finished after 98 timesteps, total rewards 1.0, mean loss 0.014891824984168087\n",
      "Episode 900 finished after 248 timesteps, total rewards 4.0, mean loss 0.01753262875770874\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 145.000000\n",
      "---------------------------------------\n",
      "Episode 901 finished after 155 timesteps, total rewards 2.0, mean loss 0.014370010669752504\n",
      "Episode 902 finished after 207 timesteps, total rewards 6.0, mean loss 0.016369964984096003\n",
      "Episode 903 finished after 175 timesteps, total rewards 2.0, mean loss 0.018286808171376054\n",
      "Episode 904 finished after 157 timesteps, total rewards 1.0, mean loss 0.01735750801311094\n",
      "Episode 905 finished after 93 timesteps, total rewards 1.0, mean loss 0.016704000256923077\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 906 finished after 152 timesteps, total rewards 1.0, mean loss 0.015202821042916375\n",
      "Episode 907 finished after 90 timesteps, total rewards 1.0, mean loss 0.01991709116054052\n",
      "Episode 908 finished after 118 timesteps, total rewards 3.0, mean loss 0.014803627146540162\n",
      "Episode 909 finished after 85 timesteps, total rewards 1.0, mean loss 0.018419965147040786\n",
      "Episode 910 finished after 299 timesteps, total rewards 9.0, mean loss 0.012954320214320065\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 911 finished after 138 timesteps, total rewards 1.0, mean loss 0.016676990842561412\n",
      "Episode 912 finished after 315 timesteps, total rewards 4.0, mean loss 0.016141858648153998\n",
      "Episode 913 finished after 214 timesteps, total rewards 4.0, mean loss 0.014766481669582143\n",
      "Episode 914 finished after 87 timesteps, total rewards 2.0, mean loss 0.014519243672069955\n",
      "Episode 915 finished after 166 timesteps, total rewards 2.0, mean loss 0.014681493830414775\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 916 finished after 231 timesteps, total rewards 5.0, mean loss 0.016226121034721132\n",
      "Episode 917 finished after 158 timesteps, total rewards 6.0, mean loss 0.01583399174162452\n",
      "Episode 918 finished after 176 timesteps, total rewards 3.0, mean loss 0.018074577005625706\n",
      "Episode 919 finished after 175 timesteps, total rewards 4.0, mean loss 0.01546427764580585\n",
      "Episode 920 finished after 228 timesteps, total rewards 1.0, mean loss 0.01674924662687485\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 921 finished after 148 timesteps, total rewards 0.0, mean loss 0.016546885585813544\n",
      "Episode 922 finished after 155 timesteps, total rewards 3.0, mean loss 0.018469095725430956\n",
      "Episode 923 finished after 128 timesteps, total rewards 1.0, mean loss 0.01599206811761178\n",
      "Episode 924 finished after 129 timesteps, total rewards 0.0, mean loss 0.014610870502159982\n",
      "Episode 925 finished after 156 timesteps, total rewards 1.0, mean loss 0.014200106517259533\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 926 finished after 208 timesteps, total rewards 2.0, mean loss 0.015653791708385913\n",
      "Episode 927 finished after 229 timesteps, total rewards 4.0, mean loss 0.017324658265471048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 928 finished after 306 timesteps, total rewards 6.0, mean loss 0.01639294968928428\n",
      "Episode 929 finished after 207 timesteps, total rewards 3.0, mean loss 0.015826959928226598\n",
      "Episode 930 finished after 285 timesteps, total rewards 8.0, mean loss 0.015433912876362324\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 931 finished after 91 timesteps, total rewards 3.0, mean loss 0.019526442445067505\n",
      "Episode 932 finished after 163 timesteps, total rewards 3.0, mean loss 0.016308359213264807\n",
      "Episode 933 finished after 158 timesteps, total rewards 3.0, mean loss 0.019326748495333522\n",
      "Episode 934 finished after 214 timesteps, total rewards 6.0, mean loss 0.018168176199031\n",
      "Episode 935 finished after 142 timesteps, total rewards 3.0, mean loss 0.015240663084985269\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 936 finished after 285 timesteps, total rewards 5.0, mean loss 0.014476940103269411\n",
      "Episode 937 finished after 239 timesteps, total rewards 4.0, mean loss 0.015638227426450872\n",
      "Episode 938 finished after 211 timesteps, total rewards 3.0, mean loss 0.014801329043164179\n",
      "Episode 939 finished after 131 timesteps, total rewards 3.0, mean loss 0.013679740649873732\n",
      "Episode 940 finished after 334 timesteps, total rewards 5.0, mean loss 0.016109700662988058\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 941 finished after 160 timesteps, total rewards 2.0, mean loss 0.017733866695834877\n",
      "Episode 942 finished after 142 timesteps, total rewards 0.0, mean loss 0.016562286257320806\n",
      "Episode 943 finished after 224 timesteps, total rewards 4.0, mean loss 0.016690106216693885\n",
      "Episode 944 finished after 158 timesteps, total rewards 4.0, mean loss 0.018312671996728234\n",
      "Episode 945 finished after 109 timesteps, total rewards 1.0, mean loss 0.017340436619183115\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 946 finished after 85 timesteps, total rewards 1.0, mean loss 0.02122841677046381\n",
      "Episode 947 finished after 200 timesteps, total rewards 3.0, mean loss 0.017005648785998348\n",
      "Episode 948 finished after 133 timesteps, total rewards 2.0, mean loss 0.014504882510855646\n",
      "Episode 949 finished after 87 timesteps, total rewards 0.0, mean loss 0.014901810350771672\n",
      "Episode 950 finished after 124 timesteps, total rewards 3.0, mean loss 0.01652344765898306\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 951 finished after 234 timesteps, total rewards 5.0, mean loss 0.01658921661830515\n",
      "Episode 952 finished after 221 timesteps, total rewards 3.0, mean loss 0.015794755081260024\n",
      "Episode 953 finished after 106 timesteps, total rewards 1.0, mean loss 0.015904710207185004\n",
      "Episode 954 finished after 238 timesteps, total rewards 5.0, mean loss 0.018072407953382445\n",
      "Episode 955 finished after 212 timesteps, total rewards 5.0, mean loss 0.017505858487911953\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 146.666667\n",
      "---------------------------------------\n",
      "Episode 956 finished after 136 timesteps, total rewards 2.0, mean loss 0.015966755678485187\n",
      "Episode 957 finished after 158 timesteps, total rewards 4.0, mean loss 0.0164062857847621\n",
      "Episode 958 finished after 169 timesteps, total rewards 1.0, mean loss 0.017802760625649353\n",
      "Episode 959 finished after 208 timesteps, total rewards 7.0, mean loss 0.0141664459045457\n",
      "Episode 960 finished after 217 timesteps, total rewards 6.0, mean loss 0.015687357428701473\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 961 finished after 232 timesteps, total rewards 2.0, mean loss 0.016784722918475443\n",
      "Episode 962 finished after 92 timesteps, total rewards 0.0, mean loss 0.016656874640751124\n",
      "Episode 963 finished after 260 timesteps, total rewards 3.0, mean loss 0.017107382825745913\n",
      "Episode 964 finished after 134 timesteps, total rewards 1.0, mean loss 0.017360142728186964\n",
      "Episode 965 finished after 128 timesteps, total rewards 1.0, mean loss 0.018505195700754484\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 966 finished after 164 timesteps, total rewards 3.0, mean loss 0.01596828567580371\n",
      "Episode 967 finished after 168 timesteps, total rewards 2.0, mean loss 0.014616854837922369\n",
      "Episode 968 finished after 109 timesteps, total rewards 4.0, mean loss 0.014186594492399108\n",
      "Episode 969 finished after 315 timesteps, total rewards 9.0, mean loss 0.017441323627569963\n",
      "Episode 970 finished after 344 timesteps, total rewards 10.0, mean loss 0.018363670126174923\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 971 finished after 210 timesteps, total rewards 4.0, mean loss 0.016989841140180248\n",
      "Episode 972 finished after 274 timesteps, total rewards 5.0, mean loss 0.014446944943892627\n",
      "Episode 973 finished after 92 timesteps, total rewards 0.0, mean loss 0.016814209668866723\n",
      "Episode 974 finished after 118 timesteps, total rewards 1.0, mean loss 0.018519872912231408\n",
      "Episode 975 finished after 264 timesteps, total rewards 6.0, mean loss 0.015591615376417229\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 976 finished after 170 timesteps, total rewards 1.0, mean loss 0.016764605922371095\n",
      "Episode 977 finished after 224 timesteps, total rewards 4.0, mean loss 0.019729488427661375\n",
      "Episode 978 finished after 234 timesteps, total rewards 4.0, mean loss 0.015607630376157781\n",
      "Episode 979 finished after 232 timesteps, total rewards 4.0, mean loss 0.015431348020338818\n",
      "Episode 980 finished after 161 timesteps, total rewards 3.0, mean loss 0.017614095006770778\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 120.000000\n",
      "---------------------------------------\n",
      "Episode 981 finished after 109 timesteps, total rewards 1.0, mean loss 0.014138842859452829\n",
      "Episode 982 finished after 177 timesteps, total rewards 3.0, mean loss 0.01732658183783895\n",
      "Episode 983 finished after 112 timesteps, total rewards 0.0, mean loss 0.014771369909794885\n",
      "Episode 984 finished after 87 timesteps, total rewards 1.0, mean loss 0.014311374922988175\n",
      "Episode 985 finished after 202 timesteps, total rewards 7.0, mean loss 0.01667996549559296\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 986 finished after 228 timesteps, total rewards 5.0, mean loss 0.015678598609315884\n",
      "Episode 987 finished after 156 timesteps, total rewards 1.0, mean loss 0.01601062191725643\n",
      "Episode 988 finished after 120 timesteps, total rewards 1.0, mean loss 0.015712891445825033\n",
      "Episode 989 finished after 193 timesteps, total rewards 4.0, mean loss 0.016490841560805256\n",
      "Episode 990 finished after 353 timesteps, total rewards 11.0, mean loss 0.015787227211105204\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 120.000000\n",
      "---------------------------------------\n",
      "Episode 991 finished after 161 timesteps, total rewards 2.0, mean loss 0.015080005475967075\n",
      "Episode 992 finished after 131 timesteps, total rewards 0.0, mean loss 0.01678559815005456\n",
      "Episode 993 finished after 84 timesteps, total rewards 1.0, mean loss 0.01607372517173644\n",
      "Episode 994 finished after 104 timesteps, total rewards 0.0, mean loss 0.01458166912771748\n",
      "Episode 995 finished after 175 timesteps, total rewards 2.0, mean loss 0.017919990186346695\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 996 finished after 249 timesteps, total rewards 4.0, mean loss 0.013371540786699396\n",
      "Episode 997 finished after 249 timesteps, total rewards 7.0, mean loss 0.014775535418795997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 998 finished after 101 timesteps, total rewards 1.0, mean loss 0.016101762845750326\n",
      "Episode 999 finished after 253 timesteps, total rewards 5.0, mean loss 0.015954807983125145\n",
      "Episode 1000 finished after 232 timesteps, total rewards 3.0, mean loss 0.016315874529341114\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 1001 finished after 247 timesteps, total rewards 4.0, mean loss 0.01900269410140277\n",
      "Episode 1002 finished after 166 timesteps, total rewards 5.0, mean loss 0.01708897440596091\n",
      "Episode 1003 finished after 161 timesteps, total rewards 2.0, mean loss 0.01796971088921021\n",
      "Episode 1004 finished after 155 timesteps, total rewards 2.0, mean loss 0.017671066502507236\n",
      "Episode 1005 finished after 185 timesteps, total rewards 6.0, mean loss 0.016316114309262737\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 1006 finished after 201 timesteps, total rewards 3.0, mean loss 0.01846067869235919\n",
      "Episode 1007 finished after 112 timesteps, total rewards 1.0, mean loss 0.017018532445685457\n",
      "Episode 1008 finished after 201 timesteps, total rewards 3.0, mean loss 0.01693471036249058\n",
      "Episode 1009 finished after 179 timesteps, total rewards 4.0, mean loss 0.01710145416547279\n",
      "Episode 1010 finished after 212 timesteps, total rewards 5.0, mean loss 0.016404743607535976\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 1011 finished after 160 timesteps, total rewards 3.0, mean loss 0.015017186595650855\n",
      "Episode 1012 finished after 309 timesteps, total rewards 4.0, mean loss 0.017670437798237813\n",
      "Episode 1013 finished after 95 timesteps, total rewards 0.0, mean loss 0.016216753351526628\n",
      "Episode 1014 finished after 151 timesteps, total rewards 2.0, mean loss 0.020067242620434207\n",
      "Episode 1015 finished after 254 timesteps, total rewards 6.0, mean loss 0.0178518656211104\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 1016 finished after 281 timesteps, total rewards 2.0, mean loss 0.01553985768723222\n",
      "Episode 1017 finished after 173 timesteps, total rewards 3.0, mean loss 0.019335760301109565\n",
      "Episode 1018 finished after 92 timesteps, total rewards 1.0, mean loss 0.019781856306982936\n",
      "Episode 1019 finished after 236 timesteps, total rewards 8.0, mean loss 0.01779451527792104\n",
      "Episode 1020 finished after 121 timesteps, total rewards 1.0, mean loss 0.015903580234475502\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 1021 finished after 312 timesteps, total rewards 5.0, mean loss 0.015516518529367227\n",
      "Episode 1022 finished after 266 timesteps, total rewards 6.0, mean loss 0.015001167893134883\n",
      "Episode 1023 finished after 256 timesteps, total rewards 3.0, mean loss 0.014738198868428753\n",
      "Episode 1024 finished after 309 timesteps, total rewards 7.0, mean loss 0.018368570516255164\n",
      "Episode 1025 finished after 244 timesteps, total rewards 6.0, mean loss 0.016903166245579908\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 106.666667\n",
      "---------------------------------------\n",
      "Episode 1026 finished after 254 timesteps, total rewards 6.0, mean loss 0.01568250896067241\n",
      "Episode 1027 finished after 89 timesteps, total rewards 1.0, mean loss 0.018644867089166295\n",
      "Episode 1028 finished after 162 timesteps, total rewards 1.0, mean loss 0.015665737778217143\n",
      "Episode 1029 finished after 203 timesteps, total rewards 1.0, mean loss 0.016922735983162907\n",
      "Episode 1030 finished after 163 timesteps, total rewards 4.0, mean loss 0.018973842381422575\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 1031 finished after 134 timesteps, total rewards 0.0, mean loss 0.01400751137125146\n",
      "Episode 1032 finished after 166 timesteps, total rewards 5.0, mean loss 0.013506827365076843\n",
      "Episode 1033 finished after 168 timesteps, total rewards 5.0, mean loss 0.01652064868747922\n",
      "Episode 1034 finished after 166 timesteps, total rewards 3.0, mean loss 0.018938236904585824\n",
      "Episode 1035 finished after 203 timesteps, total rewards 2.0, mean loss 0.014797854761700858\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 1036 finished after 243 timesteps, total rewards 7.0, mean loss 0.017269339418559\n",
      "Episode 1037 finished after 236 timesteps, total rewards 5.0, mean loss 0.015290804437689199\n",
      "Episode 1038 finished after 126 timesteps, total rewards 4.0, mean loss 0.01825207870350578\n",
      "Episode 1039 finished after 231 timesteps, total rewards 8.0, mean loss 0.01598627942534127\n",
      "Episode 1040 finished after 189 timesteps, total rewards 0.0, mean loss 0.015226287777300323\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 1041 finished after 175 timesteps, total rewards 3.0, mean loss 0.016992563067636055\n",
      "Episode 1042 finished after 106 timesteps, total rewards 1.0, mean loss 0.012712402807665057\n",
      "Episode 1043 finished after 109 timesteps, total rewards 4.0, mean loss 0.014256660097840438\n",
      "Episode 1044 finished after 199 timesteps, total rewards 1.0, mean loss 0.016068545153163298\n",
      "Episode 1045 finished after 184 timesteps, total rewards 4.0, mean loss 0.01939735195201533\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 1046 finished after 138 timesteps, total rewards 3.0, mean loss 0.01606917315697554\n",
      "Episode 1047 finished after 211 timesteps, total rewards 3.0, mean loss 0.016085370374243194\n",
      "Episode 1048 finished after 114 timesteps, total rewards 0.0, mean loss 0.012823872119489559\n",
      "Episode 1049 finished after 165 timesteps, total rewards 2.0, mean loss 0.015547258880211632\n",
      "Episode 1050 finished after 147 timesteps, total rewards 4.0, mean loss 0.02030453010605566\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 1051 finished after 94 timesteps, total rewards 2.0, mean loss 0.019999402893788893\n",
      "Episode 1052 finished after 210 timesteps, total rewards 4.0, mean loss 0.016433118580469107\n",
      "Episode 1053 finished after 129 timesteps, total rewards 2.0, mean loss 0.016573457619492974\n",
      "Episode 1054 finished after 294 timesteps, total rewards 8.0, mean loss 0.015503359447777046\n",
      "Episode 1055 finished after 155 timesteps, total rewards 0.0, mean loss 0.014940485763344764\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 1056 finished after 207 timesteps, total rewards 2.0, mean loss 0.016666721274366532\n",
      "Episode 1057 finished after 124 timesteps, total rewards 3.0, mean loss 0.019273764796502467\n",
      "Episode 1058 finished after 94 timesteps, total rewards 0.0, mean loss 0.015303178502747075\n",
      "Episode 1059 finished after 244 timesteps, total rewards 4.0, mean loss 0.016713197449864877\n",
      "Episode 1060 finished after 154 timesteps, total rewards 6.0, mean loss 0.011652565627474513\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 1061 finished after 99 timesteps, total rewards 2.0, mean loss 0.016284337899479497\n",
      "Episode 1062 finished after 304 timesteps, total rewards 4.0, mean loss 0.01841244751602445\n",
      "Episode 1063 finished after 174 timesteps, total rewards 1.0, mean loss 0.017891110429853394\n",
      "Episode 1064 finished after 228 timesteps, total rewards 3.0, mean loss 0.017125118271092345\n",
      "Episode 1065 finished after 109 timesteps, total rewards 0.0, mean loss 0.01940113931911391\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 1066 finished after 148 timesteps, total rewards 2.0, mean loss 0.013010514219982922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1067 finished after 98 timesteps, total rewards 1.0, mean loss 0.01581991635255839\n",
      "Episode 1068 finished after 216 timesteps, total rewards 3.0, mean loss 0.01686037034738143\n",
      "Episode 1069 finished after 295 timesteps, total rewards 4.0, mean loss 0.015619207097866603\n",
      "Episode 1070 finished after 141 timesteps, total rewards 1.0, mean loss 0.014366445436880508\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 10.000000\n",
      "---------------------------------------\n",
      "Episode 1071 finished after 170 timesteps, total rewards 3.0, mean loss 0.016093598844942308\n",
      "Episode 1072 finished after 208 timesteps, total rewards 2.0, mean loss 0.016457295944816836\n",
      "Episode 1073 finished after 207 timesteps, total rewards 4.0, mean loss 0.017647936362475112\n",
      "Episode 1074 finished after 200 timesteps, total rewards 4.0, mean loss 0.02121334923125687\n",
      "Episode 1075 finished after 125 timesteps, total rewards 1.0, mean loss 0.015881795924389736\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 1076 finished after 162 timesteps, total rewards 2.0, mean loss 0.016675291846673015\n",
      "Episode 1077 finished after 94 timesteps, total rewards 0.0, mean loss 0.016500729884818554\n",
      "Episode 1078 finished after 126 timesteps, total rewards 1.0, mean loss 0.01473571365793413\n",
      "Episode 1079 finished after 113 timesteps, total rewards 1.0, mean loss 0.014045200925176155\n",
      "Episode 1080 finished after 174 timesteps, total rewards 2.0, mean loss 0.01612102389021177\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 1081 finished after 169 timesteps, total rewards 1.0, mean loss 0.015060383629828096\n",
      "Episode 1082 finished after 141 timesteps, total rewards 1.0, mean loss 0.016060455331326204\n",
      "Episode 1083 finished after 194 timesteps, total rewards 2.0, mean loss 0.016832107846827975\n",
      "Episode 1084 finished after 137 timesteps, total rewards 1.0, mean loss 0.014666339665494969\n",
      "Episode 1085 finished after 248 timesteps, total rewards 3.0, mean loss 0.0173561648185686\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 1086 finished after 173 timesteps, total rewards 1.0, mean loss 0.014654639730333603\n",
      "Episode 1087 finished after 290 timesteps, total rewards 6.0, mean loss 0.015894152134042716\n",
      "Episode 1088 finished after 297 timesteps, total rewards 5.0, mean loss 0.015904632534920738\n",
      "Episode 1089 finished after 194 timesteps, total rewards 4.0, mean loss 0.016255199908535117\n",
      "Episode 1090 finished after 132 timesteps, total rewards 1.0, mean loss 0.015526783485477084\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 1091 finished after 103 timesteps, total rewards 2.0, mean loss 0.01718858953280537\n",
      "Episode 1092 finished after 203 timesteps, total rewards 5.0, mean loss 0.018615943671813753\n",
      "Episode 1093 finished after 101 timesteps, total rewards 1.0, mean loss 0.017865963196059192\n",
      "Episode 1094 finished after 177 timesteps, total rewards 5.0, mean loss 0.015588267184603212\n",
      "Episode 1095 finished after 95 timesteps, total rewards 2.0, mean loss 0.01673210596506435\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1096 finished after 114 timesteps, total rewards 1.0, mean loss 0.015456319378133406\n",
      "Episode 1097 finished after 109 timesteps, total rewards 4.0, mean loss 0.014083302587563377\n",
      "Episode 1098 finished after 181 timesteps, total rewards 1.0, mean loss 0.01801283884062226\n",
      "Episode 1099 finished after 286 timesteps, total rewards 7.0, mean loss 0.015778646377095192\n",
      "Episode 1100 finished after 276 timesteps, total rewards 3.0, mean loss 0.016828876028370545\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 1101 finished after 89 timesteps, total rewards 2.0, mean loss 0.01393041126475816\n",
      "Episode 1102 finished after 374 timesteps, total rewards 5.0, mean loss 0.014594929285755539\n",
      "Episode 1103 finished after 255 timesteps, total rewards 6.0, mean loss 0.017943129266552883\n",
      "Episode 1104 finished after 305 timesteps, total rewards 10.0, mean loss 0.015627896824000464\n",
      "Episode 1105 finished after 202 timesteps, total rewards 3.0, mean loss 0.01581629063896051\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 105.000000\n",
      "---------------------------------------\n",
      "Episode 1106 finished after 129 timesteps, total rewards 0.0, mean loss 0.016115869943444192\n",
      "Episode 1107 finished after 172 timesteps, total rewards 4.0, mean loss 0.017484723927715773\n",
      "Episode 1108 finished after 142 timesteps, total rewards 2.0, mean loss 0.020428959941442296\n",
      "Episode 1109 finished after 127 timesteps, total rewards 1.0, mean loss 0.018236090570521415\n",
      "Episode 1110 finished after 152 timesteps, total rewards 2.0, mean loss 0.018447840702145567\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 1111 finished after 244 timesteps, total rewards 4.0, mean loss 0.016307898557703888\n",
      "Episode 1112 finished after 195 timesteps, total rewards 7.0, mean loss 0.01578662156049783\n",
      "Episode 1113 finished after 137 timesteps, total rewards 3.0, mean loss 0.014510460262718766\n",
      "Episode 1114 finished after 252 timesteps, total rewards 7.0, mean loss 0.017553161654163093\n",
      "Episode 1115 finished after 206 timesteps, total rewards 6.0, mean loss 0.01765037931111083\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 1116 finished after 202 timesteps, total rewards 2.0, mean loss 0.016524999751277167\n",
      "Episode 1117 finished after 157 timesteps, total rewards 2.0, mean loss 0.017378552115350297\n",
      "Episode 1118 finished after 101 timesteps, total rewards 1.0, mean loss 0.0159275866310126\n",
      "Episode 1119 finished after 291 timesteps, total rewards 6.0, mean loss 0.017204249682064132\n",
      "Episode 1120 finished after 140 timesteps, total rewards 0.0, mean loss 0.014523757141432725\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 1121 finished after 311 timesteps, total rewards 3.0, mean loss 0.014481463754988332\n",
      "Episode 1122 finished after 88 timesteps, total rewards 2.0, mean loss 0.01476540955561425\n",
      "Episode 1123 finished after 123 timesteps, total rewards 1.0, mean loss 0.018619929273741516\n",
      "Episode 1124 finished after 254 timesteps, total rewards 3.0, mean loss 0.01593825475204604\n",
      "Episode 1125 finished after 165 timesteps, total rewards 3.0, mean loss 0.01622406484375736\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 1126 finished after 151 timesteps, total rewards 4.0, mean loss 0.019192239998173682\n",
      "Episode 1127 finished after 128 timesteps, total rewards 4.0, mean loss 0.01574468463559242\n",
      "Episode 1128 finished after 214 timesteps, total rewards 2.0, mean loss 0.015914034700319258\n",
      "Episode 1129 finished after 166 timesteps, total rewards 4.0, mean loss 0.01760502118394284\n",
      "Episode 1130 finished after 302 timesteps, total rewards 0.0, mean loss 0.014995444210115913\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 1131 finished after 336 timesteps, total rewards 5.0, mean loss 0.015973240083223487\n",
      "Episode 1132 finished after 229 timesteps, total rewards 4.0, mean loss 0.015184004185428968\n",
      "Episode 1133 finished after 277 timesteps, total rewards 5.0, mean loss 0.01732588291476272\n",
      "Episode 1134 finished after 104 timesteps, total rewards 0.0, mean loss 0.01640007205423899\n",
      "Episode 1135 finished after 88 timesteps, total rewards 2.0, mean loss 0.012897779133874627\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1136 finished after 184 timesteps, total rewards 5.0, mean loss 0.014539132459148\n",
      "Episode 1137 finished after 173 timesteps, total rewards 3.0, mean loss 0.017414671617877707\n",
      "Episode 1138 finished after 222 timesteps, total rewards 4.0, mean loss 0.015286269511053179\n",
      "Episode 1139 finished after 319 timesteps, total rewards 2.0, mean loss 0.01641739882783509\n",
      "Episode 1140 finished after 128 timesteps, total rewards 1.0, mean loss 0.01567338647737415\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 1141 finished after 91 timesteps, total rewards 0.0, mean loss 0.018767215694994205\n",
      "Episode 1142 finished after 308 timesteps, total rewards 7.0, mean loss 0.01727283547153128\n",
      "Episode 1143 finished after 416 timesteps, total rewards 11.0, mean loss 0.01642892459158947\n",
      "Episode 1144 finished after 213 timesteps, total rewards 4.0, mean loss 0.014483295854382182\n",
      "Episode 1145 finished after 152 timesteps, total rewards 1.0, mean loss 0.01776414190393744\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 1146 finished after 134 timesteps, total rewards 1.0, mean loss 0.016125011382713123\n",
      "Episode 1147 finished after 137 timesteps, total rewards 1.0, mean loss 0.017394271697279706\n",
      "Episode 1148 finished after 108 timesteps, total rewards 0.0, mean loss 0.015338121499721805\n",
      "Episode 1149 finished after 87 timesteps, total rewards 0.0, mean loss 0.018950209575544656\n",
      "Episode 1150 finished after 202 timesteps, total rewards 2.0, mean loss 0.016811065614110523\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1151 finished after 122 timesteps, total rewards 1.0, mean loss 0.01535571465907107\n",
      "Episode 1152 finished after 219 timesteps, total rewards 1.0, mean loss 0.018042323577295367\n",
      "Episode 1153 finished after 180 timesteps, total rewards 3.0, mean loss 0.014633440386549207\n",
      "Episode 1154 finished after 233 timesteps, total rewards 4.0, mean loss 0.015298049945670055\n",
      "Episode 1155 finished after 126 timesteps, total rewards 1.0, mean loss 0.015930251774816065\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 3.333333\n",
      "---------------------------------------\n",
      "Episode 1156 finished after 256 timesteps, total rewards 4.0, mean loss 0.015203139714230929\n",
      "Episode 1157 finished after 281 timesteps, total rewards 4.0, mean loss 0.01604277837689671\n",
      "Episode 1158 finished after 160 timesteps, total rewards 3.0, mean loss 0.014386853165524372\n",
      "Episode 1159 finished after 152 timesteps, total rewards 3.0, mean loss 0.018337358617971308\n",
      "Episode 1160 finished after 131 timesteps, total rewards 2.0, mean loss 0.01806060679635488\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1161 finished after 200 timesteps, total rewards 8.0, mean loss 0.018065208344341954\n",
      "Episode 1162 finished after 275 timesteps, total rewards 6.0, mean loss 0.015631196910346097\n",
      "Episode 1163 finished after 95 timesteps, total rewards 1.0, mean loss 0.012370564463229752\n",
      "Episode 1164 finished after 118 timesteps, total rewards 1.0, mean loss 0.01620950278090076\n",
      "Episode 1165 finished after 206 timesteps, total rewards 3.0, mean loss 0.014101327948206673\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 1166 finished after 268 timesteps, total rewards 4.0, mean loss 0.01446255887906352\n",
      "Episode 1167 finished after 137 timesteps, total rewards 3.0, mean loss 0.01774581552404115\n",
      "Episode 1168 finished after 190 timesteps, total rewards 2.0, mean loss 0.0174542288726355\n",
      "Episode 1169 finished after 276 timesteps, total rewards 6.0, mean loss 0.01837866817737265\n",
      "Episode 1170 finished after 125 timesteps, total rewards 0.0, mean loss 0.013423925289651379\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 1171 finished after 161 timesteps, total rewards 4.0, mean loss 0.017035939751311513\n",
      "Episode 1172 finished after 241 timesteps, total rewards 6.0, mean loss 0.018349748085609432\n",
      "Episode 1173 finished after 317 timesteps, total rewards 6.0, mean loss 0.015762334200792302\n",
      "Episode 1174 finished after 103 timesteps, total rewards 1.0, mean loss 0.01867862107645883\n",
      "Episode 1175 finished after 82 timesteps, total rewards 2.0, mean loss 0.018543767728615643\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1176 finished after 162 timesteps, total rewards 0.0, mean loss 0.016607811220132537\n",
      "Episode 1177 finished after 194 timesteps, total rewards 3.0, mean loss 0.013119248420086488\n",
      "Episode 1178 finished after 99 timesteps, total rewards 0.0, mean loss 0.013387874819778113\n",
      "Episode 1179 finished after 211 timesteps, total rewards 2.0, mean loss 0.01538697307238629\n",
      "Episode 1180 finished after 327 timesteps, total rewards 4.0, mean loss 0.01487661577259733\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 1181 finished after 160 timesteps, total rewards 5.0, mean loss 0.015140496999265451\n",
      "Episode 1182 finished after 189 timesteps, total rewards 5.0, mean loss 0.01701021510119077\n",
      "Episode 1183 finished after 205 timesteps, total rewards 4.0, mean loss 0.01611684876119309\n",
      "Episode 1184 finished after 111 timesteps, total rewards 1.0, mean loss 0.016033012718843245\n",
      "Episode 1185 finished after 171 timesteps, total rewards 2.0, mean loss 0.016976075661021463\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 1186 finished after 124 timesteps, total rewards 1.0, mean loss 0.011508034461228582\n",
      "Episode 1187 finished after 284 timesteps, total rewards 7.0, mean loss 0.015504615004496193\n",
      "Episode 1188 finished after 105 timesteps, total rewards 1.0, mean loss 0.015055359947395378\n",
      "Episode 1189 finished after 133 timesteps, total rewards 0.0, mean loss 0.0162000318698557\n",
      "Episode 1190 finished after 87 timesteps, total rewards 1.0, mean loss 0.010959941026722562\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 1191 finished after 167 timesteps, total rewards 4.0, mean loss 0.02117636253705951\n",
      "Episode 1192 finished after 97 timesteps, total rewards 1.0, mean loss 0.014500451452139592\n",
      "Episode 1193 finished after 118 timesteps, total rewards 0.0, mean loss 0.016188517828487743\n",
      "Episode 1194 finished after 199 timesteps, total rewards 6.0, mean loss 0.016470753805987986\n",
      "Episode 1195 finished after 174 timesteps, total rewards 2.0, mean loss 0.014880867961468978\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 1196 finished after 158 timesteps, total rewards 2.0, mean loss 0.01808234476068063\n",
      "Episode 1197 finished after 162 timesteps, total rewards 1.0, mean loss 0.017499845292071388\n",
      "Episode 1198 finished after 174 timesteps, total rewards 2.0, mean loss 0.015008177667964603\n",
      "Episode 1199 finished after 102 timesteps, total rewards 0.0, mean loss 0.00973862799240605\n",
      "Episode 1200 finished after 122 timesteps, total rewards 3.0, mean loss 0.011793006931242656\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 88.333333\n",
      "---------------------------------------\n",
      "Episode 1201 finished after 92 timesteps, total rewards 2.0, mean loss 0.018644050785160685\n",
      "Episode 1202 finished after 102 timesteps, total rewards 4.0, mean loss 0.023220974628987045\n",
      "Episode 1203 finished after 132 timesteps, total rewards 1.0, mean loss 0.01727494026404615\n",
      "Episode 1204 finished after 185 timesteps, total rewards 4.0, mean loss 0.01494092846515815\n",
      "Episode 1205 finished after 90 timesteps, total rewards 1.0, mean loss 0.018264223005260444\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1206 finished after 91 timesteps, total rewards 1.0, mean loss 0.015257312667397474\n",
      "Episode 1207 finished after 86 timesteps, total rewards 2.0, mean loss 0.019771696885284254\n",
      "Episode 1208 finished after 143 timesteps, total rewards 1.0, mean loss 0.014365177413520332\n",
      "Episode 1209 finished after 150 timesteps, total rewards 1.0, mean loss 0.01657241191113523\n",
      "Episode 1210 finished after 205 timesteps, total rewards 1.0, mean loss 0.016845071816493234\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 1211 finished after 171 timesteps, total rewards 1.0, mean loss 0.01781940042573621\n",
      "Episode 1212 finished after 119 timesteps, total rewards 1.0, mean loss 0.01388726441117254\n",
      "Episode 1213 finished after 278 timesteps, total rewards 8.0, mean loss 0.01713297700121238\n",
      "Episode 1214 finished after 95 timesteps, total rewards 0.0, mean loss 0.018485006334345886\n",
      "Episode 1215 finished after 174 timesteps, total rewards 3.0, mean loss 0.017988128426696184\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 1216 finished after 235 timesteps, total rewards 4.0, mean loss 0.01600558654110423\n",
      "Episode 1217 finished after 259 timesteps, total rewards 3.0, mean loss 0.015628007630798264\n",
      "Episode 1218 finished after 97 timesteps, total rewards 0.0, mean loss 0.016492109787746418\n",
      "Episode 1219 finished after 209 timesteps, total rewards 4.0, mean loss 0.016030689008376876\n",
      "Episode 1220 finished after 161 timesteps, total rewards 0.0, mean loss 0.013990972000022379\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 105.000000\n",
      "---------------------------------------\n",
      "Episode 1221 finished after 149 timesteps, total rewards 1.0, mean loss 0.017053586415221426\n",
      "Episode 1222 finished after 110 timesteps, total rewards 3.0, mean loss 0.015727323633671568\n",
      "Episode 1223 finished after 127 timesteps, total rewards 0.0, mean loss 0.015800529452935447\n",
      "Episode 1224 finished after 289 timesteps, total rewards 7.0, mean loss 0.01616946686337989\n",
      "Episode 1225 finished after 101 timesteps, total rewards 0.0, mean loss 0.01626374483734397\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 93.333333\n",
      "---------------------------------------\n",
      "Episode 1226 finished after 247 timesteps, total rewards 4.0, mean loss 0.015658726224471696\n",
      "Episode 1227 finished after 262 timesteps, total rewards 6.0, mean loss 0.02019019603937196\n",
      "Episode 1228 finished after 196 timesteps, total rewards 4.0, mean loss 0.016442486289169934\n",
      "Episode 1229 finished after 212 timesteps, total rewards 5.0, mean loss 0.014298925105287949\n",
      "Episode 1230 finished after 202 timesteps, total rewards 4.0, mean loss 0.014420961251708282\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 1231 finished after 79 timesteps, total rewards 1.0, mean loss 0.015573421510671785\n",
      "Episode 1232 finished after 171 timesteps, total rewards 6.0, mean loss 0.015538889457769855\n",
      "Episode 1233 finished after 180 timesteps, total rewards 1.0, mean loss 0.017406792772332362\n",
      "Episode 1234 finished after 248 timesteps, total rewards 2.0, mean loss 0.013310547558508152\n",
      "Episode 1235 finished after 273 timesteps, total rewards 6.0, mean loss 0.01750633434154564\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 1236 finished after 287 timesteps, total rewards 3.0, mean loss 0.015062403998306047\n",
      "Episode 1237 finished after 227 timesteps, total rewards 2.0, mean loss 0.01584054302221107\n",
      "Episode 1238 finished after 90 timesteps, total rewards 3.0, mean loss 0.011682061162880725\n",
      "Episode 1239 finished after 112 timesteps, total rewards 3.0, mean loss 0.018591365463310337\n",
      "Episode 1240 finished after 122 timesteps, total rewards 1.0, mean loss 0.015436988782431533\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 1241 finished after 131 timesteps, total rewards 2.0, mean loss 0.01779777808950233\n",
      "Episode 1242 finished after 92 timesteps, total rewards 1.0, mean loss 0.01596424835904896\n",
      "Episode 1243 finished after 162 timesteps, total rewards 3.0, mean loss 0.015342824145063563\n",
      "Episode 1244 finished after 127 timesteps, total rewards 0.0, mean loss 0.013351033492596805\n",
      "Episode 1245 finished after 98 timesteps, total rewards 0.0, mean loss 0.013753370411144047\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 1246 finished after 226 timesteps, total rewards 8.0, mean loss 0.01571662829268365\n",
      "Episode 1247 finished after 132 timesteps, total rewards 1.0, mean loss 0.017086846731686048\n",
      "Episode 1248 finished after 98 timesteps, total rewards 0.0, mean loss 0.014637283209000942\n",
      "Episode 1249 finished after 220 timesteps, total rewards 5.0, mean loss 0.017060373282716187\n",
      "Episode 1250 finished after 251 timesteps, total rewards 4.0, mean loss 0.014764767232911282\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 1251 finished after 284 timesteps, total rewards 4.0, mean loss 0.01866826495343432\n",
      "Episode 1252 finished after 93 timesteps, total rewards 0.0, mean loss 0.015434198353442074\n",
      "Episode 1253 finished after 210 timesteps, total rewards 1.0, mean loss 0.015230235329779265\n",
      "Episode 1254 finished after 244 timesteps, total rewards 4.0, mean loss 0.015865777883622374\n",
      "Episode 1255 finished after 218 timesteps, total rewards 7.0, mean loss 0.015191472617566475\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 1256 finished after 90 timesteps, total rewards 0.0, mean loss 0.01577098369727739\n",
      "Episode 1257 finished after 106 timesteps, total rewards 0.0, mean loss 0.012565225766661999\n",
      "Episode 1258 finished after 179 timesteps, total rewards 1.0, mean loss 0.016115374333588093\n",
      "Episode 1259 finished after 222 timesteps, total rewards 6.0, mean loss 0.01656727793955282\n",
      "Episode 1260 finished after 263 timesteps, total rewards 4.0, mean loss 0.013959971368653672\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 1261 finished after 204 timesteps, total rewards 2.0, mean loss 0.0163101425714558\n",
      "Episode 1262 finished after 275 timesteps, total rewards 4.0, mean loss 0.01372782364985059\n",
      "Episode 1263 finished after 183 timesteps, total rewards 4.0, mean loss 0.013852182585686252\n",
      "Episode 1264 finished after 135 timesteps, total rewards 4.0, mean loss 0.014225884507996616\n",
      "Episode 1265 finished after 197 timesteps, total rewards 7.0, mean loss 0.015466825167741862\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 101.666667\n",
      "---------------------------------------\n",
      "Episode 1266 finished after 93 timesteps, total rewards 3.0, mean loss 0.014120721335520326\n",
      "Episode 1267 finished after 83 timesteps, total rewards 1.0, mean loss 0.01716358324667826\n",
      "Episode 1268 finished after 121 timesteps, total rewards 1.0, mean loss 0.016211104337539722\n",
      "Episode 1269 finished after 199 timesteps, total rewards 4.0, mean loss 0.01516660207683038\n",
      "Episode 1270 finished after 95 timesteps, total rewards 0.0, mean loss 0.01687465168805303\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 1271 finished after 231 timesteps, total rewards 4.0, mean loss 0.016492495398708413\n",
      "Episode 1272 finished after 220 timesteps, total rewards 4.0, mean loss 0.015552134987691799\n",
      "Episode 1273 finished after 389 timesteps, total rewards 9.0, mean loss 0.014757963653072083\n",
      "Episode 1274 finished after 255 timesteps, total rewards 3.0, mean loss 0.017714169556982632\n",
      "Episode 1275 finished after 100 timesteps, total rewards 3.0, mean loss 0.017632223706459626\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1276 finished after 200 timesteps, total rewards 3.0, mean loss 0.01570235563442111\n",
      "Episode 1277 finished after 351 timesteps, total rewards 2.0, mean loss 0.014506565724904821\n",
      "Episode 1278 finished after 149 timesteps, total rewards 4.0, mean loss 0.015195399447775527\n",
      "Episode 1279 finished after 186 timesteps, total rewards 1.0, mean loss 0.015836231698412247\n",
      "Episode 1280 finished after 94 timesteps, total rewards 1.0, mean loss 0.01262572723282303\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 1281 finished after 161 timesteps, total rewards 3.0, mean loss 0.015845815546363044\n",
      "Episode 1282 finished after 92 timesteps, total rewards 1.0, mean loss 0.016166837746783604\n",
      "Episode 1283 finished after 268 timesteps, total rewards 8.0, mean loss 0.01612432501567684\n",
      "Episode 1284 finished after 336 timesteps, total rewards 5.0, mean loss 0.015988569140366355\n",
      "Episode 1285 finished after 112 timesteps, total rewards 1.0, mean loss 0.01218979168658864\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 1286 finished after 88 timesteps, total rewards 1.0, mean loss 0.012982028676064643\n",
      "Episode 1287 finished after 312 timesteps, total rewards 4.0, mean loss 0.014556270826011687\n",
      "Episode 1288 finished after 140 timesteps, total rewards 4.0, mean loss 0.012869590015699421\n",
      "Episode 1289 finished after 335 timesteps, total rewards 5.0, mean loss 0.014753920460161545\n",
      "Episode 1290 finished after 234 timesteps, total rewards 1.0, mean loss 0.015109996218706926\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 1291 finished after 128 timesteps, total rewards 0.0, mean loss 0.013715781645259995\n",
      "Episode 1292 finished after 178 timesteps, total rewards 4.0, mean loss 0.017083334275115845\n",
      "Episode 1293 finished after 154 timesteps, total rewards 3.0, mean loss 0.01350463692581904\n",
      "Episode 1294 finished after 159 timesteps, total rewards 1.0, mean loss 0.017200895425506263\n",
      "Episode 1295 finished after 81 timesteps, total rewards 1.0, mean loss 0.015999100426137218\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 1296 finished after 103 timesteps, total rewards 0.0, mean loss 0.015792363140171448\n",
      "Episode 1297 finished after 180 timesteps, total rewards 6.0, mean loss 0.017158171652893846\n",
      "Episode 1298 finished after 226 timesteps, total rewards 3.0, mean loss 0.015187057857329553\n",
      "Episode 1299 finished after 121 timesteps, total rewards 0.0, mean loss 0.014325484621900418\n",
      "Episode 1300 finished after 99 timesteps, total rewards 3.0, mean loss 0.018269931644405886\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1301 finished after 343 timesteps, total rewards 9.0, mean loss 0.015239769858633963\n",
      "Episode 1302 finished after 125 timesteps, total rewards 5.0, mean loss 0.014277068959549069\n",
      "Episode 1303 finished after 215 timesteps, total rewards 6.0, mean loss 0.01440926598585319\n",
      "Episode 1304 finished after 244 timesteps, total rewards 9.0, mean loss 0.014678756409772236\n",
      "Episode 1305 finished after 231 timesteps, total rewards 6.0, mean loss 0.015036860704159936\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 1306 finished after 136 timesteps, total rewards 1.0, mean loss 0.01848950955172768\n",
      "Episode 1307 finished after 169 timesteps, total rewards 1.0, mean loss 0.01586895258101833\n",
      "Episode 1308 finished after 249 timesteps, total rewards 6.0, mean loss 0.015225451091425044\n",
      "Episode 1309 finished after 176 timesteps, total rewards 4.0, mean loss 0.01949859740720554\n",
      "Episode 1310 finished after 345 timesteps, total rewards 6.0, mean loss 0.014266912296390755\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 1311 finished after 154 timesteps, total rewards 5.0, mean loss 0.014511038944980903\n",
      "Episode 1312 finished after 91 timesteps, total rewards 1.0, mean loss 0.016537345085832916\n",
      "Episode 1313 finished after 78 timesteps, total rewards 1.0, mean loss 0.01657432868020036\n",
      "Episode 1314 finished after 130 timesteps, total rewards 1.0, mean loss 0.013843594074849254\n",
      "Episode 1315 finished after 162 timesteps, total rewards 2.0, mean loss 0.01404392526509818\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 1316 finished after 82 timesteps, total rewards 3.0, mean loss 0.01404823511682746\n",
      "Episode 1317 finished after 227 timesteps, total rewards 4.0, mean loss 0.014650680400028062\n",
      "Episode 1318 finished after 200 timesteps, total rewards 3.0, mean loss 0.015218681195256068\n",
      "Episode 1319 finished after 176 timesteps, total rewards 1.0, mean loss 0.014469186680798884\n",
      "Episode 1320 finished after 174 timesteps, total rewards 2.0, mean loss 0.015861597601103533\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 1321 finished after 283 timesteps, total rewards 3.0, mean loss 0.015698177855416176\n",
      "Episode 1322 finished after 132 timesteps, total rewards 2.0, mean loss 0.013698507075649544\n",
      "Episode 1323 finished after 120 timesteps, total rewards 2.0, mean loss 0.015578351169097004\n",
      "Episode 1324 finished after 137 timesteps, total rewards 3.0, mean loss 0.01656797292058519\n",
      "Episode 1325 finished after 261 timesteps, total rewards 7.0, mean loss 0.017280601938049953\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1326 finished after 294 timesteps, total rewards 6.0, mean loss 0.013741280990462673\n",
      "Episode 1327 finished after 167 timesteps, total rewards 4.0, mean loss 0.01511243406240898\n",
      "Episode 1328 finished after 162 timesteps, total rewards 3.0, mean loss 0.016121240385001765\n",
      "Episode 1329 finished after 250 timesteps, total rewards 4.0, mean loss 0.016829936786554752\n",
      "Episode 1330 finished after 155 timesteps, total rewards 2.0, mean loss 0.01550397912675004\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 1331 finished after 88 timesteps, total rewards 2.0, mean loss 0.014283302208985499\n",
      "Episode 1332 finished after 270 timesteps, total rewards 3.0, mean loss 0.011946320268153041\n",
      "Episode 1333 finished after 213 timesteps, total rewards 1.0, mean loss 0.014755091672963266\n",
      "Episode 1334 finished after 195 timesteps, total rewards 6.0, mean loss 0.018618029872110736\n",
      "Episode 1335 finished after 123 timesteps, total rewards 0.0, mean loss 0.01681800608332004\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1336 finished after 156 timesteps, total rewards 3.0, mean loss 0.014962262968765572\n",
      "Episode 1337 finished after 157 timesteps, total rewards 2.0, mean loss 0.015992595871634617\n",
      "Episode 1338 finished after 125 timesteps, total rewards 0.0, mean loss 0.01663324009324424\n",
      "Episode 1339 finished after 165 timesteps, total rewards 1.0, mean loss 0.017497584384788448\n",
      "Episode 1340 finished after 182 timesteps, total rewards 2.0, mean loss 0.018713545452695914\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1341 finished after 92 timesteps, total rewards 1.0, mean loss 0.016155388812678764\n",
      "Episode 1342 finished after 253 timesteps, total rewards 3.0, mean loss 0.015143556850377963\n",
      "Episode 1343 finished after 159 timesteps, total rewards 4.0, mean loss 0.014117633512041549\n",
      "Episode 1344 finished after 165 timesteps, total rewards 3.0, mean loss 0.015513167898946752\n",
      "Episode 1345 finished after 88 timesteps, total rewards 3.0, mean loss 0.015668174529723314\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1346 finished after 97 timesteps, total rewards 3.0, mean loss 0.015089888721536462\n",
      "Episode 1347 finished after 155 timesteps, total rewards 1.0, mean loss 0.014384972132111509\n",
      "Episode 1348 finished after 138 timesteps, total rewards 2.0, mean loss 0.0161601860597949\n",
      "Episode 1349 finished after 171 timesteps, total rewards 3.0, mean loss 0.014882672301695099\n",
      "Episode 1350 finished after 246 timesteps, total rewards 5.0, mean loss 0.014554126674405912\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1351 finished after 148 timesteps, total rewards 1.0, mean loss 0.015736633853914495\n",
      "Episode 1352 finished after 263 timesteps, total rewards 5.0, mean loss 0.0159682624819206\n",
      "Episode 1353 finished after 132 timesteps, total rewards 1.0, mean loss 0.01869527365797143\n",
      "Episode 1354 finished after 204 timesteps, total rewards 0.0, mean loss 0.016232131880350803\n",
      "Episode 1355 finished after 219 timesteps, total rewards 6.0, mean loss 0.01701438355155408\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 1356 finished after 178 timesteps, total rewards 1.0, mean loss 0.012959382821984977\n",
      "Episode 1357 finished after 226 timesteps, total rewards 1.0, mean loss 0.016789488877897125\n",
      "Episode 1358 finished after 243 timesteps, total rewards 6.0, mean loss 0.01688572464279884\n",
      "Episode 1359 finished after 294 timesteps, total rewards 11.0, mean loss 0.015406963306453022\n",
      "Episode 1360 finished after 169 timesteps, total rewards 4.0, mean loss 0.01692445243029585\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 3.333333\n",
      "---------------------------------------\n",
      "Episode 1361 finished after 179 timesteps, total rewards 2.0, mean loss 0.020091414821494173\n",
      "Episode 1362 finished after 210 timesteps, total rewards 2.0, mean loss 0.015723421659952563\n",
      "Episode 1363 finished after 168 timesteps, total rewards 6.0, mean loss 0.013477098145466741\n",
      "Episode 1364 finished after 98 timesteps, total rewards 0.0, mean loss 0.018984824457034772\n",
      "Episode 1365 finished after 93 timesteps, total rewards 0.0, mean loss 0.014576719517091049\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 1366 finished after 152 timesteps, total rewards 3.0, mean loss 0.01515433360500059\n",
      "Episode 1367 finished after 168 timesteps, total rewards 3.0, mean loss 0.013602554349615405\n",
      "Episode 1368 finished after 160 timesteps, total rewards 1.0, mean loss 0.015004672771465267\n",
      "Episode 1369 finished after 133 timesteps, total rewards 2.0, mean loss 0.013404515276696267\n",
      "Episode 1370 finished after 199 timesteps, total rewards 4.0, mean loss 0.016684545266346062\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 1371 finished after 134 timesteps, total rewards 2.0, mean loss 0.015435297414399822\n",
      "Episode 1372 finished after 164 timesteps, total rewards 4.0, mean loss 0.017092432320279797\n",
      "Episode 1373 finished after 166 timesteps, total rewards 2.0, mean loss 0.015339648694358205\n",
      "Episode 1374 finished after 133 timesteps, total rewards 2.0, mean loss 0.01302563139417776\n",
      "Episode 1375 finished after 103 timesteps, total rewards 1.0, mean loss 0.014875066526815096\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 1376 finished after 156 timesteps, total rewards 1.0, mean loss 0.015440703392437754\n",
      "Episode 1377 finished after 154 timesteps, total rewards 1.0, mean loss 0.011877820458355384\n",
      "Episode 1378 finished after 122 timesteps, total rewards 0.0, mean loss 0.018014484419598107\n",
      "Episode 1379 finished after 115 timesteps, total rewards 1.0, mean loss 0.014707830285592733\n",
      "Episode 1380 finished after 157 timesteps, total rewards 2.0, mean loss 0.01742971332663682\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 1381 finished after 101 timesteps, total rewards 0.0, mean loss 0.017892751327801108\n",
      "Episode 1382 finished after 93 timesteps, total rewards 2.0, mean loss 0.014251011799299908\n",
      "Episode 1383 finished after 94 timesteps, total rewards 0.0, mean loss 0.01624755567203504\n",
      "Episode 1384 finished after 323 timesteps, total rewards 3.0, mean loss 0.01668276441238111\n",
      "Episode 1385 finished after 141 timesteps, total rewards 2.0, mean loss 0.01527758215530115\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 133.333333\n",
      "---------------------------------------\n",
      "Episode 1386 finished after 232 timesteps, total rewards 2.0, mean loss 0.015411179912540507\n",
      "Episode 1387 finished after 195 timesteps, total rewards 2.0, mean loss 0.01397191885476693\n",
      "Episode 1388 finished after 223 timesteps, total rewards 2.0, mean loss 0.015784807188954253\n",
      "Episode 1389 finished after 174 timesteps, total rewards 1.0, mean loss 0.014417562159226576\n",
      "Episode 1390 finished after 307 timesteps, total rewards 4.0, mean loss 0.015502004575322562\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 1391 finished after 111 timesteps, total rewards 0.0, mean loss 0.015716546449782997\n",
      "Episode 1392 finished after 148 timesteps, total rewards 2.0, mean loss 0.016956889473304555\n",
      "Episode 1393 finished after 131 timesteps, total rewards 3.0, mean loss 0.015474936763137465\n",
      "Episode 1394 finished after 90 timesteps, total rewards 1.0, mean loss 0.014247485593336428\n",
      "Episode 1395 finished after 230 timesteps, total rewards 2.0, mean loss 0.014891714627383803\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 1396 finished after 196 timesteps, total rewards 4.0, mean loss 0.014333466463633196\n",
      "Episode 1397 finished after 151 timesteps, total rewards 3.0, mean loss 0.017272786588665556\n",
      "Episode 1398 finished after 181 timesteps, total rewards 2.0, mean loss 0.015570731797398362\n",
      "Episode 1399 finished after 259 timesteps, total rewards 5.0, mean loss 0.013763169463612427\n",
      "Episode 1400 finished after 165 timesteps, total rewards 4.0, mean loss 0.015168048791714352\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 1401 finished after 249 timesteps, total rewards 3.0, mean loss 0.01592812706806592\n",
      "Episode 1402 finished after 127 timesteps, total rewards 0.0, mean loss 0.017053000797383765\n",
      "Episode 1403 finished after 92 timesteps, total rewards 2.0, mean loss 0.014693577297451988\n",
      "Episode 1404 finished after 317 timesteps, total rewards 9.0, mean loss 0.016387233776954804\n",
      "Episode 1405 finished after 249 timesteps, total rewards 9.0, mean loss 0.016113613771899307\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 1406 finished after 91 timesteps, total rewards 1.0, mean loss 0.014621968533662096\n",
      "Episode 1407 finished after 94 timesteps, total rewards 2.0, mean loss 0.0157452823737874\n",
      "Episode 1408 finished after 188 timesteps, total rewards 3.0, mean loss 0.015091805911884257\n",
      "Episode 1409 finished after 231 timesteps, total rewards 4.0, mean loss 0.01566221849728039\n",
      "Episode 1410 finished after 135 timesteps, total rewards 4.0, mean loss 0.012809843142059874\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 1411 finished after 168 timesteps, total rewards 4.0, mean loss 0.014710425456065596\n",
      "Episode 1412 finished after 169 timesteps, total rewards 2.0, mean loss 0.01987468446009054\n",
      "Episode 1413 finished after 150 timesteps, total rewards 0.0, mean loss 0.016580103903155154\n",
      "Episode 1414 finished after 288 timesteps, total rewards 2.0, mean loss 0.015269294457842383\n",
      "Episode 1415 finished after 93 timesteps, total rewards 3.0, mean loss 0.01815645107673982\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1416 finished after 127 timesteps, total rewards 3.0, mean loss 0.016281510829993368\n",
      "Episode 1417 finished after 231 timesteps, total rewards 3.0, mean loss 0.01666780283764807\n",
      "Episode 1418 finished after 401 timesteps, total rewards 11.0, mean loss 0.01567707900692386\n",
      "Episode 1419 finished after 124 timesteps, total rewards 1.0, mean loss 0.018122324678012273\n",
      "Episode 1420 finished after 103 timesteps, total rewards 1.0, mean loss 0.01518799114669447\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 1421 finished after 151 timesteps, total rewards 4.0, mean loss 0.015823257402424386\n",
      "Episode 1422 finished after 301 timesteps, total rewards 3.0, mean loss 0.015265562154849267\n",
      "Episode 1423 finished after 91 timesteps, total rewards 0.0, mean loss 0.013113698864689579\n",
      "Episode 1424 finished after 96 timesteps, total rewards 3.0, mean loss 0.01618810992295039\n",
      "Episode 1425 finished after 195 timesteps, total rewards 4.0, mean loss 0.015225025076711645\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 1426 finished after 170 timesteps, total rewards 4.0, mean loss 0.01653184202972197\n",
      "Episode 1427 finished after 125 timesteps, total rewards 1.0, mean loss 0.014874359895475208\n",
      "Episode 1428 finished after 158 timesteps, total rewards 2.0, mean loss 0.02020276081674626\n",
      "Episode 1429 finished after 287 timesteps, total rewards 5.0, mean loss 0.014768495686528296\n",
      "Episode 1430 finished after 246 timesteps, total rewards 2.0, mean loss 0.014157512120981096\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1431 finished after 262 timesteps, total rewards 3.0, mean loss 0.015794129952162287\n",
      "Episode 1432 finished after 262 timesteps, total rewards 2.0, mean loss 0.01570367954152846\n",
      "Episode 1433 finished after 100 timesteps, total rewards 0.0, mean loss 0.014145903618482407\n",
      "Episode 1434 finished after 169 timesteps, total rewards 0.0, mean loss 0.016613930689560845\n",
      "Episode 1435 finished after 85 timesteps, total rewards 0.0, mean loss 0.014341276711167987\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 1436 finished after 216 timesteps, total rewards 6.0, mean loss 0.01583284336488254\n",
      "Episode 1437 finished after 239 timesteps, total rewards 4.0, mean loss 0.012848469477183882\n",
      "Episode 1438 finished after 190 timesteps, total rewards 4.0, mean loss 0.017265168384275106\n",
      "Episode 1439 finished after 227 timesteps, total rewards 3.0, mean loss 0.015495171096577979\n",
      "Episode 1440 finished after 143 timesteps, total rewards 0.0, mean loss 0.013452639999486819\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1441 finished after 166 timesteps, total rewards 0.0, mean loss 0.019833301959302645\n",
      "Episode 1442 finished after 178 timesteps, total rewards 3.0, mean loss 0.013130770120318467\n",
      "Episode 1443 finished after 154 timesteps, total rewards 2.0, mean loss 0.015146978343678215\n",
      "Episode 1444 finished after 136 timesteps, total rewards 3.0, mean loss 0.015601726664445939\n",
      "Episode 1445 finished after 230 timesteps, total rewards 6.0, mean loss 0.01424203267708198\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1446 finished after 116 timesteps, total rewards 0.0, mean loss 0.013251141707863507\n",
      "Episode 1447 finished after 240 timesteps, total rewards 6.0, mean loss 0.014137947923275836\n",
      "Episode 1448 finished after 300 timesteps, total rewards 5.0, mean loss 0.015123744282172993\n",
      "Episode 1449 finished after 159 timesteps, total rewards 0.0, mean loss 0.014962436383847826\n",
      "Episode 1450 finished after 195 timesteps, total rewards 1.0, mean loss 0.014649020938071399\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 1451 finished after 305 timesteps, total rewards 6.0, mean loss 0.016897058135879484\n",
      "Episode 1452 finished after 166 timesteps, total rewards 2.0, mean loss 0.01473716791916964\n",
      "Episode 1453 finished after 124 timesteps, total rewards 2.0, mean loss 0.017891766360515895\n",
      "Episode 1454 finished after 225 timesteps, total rewards 3.0, mean loss 0.012464284856751976\n",
      "Episode 1455 finished after 113 timesteps, total rewards 4.0, mean loss 0.012786325113770584\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 1456 finished after 207 timesteps, total rewards 4.0, mean loss 0.017420712787295796\n",
      "Episode 1457 finished after 167 timesteps, total rewards 3.0, mean loss 0.01474300922472504\n",
      "Episode 1458 finished after 177 timesteps, total rewards 3.0, mean loss 0.014209292168034461\n",
      "Episode 1459 finished after 158 timesteps, total rewards 1.0, mean loss 0.017063722331437894\n",
      "Episode 1460 finished after 164 timesteps, total rewards 3.0, mean loss 0.01554447400884805\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 1461 finished after 178 timesteps, total rewards 1.0, mean loss 0.015316791393273521\n",
      "Episode 1462 finished after 337 timesteps, total rewards 6.0, mean loss 0.014149453808523808\n",
      "Episode 1463 finished after 106 timesteps, total rewards 0.0, mean loss 0.010610005285377824\n",
      "Episode 1464 finished after 86 timesteps, total rewards 1.0, mean loss 0.0164021107860229\n",
      "Episode 1465 finished after 92 timesteps, total rewards 0.0, mean loss 0.014956353654902752\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 1466 finished after 164 timesteps, total rewards 4.0, mean loss 0.012884720255320512\n",
      "Episode 1467 finished after 125 timesteps, total rewards 2.0, mean loss 0.013799976147944107\n",
      "Episode 1468 finished after 101 timesteps, total rewards 2.0, mean loss 0.014116833376061946\n",
      "Episode 1469 finished after 124 timesteps, total rewards 0.0, mean loss 0.01180471619285996\n",
      "Episode 1470 finished after 254 timesteps, total rewards 5.0, mean loss 0.013443583218615931\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 1471 finished after 259 timesteps, total rewards 8.0, mean loss 0.013789499500019361\n",
      "Episode 1472 finished after 232 timesteps, total rewards 3.0, mean loss 0.01602168663885095\n",
      "Episode 1473 finished after 113 timesteps, total rewards 0.0, mean loss 0.01580428823636045\n",
      "Episode 1474 finished after 155 timesteps, total rewards 4.0, mean loss 0.015425042263157066\n",
      "Episode 1475 finished after 136 timesteps, total rewards 2.0, mean loss 0.01534252021669732\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 108.333333\n",
      "---------------------------------------\n",
      "Episode 1476 finished after 153 timesteps, total rewards 2.0, mean loss 0.015420560428668788\n",
      "Episode 1477 finished after 155 timesteps, total rewards 2.0, mean loss 0.01667684134895793\n",
      "Episode 1478 finished after 202 timesteps, total rewards 5.0, mean loss 0.016756931246785067\n",
      "Episode 1479 finished after 295 timesteps, total rewards 5.0, mean loss 0.01573275835332224\n",
      "Episode 1480 finished after 79 timesteps, total rewards 1.0, mean loss 0.010870121643787155\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 1481 finished after 135 timesteps, total rewards 1.0, mean loss 0.012924636613913916\n",
      "Episode 1482 finished after 183 timesteps, total rewards 3.0, mean loss 0.0124279627101903\n",
      "Episode 1483 finished after 120 timesteps, total rewards 2.0, mean loss 0.01357553271324529\n",
      "Episode 1484 finished after 244 timesteps, total rewards 6.0, mean loss 0.014375799734882063\n",
      "Episode 1485 finished after 366 timesteps, total rewards 10.0, mean loss 0.015470860371540449\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 6.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1486 finished after 161 timesteps, total rewards 3.0, mean loss 0.014011573611168619\n",
      "Episode 1487 finished after 230 timesteps, total rewards 3.0, mean loss 0.015147826376082578\n",
      "Episode 1488 finished after 98 timesteps, total rewards 1.0, mean loss 0.01391883580398993\n",
      "Episode 1489 finished after 230 timesteps, total rewards 4.0, mean loss 0.013280751093786777\n",
      "Episode 1490 finished after 84 timesteps, total rewards 0.0, mean loss 0.018295925753288127\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 6.666667\n",
      "---------------------------------------\n",
      "Episode 1491 finished after 131 timesteps, total rewards 0.0, mean loss 0.014605399545323004\n",
      "Episode 1492 finished after 259 timesteps, total rewards 5.0, mean loss 0.013436302616850663\n",
      "Episode 1493 finished after 86 timesteps, total rewards 2.0, mean loss 0.016515482320459723\n",
      "Episode 1494 finished after 79 timesteps, total rewards 0.0, mean loss 0.016248071975151337\n",
      "Episode 1495 finished after 249 timesteps, total rewards 3.0, mean loss 0.01222698460975905\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 1496 finished after 116 timesteps, total rewards 1.0, mean loss 0.014547541700423983\n",
      "Episode 1497 finished after 169 timesteps, total rewards 3.0, mean loss 0.01593058045038033\n",
      "Episode 1498 finished after 125 timesteps, total rewards 1.0, mean loss 0.014051350547000767\n",
      "Episode 1499 finished after 141 timesteps, total rewards 0.0, mean loss 0.015991146439283507\n",
      "Episode 1500 finished after 113 timesteps, total rewards 1.0, mean loss 0.01421741753547716\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 1501 finished after 235 timesteps, total rewards 8.0, mean loss 0.015031846937787858\n",
      "Episode 1502 finished after 182 timesteps, total rewards 7.0, mean loss 0.016184700656155186\n",
      "Episode 1503 finished after 162 timesteps, total rewards 3.0, mean loss 0.013217095426502234\n",
      "Episode 1504 finished after 142 timesteps, total rewards 3.0, mean loss 0.017113247046798046\n",
      "Episode 1505 finished after 144 timesteps, total rewards 0.0, mean loss 0.015203707620255753\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 1506 finished after 215 timesteps, total rewards 5.0, mean loss 0.014197146430405852\n",
      "Episode 1507 finished after 230 timesteps, total rewards 2.0, mean loss 0.011830820656842147\n",
      "Episode 1508 finished after 210 timesteps, total rewards 9.0, mean loss 0.01583186270450131\n",
      "Episode 1509 finished after 208 timesteps, total rewards 4.0, mean loss 0.013466665080229107\n",
      "Episode 1510 finished after 278 timesteps, total rewards 8.0, mean loss 0.013293172048028766\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 1511 finished after 203 timesteps, total rewards 2.0, mean loss 0.017016716130115687\n",
      "Episode 1512 finished after 172 timesteps, total rewards 2.0, mean loss 0.012204281370687504\n",
      "Episode 1513 finished after 97 timesteps, total rewards 0.0, mean loss 0.014841773086671535\n",
      "Episode 1514 finished after 143 timesteps, total rewards 0.0, mean loss 0.012866808408244442\n",
      "Episode 1515 finished after 178 timesteps, total rewards 5.0, mean loss 0.0178475095244619\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 1516 finished after 126 timesteps, total rewards 1.0, mean loss 0.013951181085079924\n",
      "Episode 1517 finished after 89 timesteps, total rewards 0.0, mean loss 0.012727772529901456\n",
      "Episode 1518 finished after 154 timesteps, total rewards 3.0, mean loss 0.015077847117889225\n",
      "Episode 1519 finished after 166 timesteps, total rewards 2.0, mean loss 0.015643702791540894\n",
      "Episode 1520 finished after 169 timesteps, total rewards 6.0, mean loss 0.015591385589014276\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 1521 finished after 283 timesteps, total rewards 4.0, mean loss 0.014646147230727262\n",
      "Episode 1522 finished after 133 timesteps, total rewards 1.0, mean loss 0.014628390614398495\n",
      "Episode 1523 finished after 133 timesteps, total rewards 2.0, mean loss 0.015139883501180342\n",
      "Episode 1524 finished after 160 timesteps, total rewards 1.0, mean loss 0.016257323079662454\n",
      "Episode 1525 finished after 160 timesteps, total rewards 2.0, mean loss 0.016147889371131895\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1526 finished after 124 timesteps, total rewards 1.0, mean loss 0.014133472752057001\n",
      "Episode 1527 finished after 219 timesteps, total rewards 3.0, mean loss 0.016234458879845166\n",
      "Episode 1528 finished after 293 timesteps, total rewards 3.0, mean loss 0.014190556718880445\n",
      "Episode 1529 finished after 206 timesteps, total rewards 5.0, mean loss 0.015362223669478571\n",
      "Episode 1530 finished after 149 timesteps, total rewards 3.0, mean loss 0.012350851348984124\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 1531 finished after 308 timesteps, total rewards 5.0, mean loss 0.015040850231095894\n",
      "Episode 1532 finished after 265 timesteps, total rewards 5.0, mean loss 0.014657139462937232\n",
      "Episode 1533 finished after 100 timesteps, total rewards 1.0, mean loss 0.014143390037934295\n",
      "Episode 1534 finished after 251 timesteps, total rewards 7.0, mean loss 0.015424600568835612\n",
      "Episode 1535 finished after 127 timesteps, total rewards 8.0, mean loss 0.01804273675643201\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 10.000000\n",
      "---------------------------------------\n",
      "Episode 1536 finished after 283 timesteps, total rewards 9.0, mean loss 0.014658176497508552\n",
      "Episode 1537 finished after 229 timesteps, total rewards 3.0, mean loss 0.014768650265350555\n",
      "Episode 1538 finished after 221 timesteps, total rewards 4.0, mean loss 0.013207961743599018\n",
      "Episode 1539 finished after 287 timesteps, total rewards 5.0, mean loss 0.015160485410812158\n",
      "Episode 1540 finished after 257 timesteps, total rewards 3.0, mean loss 0.013227016366981984\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 1541 finished after 252 timesteps, total rewards 4.0, mean loss 0.01604657345899521\n",
      "Episode 1542 finished after 248 timesteps, total rewards 3.0, mean loss 0.014153753151115398\n",
      "Episode 1543 finished after 159 timesteps, total rewards 2.0, mean loss 0.016126192281932024\n",
      "Episode 1544 finished after 170 timesteps, total rewards 2.0, mean loss 0.014847190491170349\n",
      "Episode 1545 finished after 87 timesteps, total rewards 2.0, mean loss 0.014289876914130866\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 1546 finished after 233 timesteps, total rewards 3.0, mean loss 0.012773606629043921\n",
      "Episode 1547 finished after 162 timesteps, total rewards 2.0, mean loss 0.01599892140120012\n",
      "Episode 1548 finished after 172 timesteps, total rewards 5.0, mean loss 0.014529364440149524\n",
      "Episode 1549 finished after 292 timesteps, total rewards 7.0, mean loss 0.0161459858245009\n",
      "Episode 1550 finished after 173 timesteps, total rewards 5.0, mean loss 0.014031628457488858\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 1551 finished after 177 timesteps, total rewards 2.0, mean loss 0.017519250574934763\n",
      "Episode 1552 finished after 221 timesteps, total rewards 6.0, mean loss 0.013260032872988619\n",
      "Episode 1553 finished after 305 timesteps, total rewards 7.0, mean loss 0.014461372094562842\n",
      "Episode 1554 finished after 88 timesteps, total rewards 1.0, mean loss 0.017785014553572346\n",
      "Episode 1555 finished after 186 timesteps, total rewards 4.0, mean loss 0.014896954722409587\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1556 finished after 168 timesteps, total rewards 4.0, mean loss 0.013535066318928287\n",
      "Episode 1557 finished after 236 timesteps, total rewards 6.0, mean loss 0.01242221002579199\n",
      "Episode 1558 finished after 226 timesteps, total rewards 4.0, mean loss 0.013576469787659712\n",
      "Episode 1559 finished after 196 timesteps, total rewards 2.0, mean loss 0.01496696260170322\n",
      "Episode 1560 finished after 159 timesteps, total rewards 2.0, mean loss 0.015333576040392238\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 1561 finished after 166 timesteps, total rewards 3.0, mean loss 0.01676081109168541\n",
      "Episode 1562 finished after 268 timesteps, total rewards 6.0, mean loss 0.016228789704234543\n",
      "Episode 1563 finished after 272 timesteps, total rewards 11.0, mean loss 0.015752512184917578\n",
      "Episode 1564 finished after 377 timesteps, total rewards 6.0, mean loss 0.015446957413293052\n",
      "Episode 1565 finished after 277 timesteps, total rewards 4.0, mean loss 0.015059042897450298\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 1566 finished after 212 timesteps, total rewards 5.0, mean loss 0.014644054587785471\n",
      "Episode 1567 finished after 114 timesteps, total rewards 0.0, mean loss 0.018661911105083532\n",
      "Episode 1568 finished after 163 timesteps, total rewards 2.0, mean loss 0.01916815211944299\n",
      "Episode 1569 finished after 165 timesteps, total rewards 3.0, mean loss 0.01560073977717283\n",
      "Episode 1570 finished after 90 timesteps, total rewards 1.0, mean loss 0.01299654252620207\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 1571 finished after 186 timesteps, total rewards 3.0, mean loss 0.014943557203232852\n",
      "Episode 1572 finished after 147 timesteps, total rewards 1.0, mean loss 0.016360148437658553\n",
      "Episode 1573 finished after 176 timesteps, total rewards 4.0, mean loss 0.013789840299895413\n",
      "Episode 1574 finished after 175 timesteps, total rewards 2.0, mean loss 0.015365928249666467\n",
      "Episode 1575 finished after 93 timesteps, total rewards 1.0, mean loss 0.013651162757092626\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 1576 finished after 156 timesteps, total rewards 2.0, mean loss 0.01632061310635599\n",
      "Episode 1577 finished after 227 timesteps, total rewards 2.0, mean loss 0.014390258070559441\n",
      "Episode 1578 finished after 160 timesteps, total rewards 2.0, mean loss 0.016347448616943438\n",
      "Episode 1579 finished after 125 timesteps, total rewards 4.0, mean loss 0.014403994365595281\n",
      "Episode 1580 finished after 271 timesteps, total rewards 8.0, mean loss 0.014906698891573424\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 1581 finished after 195 timesteps, total rewards 4.0, mean loss 0.016576011788446265\n",
      "Episode 1582 finished after 87 timesteps, total rewards 0.0, mean loss 0.01716802035586844\n",
      "Episode 1583 finished after 133 timesteps, total rewards 4.0, mean loss 0.013162899698932563\n",
      "Episode 1584 finished after 139 timesteps, total rewards 1.0, mean loss 0.015775254514259318\n",
      "Episode 1585 finished after 311 timesteps, total rewards 3.0, mean loss 0.013221981012115669\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 1586 finished after 173 timesteps, total rewards 3.0, mean loss 0.014210684735878408\n",
      "Episode 1587 finished after 270 timesteps, total rewards 5.0, mean loss 0.015907993027940392\n",
      "Episode 1588 finished after 276 timesteps, total rewards 5.0, mean loss 0.016447417866889948\n",
      "Episode 1589 finished after 271 timesteps, total rewards 4.0, mean loss 0.015137397867156626\n",
      "Episode 1590 finished after 116 timesteps, total rewards 1.0, mean loss 0.012375163266024801\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1591 finished after 163 timesteps, total rewards 3.0, mean loss 0.014024172424523737\n",
      "Episode 1592 finished after 236 timesteps, total rewards 5.0, mean loss 0.014985890966426715\n",
      "Episode 1593 finished after 170 timesteps, total rewards 5.0, mean loss 0.015311512210310492\n",
      "Episode 1594 finished after 241 timesteps, total rewards 5.0, mean loss 0.01717622558846181\n",
      "Episode 1595 finished after 251 timesteps, total rewards 8.0, mean loss 0.016789083008351087\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 1596 finished after 243 timesteps, total rewards 3.0, mean loss 0.01310937044197797\n",
      "Episode 1597 finished after 142 timesteps, total rewards 2.0, mean loss 0.01474481724669114\n",
      "Episode 1598 finished after 384 timesteps, total rewards 9.0, mean loss 0.01687716112663414\n",
      "Episode 1599 finished after 232 timesteps, total rewards 4.0, mean loss 0.013850622347835552\n",
      "Episode 1600 finished after 167 timesteps, total rewards 2.0, mean loss 0.01475535461422569\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 163.333333\n",
      "---------------------------------------\n",
      "Episode 1601 finished after 150 timesteps, total rewards 1.0, mean loss 0.020185382759276158\n",
      "Episode 1602 finished after 173 timesteps, total rewards 1.0, mean loss 0.017865223780468193\n",
      "Episode 1603 finished after 312 timesteps, total rewards 7.0, mean loss 0.014844806320653357\n",
      "Episode 1604 finished after 258 timesteps, total rewards 3.0, mean loss 0.015178881357617987\n",
      "Episode 1605 finished after 167 timesteps, total rewards 2.0, mean loss 0.015643574949388787\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 1606 finished after 86 timesteps, total rewards 1.0, mean loss 0.018592289503430948\n",
      "Episode 1607 finished after 246 timesteps, total rewards 6.0, mean loss 0.018003138924955496\n",
      "Episode 1608 finished after 156 timesteps, total rewards 2.0, mean loss 0.01621163945804792\n",
      "Episode 1609 finished after 205 timesteps, total rewards 2.0, mean loss 0.01636676714122409\n",
      "Episode 1610 finished after 159 timesteps, total rewards 3.0, mean loss 0.01623160886024008\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 108.333333\n",
      "---------------------------------------\n",
      "Episode 1611 finished after 159 timesteps, total rewards 1.0, mean loss 0.014576004738641212\n",
      "Episode 1612 finished after 274 timesteps, total rewards 2.0, mean loss 0.01636768187389643\n",
      "Episode 1613 finished after 157 timesteps, total rewards 2.0, mean loss 0.01629457195959748\n",
      "Episode 1614 finished after 95 timesteps, total rewards 3.0, mean loss 0.0145533738400493\n",
      "Episode 1615 finished after 140 timesteps, total rewards 2.0, mean loss 0.013152976374840364\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 1616 finished after 179 timesteps, total rewards 1.0, mean loss 0.01754502475755315\n",
      "Episode 1617 finished after 327 timesteps, total rewards 4.0, mean loss 0.014335137014761454\n",
      "Episode 1618 finished after 87 timesteps, total rewards 0.0, mean loss 0.01743973011750428\n",
      "Episode 1619 finished after 255 timesteps, total rewards 6.0, mean loss 0.015912565612234177\n",
      "Episode 1620 finished after 104 timesteps, total rewards 0.0, mean loss 0.015431958892664764\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 8.333333\n",
      "---------------------------------------\n",
      "Episode 1621 finished after 221 timesteps, total rewards 3.0, mean loss 0.016741001661271657\n",
      "Episode 1622 finished after 148 timesteps, total rewards 1.0, mean loss 0.013933851091014976\n",
      "Episode 1623 finished after 316 timesteps, total rewards 6.0, mean loss 0.01589156083253881\n",
      "Episode 1624 finished after 303 timesteps, total rewards 4.0, mean loss 0.013089244900941356\n",
      "Episode 1625 finished after 92 timesteps, total rewards 2.0, mean loss 0.016896686858672183\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1626 finished after 192 timesteps, total rewards 2.0, mean loss 0.014777813596841346\n",
      "Episode 1627 finished after 208 timesteps, total rewards 6.0, mean loss 0.013820687914351136\n",
      "Episode 1628 finished after 334 timesteps, total rewards 5.0, mean loss 0.0153415093366942\n",
      "Episode 1629 finished after 97 timesteps, total rewards 2.0, mean loss 0.013581587670769389\n",
      "Episode 1630 finished after 233 timesteps, total rewards 3.0, mean loss 0.01529036717446393\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 1631 finished after 170 timesteps, total rewards 1.0, mean loss 0.014608223244773826\n",
      "Episode 1632 finished after 123 timesteps, total rewards 1.0, mean loss 0.01575435816340633\n",
      "Episode 1633 finished after 164 timesteps, total rewards 2.0, mean loss 0.014222373110497734\n",
      "Episode 1634 finished after 86 timesteps, total rewards 0.0, mean loss 0.013956857495264388\n",
      "Episode 1635 finished after 110 timesteps, total rewards 0.0, mean loss 0.015954712883103638\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 1636 finished after 87 timesteps, total rewards 2.0, mean loss 0.01662570191313106\n",
      "Episode 1637 finished after 90 timesteps, total rewards 3.0, mean loss 0.015360751233917351\n",
      "Episode 1638 finished after 85 timesteps, total rewards 1.0, mean loss 0.016346547778983436\n",
      "Episode 1639 finished after 167 timesteps, total rewards 2.0, mean loss 0.01439698846243227\n",
      "Episode 1640 finished after 159 timesteps, total rewards 3.0, mean loss 0.015194196040403837\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 1641 finished after 171 timesteps, total rewards 6.0, mean loss 0.012850238965522657\n",
      "Episode 1642 finished after 110 timesteps, total rewards 1.0, mean loss 0.017090571007627824\n",
      "Episode 1643 finished after 128 timesteps, total rewards 2.0, mean loss 0.01711782803931783\n",
      "Episode 1644 finished after 176 timesteps, total rewards 6.0, mean loss 0.015114791983829822\n",
      "Episode 1645 finished after 213 timesteps, total rewards 7.0, mean loss 0.01621051846119081\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 1646 finished after 80 timesteps, total rewards 1.0, mean loss 0.01687222221880802\n",
      "Episode 1647 finished after 175 timesteps, total rewards 2.0, mean loss 0.017568420550919006\n",
      "Episode 1648 finished after 192 timesteps, total rewards 3.0, mean loss 0.014349126188411295\n",
      "Episode 1649 finished after 123 timesteps, total rewards 2.0, mean loss 0.014519669428350389\n",
      "Episode 1650 finished after 152 timesteps, total rewards 0.0, mean loss 0.015327558709319228\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 1651 finished after 153 timesteps, total rewards 4.0, mean loss 0.013045429277007036\n",
      "Episode 1652 finished after 198 timesteps, total rewards 1.0, mean loss 0.013526447201847343\n",
      "Episode 1653 finished after 155 timesteps, total rewards 0.0, mean loss 0.014040304416783094\n",
      "Episode 1654 finished after 158 timesteps, total rewards 3.0, mean loss 0.016193802442874678\n",
      "Episode 1655 finished after 87 timesteps, total rewards 1.0, mean loss 0.017642391767304647\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 1656 finished after 162 timesteps, total rewards 4.0, mean loss 0.016107226458750377\n",
      "Episode 1657 finished after 97 timesteps, total rewards 2.0, mean loss 0.016103911125665223\n",
      "Episode 1658 finished after 255 timesteps, total rewards 3.0, mean loss 0.014794192742898732\n",
      "Episode 1659 finished after 314 timesteps, total rewards 6.0, mean loss 0.017757363740601696\n",
      "Episode 1660 finished after 277 timesteps, total rewards 7.0, mean loss 0.015387199161362428\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 1661 finished after 153 timesteps, total rewards 4.0, mean loss 0.016416828907239465\n",
      "Episode 1662 finished after 224 timesteps, total rewards 2.0, mean loss 0.017333128797093065\n",
      "Episode 1663 finished after 147 timesteps, total rewards 3.0, mean loss 0.015968670319373005\n",
      "Episode 1664 finished after 135 timesteps, total rewards 2.0, mean loss 0.011757066855810721\n",
      "Episode 1665 finished after 164 timesteps, total rewards 3.0, mean loss 0.017476992154559234\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 1666 finished after 89 timesteps, total rewards 0.0, mean loss 0.017319804411303\n",
      "Episode 1667 finished after 107 timesteps, total rewards 1.0, mean loss 0.01758732495472601\n",
      "Episode 1668 finished after 231 timesteps, total rewards 2.0, mean loss 0.015399054080364799\n",
      "Episode 1669 finished after 196 timesteps, total rewards 3.0, mean loss 0.015197801572978687\n",
      "Episode 1670 finished after 217 timesteps, total rewards 4.0, mean loss 0.014754362450885771\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 1671 finished after 259 timesteps, total rewards 7.0, mean loss 0.015807868374226628\n",
      "Episode 1672 finished after 182 timesteps, total rewards 7.0, mean loss 0.015396600886271079\n",
      "Episode 1673 finished after 182 timesteps, total rewards 2.0, mean loss 0.015745084690714287\n",
      "Episode 1674 finished after 108 timesteps, total rewards 0.0, mean loss 0.015904442768024402\n",
      "Episode 1675 finished after 186 timesteps, total rewards 3.0, mean loss 0.0166597346436002\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 1676 finished after 240 timesteps, total rewards 4.0, mean loss 0.016839159079487822\n",
      "Episode 1677 finished after 96 timesteps, total rewards 2.0, mean loss 0.012788818434576873\n",
      "Episode 1678 finished after 130 timesteps, total rewards 2.0, mean loss 0.01741905635079512\n",
      "Episode 1679 finished after 222 timesteps, total rewards 7.0, mean loss 0.017100402798507833\n",
      "Episode 1680 finished after 112 timesteps, total rewards 1.0, mean loss 0.015993575139353716\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 1681 finished after 372 timesteps, total rewards 7.0, mean loss 0.01600164288421154\n",
      "Episode 1682 finished after 98 timesteps, total rewards 3.0, mean loss 0.017890379961926908\n",
      "Episode 1683 finished after 298 timesteps, total rewards 4.0, mean loss 0.015501809418756177\n",
      "Episode 1684 finished after 117 timesteps, total rewards 1.0, mean loss 0.018471671645947468\n",
      "Episode 1685 finished after 129 timesteps, total rewards 1.0, mean loss 0.013761982436349629\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 1686 finished after 96 timesteps, total rewards 3.0, mean loss 0.01859496799018719\n",
      "Episode 1687 finished after 253 timesteps, total rewards 3.0, mean loss 0.01508377475885807\n",
      "Episode 1688 finished after 234 timesteps, total rewards 7.0, mean loss 0.012394066131997129\n",
      "Episode 1689 finished after 161 timesteps, total rewards 0.0, mean loss 0.014460342913260106\n",
      "Episode 1690 finished after 139 timesteps, total rewards 0.0, mean loss 0.015575131488521752\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 1691 finished after 164 timesteps, total rewards 0.0, mean loss 0.014103234219741042\n",
      "Episode 1692 finished after 245 timesteps, total rewards 9.0, mean loss 0.015844658066932\n",
      "Episode 1693 finished after 177 timesteps, total rewards 4.0, mean loss 0.016363645379897207\n",
      "Episode 1694 finished after 195 timesteps, total rewards 2.0, mean loss 0.014932213662168346\n",
      "Episode 1695 finished after 126 timesteps, total rewards 1.0, mean loss 0.016108383536837727\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 6.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1696 finished after 261 timesteps, total rewards 6.0, mean loss 0.016077665997141888\n",
      "Episode 1697 finished after 132 timesteps, total rewards 2.0, mean loss 0.016365769064798245\n",
      "Episode 1698 finished after 278 timesteps, total rewards 7.0, mean loss 0.016044773941561445\n",
      "Episode 1699 finished after 95 timesteps, total rewards 3.0, mean loss 0.014710893125984033\n",
      "Episode 1700 finished after 143 timesteps, total rewards 4.0, mean loss 0.012908261772847597\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 1701 finished after 303 timesteps, total rewards 3.0, mean loss 0.014199176741181489\n",
      "Episode 1702 finished after 78 timesteps, total rewards 0.0, mean loss 0.01539345049726156\n",
      "Episode 1703 finished after 319 timesteps, total rewards 9.0, mean loss 0.015263844700566666\n",
      "Episode 1704 finished after 152 timesteps, total rewards 2.0, mean loss 0.011634499256998472\n",
      "Episode 1705 finished after 260 timesteps, total rewards 3.0, mean loss 0.014879457212877102\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 1706 finished after 93 timesteps, total rewards 2.0, mean loss 0.02036892127693801\n",
      "Episode 1707 finished after 114 timesteps, total rewards 0.0, mean loss 0.014156022108070095\n",
      "Episode 1708 finished after 188 timesteps, total rewards 4.0, mean loss 0.015554083077772025\n",
      "Episode 1709 finished after 90 timesteps, total rewards 0.0, mean loss 0.01581530063154383\n",
      "Episode 1710 finished after 228 timesteps, total rewards 4.0, mean loss 0.01609704323307501\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 85.000000\n",
      "---------------------------------------\n",
      "Episode 1711 finished after 352 timesteps, total rewards 4.0, mean loss 0.015938455600503817\n",
      "Episode 1712 finished after 283 timesteps, total rewards 8.0, mean loss 0.015247828134499299\n",
      "Episode 1713 finished after 294 timesteps, total rewards 8.0, mean loss 0.015328561972872651\n",
      "Episode 1714 finished after 111 timesteps, total rewards 1.0, mean loss 0.014282611809117166\n",
      "Episode 1715 finished after 210 timesteps, total rewards 2.0, mean loss 0.01726190932815717\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 1716 finished after 315 timesteps, total rewards 3.0, mean loss 0.013775720390664886\n",
      "Episode 1717 finished after 105 timesteps, total rewards 1.0, mean loss 0.018166450712098076\n",
      "Episode 1718 finished after 182 timesteps, total rewards 6.0, mean loss 0.013677886028864912\n",
      "Episode 1719 finished after 130 timesteps, total rewards 0.0, mean loss 0.018195793884716784\n",
      "Episode 1720 finished after 244 timesteps, total rewards 6.0, mean loss 0.01794878144764731\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 1721 finished after 105 timesteps, total rewards 0.0, mean loss 0.015550787253527059\n",
      "Episode 1722 finished after 114 timesteps, total rewards 1.0, mean loss 0.016590572706997608\n",
      "Episode 1723 finished after 223 timesteps, total rewards 8.0, mean loss 0.017228907174268955\n",
      "Episode 1724 finished after 285 timesteps, total rewards 6.0, mean loss 0.017046001995709447\n",
      "Episode 1725 finished after 100 timesteps, total rewards 1.0, mean loss 0.016550354616483673\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 1726 finished after 244 timesteps, total rewards 4.0, mean loss 0.015179750552500187\n",
      "Episode 1727 finished after 298 timesteps, total rewards 6.0, mean loss 0.014755452630514804\n",
      "Episode 1728 finished after 117 timesteps, total rewards 1.0, mean loss 0.017116892727392398\n",
      "Episode 1729 finished after 164 timesteps, total rewards 3.0, mean loss 0.013904811824897657\n",
      "Episode 1730 finished after 246 timesteps, total rewards 3.0, mean loss 0.015297769696751753\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 1731 finished after 95 timesteps, total rewards 0.0, mean loss 0.013925197333293525\n",
      "Episode 1732 finished after 128 timesteps, total rewards 1.0, mean loss 0.01554611257029137\n",
      "Episode 1733 finished after 152 timesteps, total rewards 2.0, mean loss 0.013065791718154765\n",
      "Episode 1734 finished after 172 timesteps, total rewards 3.0, mean loss 0.014892776112774724\n",
      "Episode 1735 finished after 95 timesteps, total rewards 0.0, mean loss 0.014246740775476946\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 1736 finished after 161 timesteps, total rewards 2.0, mean loss 0.01473774355207136\n",
      "Episode 1737 finished after 245 timesteps, total rewards 8.0, mean loss 0.01450022915889015\n",
      "Episode 1738 finished after 160 timesteps, total rewards 5.0, mean loss 0.014857966048475646\n",
      "Episode 1739 finished after 127 timesteps, total rewards 2.0, mean loss 0.013514330327615332\n",
      "Episode 1740 finished after 210 timesteps, total rewards 3.0, mean loss 0.015162312465032473\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 1741 finished after 76 timesteps, total rewards 0.0, mean loss 0.01718389444967646\n",
      "Episode 1742 finished after 179 timesteps, total rewards 5.0, mean loss 0.016375925446868295\n",
      "Episode 1743 finished after 173 timesteps, total rewards 5.0, mean loss 0.014074575943290929\n",
      "Episode 1744 finished after 206 timesteps, total rewards 4.0, mean loss 0.012506446437469404\n",
      "Episode 1745 finished after 197 timesteps, total rewards 4.0, mean loss 0.015501045501084176\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 1746 finished after 258 timesteps, total rewards 7.0, mean loss 0.014052827095422467\n",
      "Episode 1747 finished after 234 timesteps, total rewards 3.0, mean loss 0.01691231178903343\n",
      "Episode 1748 finished after 201 timesteps, total rewards 2.0, mean loss 0.014638141783814998\n",
      "Episode 1749 finished after 319 timesteps, total rewards 4.0, mean loss 0.014893612279253792\n",
      "Episode 1750 finished after 126 timesteps, total rewards 2.0, mean loss 0.014289036595506505\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 1751 finished after 194 timesteps, total rewards 7.0, mean loss 0.01399876223982086\n",
      "Episode 1752 finished after 159 timesteps, total rewards 3.0, mean loss 0.015170760523218883\n",
      "Episode 1753 finished after 218 timesteps, total rewards 3.0, mean loss 0.01552355217740982\n",
      "Episode 1754 finished after 269 timesteps, total rewards 2.0, mean loss 0.017027590623437894\n",
      "Episode 1755 finished after 292 timesteps, total rewards 4.0, mean loss 0.015077861063006934\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 1756 finished after 89 timesteps, total rewards 0.0, mean loss 0.012949102687691286\n",
      "Episode 1757 finished after 85 timesteps, total rewards 0.0, mean loss 0.01586728566120762\n",
      "Episode 1758 finished after 227 timesteps, total rewards 2.0, mean loss 0.017026770171208825\n",
      "Episode 1759 finished after 99 timesteps, total rewards 3.0, mean loss 0.014321405287112364\n",
      "Episode 1760 finished after 176 timesteps, total rewards 2.0, mean loss 0.014477901265143539\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 10.000000\n",
      "---------------------------------------\n",
      "Episode 1761 finished after 87 timesteps, total rewards 1.0, mean loss 0.015397743597605276\n",
      "Episode 1762 finished after 101 timesteps, total rewards 1.0, mean loss 0.0177144972154511\n",
      "Episode 1763 finished after 125 timesteps, total rewards 2.0, mean loss 0.014760384668130427\n",
      "Episode 1764 finished after 161 timesteps, total rewards 1.0, mean loss 0.016950535430697296\n",
      "Episode 1765 finished after 151 timesteps, total rewards 0.0, mean loss 0.016217010565809844\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1766 finished after 399 timesteps, total rewards 6.0, mean loss 0.016045353558313633\n",
      "Episode 1767 finished after 159 timesteps, total rewards 3.0, mean loss 0.014023839959540879\n",
      "Episode 1768 finished after 82 timesteps, total rewards 1.0, mean loss 0.01568131449365443\n",
      "Episode 1769 finished after 131 timesteps, total rewards 4.0, mean loss 0.01570423852469246\n",
      "Episode 1770 finished after 87 timesteps, total rewards 1.0, mean loss 0.01579447415752347\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 1771 finished after 121 timesteps, total rewards 1.0, mean loss 0.01604526316937567\n",
      "Episode 1772 finished after 174 timesteps, total rewards 3.0, mean loss 0.013060327843112614\n",
      "Episode 1773 finished after 205 timesteps, total rewards 5.0, mean loss 0.019038236092935066\n",
      "Episode 1774 finished after 212 timesteps, total rewards 4.0, mean loss 0.016120136764481496\n",
      "Episode 1775 finished after 92 timesteps, total rewards 1.0, mean loss 0.01408979234712087\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 1776 finished after 193 timesteps, total rewards 2.0, mean loss 0.014417439008371693\n",
      "Episode 1777 finished after 269 timesteps, total rewards 3.0, mean loss 0.017533639781081835\n",
      "Episode 1778 finished after 165 timesteps, total rewards 4.0, mean loss 0.014765068444169381\n",
      "Episode 1779 finished after 168 timesteps, total rewards 2.0, mean loss 0.01554678278972417\n",
      "Episode 1780 finished after 177 timesteps, total rewards 5.0, mean loss 0.014968114057266401\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 1781 finished after 88 timesteps, total rewards 0.0, mean loss 0.0185027998178901\n",
      "Episode 1782 finished after 101 timesteps, total rewards 1.0, mean loss 0.014811684971670666\n",
      "Episode 1783 finished after 194 timesteps, total rewards 3.0, mean loss 0.016373793630093846\n",
      "Episode 1784 finished after 210 timesteps, total rewards 5.0, mean loss 0.014026904526066834\n",
      "Episode 1785 finished after 241 timesteps, total rewards 6.0, mean loss 0.014223250465301403\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 1786 finished after 126 timesteps, total rewards 1.0, mean loss 0.015501140204991864\n",
      "Episode 1787 finished after 170 timesteps, total rewards 4.0, mean loss 0.014515561844318119\n",
      "Episode 1788 finished after 233 timesteps, total rewards 2.0, mean loss 0.01680890203737394\n",
      "Episode 1789 finished after 304 timesteps, total rewards 2.0, mean loss 0.016267108405851393\n",
      "Episode 1790 finished after 238 timesteps, total rewards 1.0, mean loss 0.016271076036128998\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 1791 finished after 248 timesteps, total rewards 5.0, mean loss 0.016758365937998714\n",
      "Episode 1792 finished after 319 timesteps, total rewards 4.0, mean loss 0.014410275224283144\n",
      "Episode 1793 finished after 209 timesteps, total rewards 1.0, mean loss 0.015828981124974217\n",
      "Episode 1794 finished after 227 timesteps, total rewards 4.0, mean loss 0.015899470592269444\n",
      "Episode 1795 finished after 124 timesteps, total rewards 1.0, mean loss 0.01664735146304367\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 1796 finished after 147 timesteps, total rewards 2.0, mean loss 0.018857652337692654\n",
      "Episode 1797 finished after 85 timesteps, total rewards 0.0, mean loss 0.015186925971305327\n",
      "Episode 1798 finished after 227 timesteps, total rewards 12.0, mean loss 0.017386219345262924\n",
      "Episode 1799 finished after 165 timesteps, total rewards 1.0, mean loss 0.015865166410521575\n",
      "Episode 1800 finished after 100 timesteps, total rewards 0.0, mean loss 0.015947463884949685\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 1801 finished after 272 timesteps, total rewards 9.0, mean loss 0.01665971076759246\n",
      "Episode 1802 finished after 200 timesteps, total rewards 3.0, mean loss 0.016613050531304907\n",
      "Episode 1803 finished after 313 timesteps, total rewards 4.0, mean loss 0.014809150693041031\n",
      "Episode 1804 finished after 205 timesteps, total rewards 1.0, mean loss 0.014036144037367549\n",
      "Episode 1805 finished after 206 timesteps, total rewards 5.0, mean loss 0.015589328782262227\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 1806 finished after 121 timesteps, total rewards 2.0, mean loss 0.016463703223338642\n",
      "Episode 1807 finished after 218 timesteps, total rewards 4.0, mean loss 0.016891734183812943\n",
      "Episode 1808 finished after 165 timesteps, total rewards 4.0, mean loss 0.016375092119966267\n",
      "Episode 1809 finished after 148 timesteps, total rewards 2.0, mean loss 0.01743175013708989\n",
      "Episode 1810 finished after 115 timesteps, total rewards 0.0, mean loss 0.016433334782334934\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 1811 finished after 215 timesteps, total rewards 6.0, mean loss 0.01615350458021059\n",
      "Episode 1812 finished after 130 timesteps, total rewards 3.0, mean loss 0.016003052693290207\n",
      "Episode 1813 finished after 167 timesteps, total rewards 2.0, mean loss 0.0161748952451887\n",
      "Episode 1814 finished after 150 timesteps, total rewards 2.0, mean loss 0.0173596318551184\n",
      "Episode 1815 finished after 122 timesteps, total rewards 1.0, mean loss 0.01734254142550034\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 1816 finished after 207 timesteps, total rewards 2.0, mean loss 0.01452848941786777\n",
      "Episode 1817 finished after 267 timesteps, total rewards 6.0, mean loss 0.015454833599614949\n",
      "Episode 1818 finished after 208 timesteps, total rewards 4.0, mean loss 0.014055978611885131\n",
      "Episode 1819 finished after 232 timesteps, total rewards 5.0, mean loss 0.015434102017275892\n",
      "Episode 1820 finished after 168 timesteps, total rewards 3.0, mean loss 0.01757892857754736\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1821 finished after 87 timesteps, total rewards 2.0, mean loss 0.013037481433551374\n",
      "Episode 1822 finished after 88 timesteps, total rewards 1.0, mean loss 0.015277785144545223\n",
      "Episode 1823 finished after 162 timesteps, total rewards 1.0, mean loss 0.015954320207072915\n",
      "Episode 1824 finished after 175 timesteps, total rewards 2.0, mean loss 0.016395892775284925\n",
      "Episode 1825 finished after 255 timesteps, total rewards 4.0, mean loss 0.015856451095611443\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 1826 finished after 101 timesteps, total rewards 0.0, mean loss 0.016819462513823137\n",
      "Episode 1827 finished after 98 timesteps, total rewards 1.0, mean loss 0.013308555504000669\n",
      "Episode 1828 finished after 346 timesteps, total rewards 6.0, mean loss 0.01652470833034459\n",
      "Episode 1829 finished after 128 timesteps, total rewards 1.0, mean loss 0.016213316692756052\n",
      "Episode 1830 finished after 263 timesteps, total rewards 9.0, mean loss 0.014711783787774758\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 1831 finished after 156 timesteps, total rewards 3.0, mean loss 0.015645241901391927\n",
      "Episode 1832 finished after 169 timesteps, total rewards 4.0, mean loss 0.013620771736853079\n",
      "Episode 1833 finished after 283 timesteps, total rewards 8.0, mean loss 0.016409645436246068\n",
      "Episode 1834 finished after 380 timesteps, total rewards 7.0, mean loss 0.014702836434681605\n",
      "Episode 1835 finished after 95 timesteps, total rewards 1.0, mean loss 0.015082564639956936\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1836 finished after 156 timesteps, total rewards 0.0, mean loss 0.012936055501855802\n",
      "Episode 1837 finished after 263 timesteps, total rewards 4.0, mean loss 0.01482251777011324\n",
      "Episode 1838 finished after 101 timesteps, total rewards 2.0, mean loss 0.013279599086312337\n",
      "Episode 1839 finished after 217 timesteps, total rewards 3.0, mean loss 0.01769341303524883\n",
      "Episode 1840 finished after 301 timesteps, total rewards 2.0, mean loss 0.017646930329006574\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 93.333333\n",
      "---------------------------------------\n",
      "Episode 1841 finished after 226 timesteps, total rewards 3.0, mean loss 0.017174047761904777\n",
      "Episode 1842 finished after 175 timesteps, total rewards 1.0, mean loss 0.014859225009568036\n",
      "Episode 1843 finished after 254 timesteps, total rewards 5.0, mean loss 0.018054208799364323\n",
      "Episode 1844 finished after 218 timesteps, total rewards 8.0, mean loss 0.015499586655855511\n",
      "Episode 1845 finished after 262 timesteps, total rewards 6.0, mean loss 0.013520456303443354\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 1846 finished after 245 timesteps, total rewards 7.0, mean loss 0.012692112273451094\n",
      "Episode 1847 finished after 87 timesteps, total rewards 1.0, mean loss 0.014458200025195577\n",
      "Episode 1848 finished after 160 timesteps, total rewards 2.0, mean loss 0.01571546160939761\n",
      "Episode 1849 finished after 125 timesteps, total rewards 3.0, mean loss 0.019740361979696898\n",
      "Episode 1850 finished after 76 timesteps, total rewards 0.0, mean loss 0.01787956303221443\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 1851 finished after 283 timesteps, total rewards 5.0, mean loss 0.01622403695991809\n",
      "Episode 1852 finished after 96 timesteps, total rewards 2.0, mean loss 0.01867114816256314\n",
      "Episode 1853 finished after 166 timesteps, total rewards 3.0, mean loss 0.015691202738762733\n",
      "Episode 1854 finished after 104 timesteps, total rewards 0.0, mean loss 0.0149298057496288\n",
      "Episode 1855 finished after 85 timesteps, total rewards 2.0, mean loss 0.01696746318724335\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 1856 finished after 300 timesteps, total rewards 8.0, mean loss 0.015063369045771349\n",
      "Episode 1857 finished after 199 timesteps, total rewards 1.0, mean loss 0.017370797801077198\n",
      "Episode 1858 finished after 307 timesteps, total rewards 7.0, mean loss 0.01666410564894694\n",
      "Episode 1859 finished after 90 timesteps, total rewards 2.0, mean loss 0.015922042416382787\n",
      "Episode 1860 finished after 186 timesteps, total rewards 7.0, mean loss 0.016136915699931823\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 1861 finished after 285 timesteps, total rewards 8.0, mean loss 0.018008578398567215\n",
      "Episode 1862 finished after 90 timesteps, total rewards 1.0, mean loss 0.018310230374724294\n",
      "Episode 1863 finished after 95 timesteps, total rewards 4.0, mean loss 0.018781370233352246\n",
      "Episode 1864 finished after 86 timesteps, total rewards 1.0, mean loss 0.018814317040757256\n",
      "Episode 1865 finished after 214 timesteps, total rewards 6.0, mean loss 0.014419750094649852\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 1866 finished after 158 timesteps, total rewards 0.0, mean loss 0.01308995482874327\n",
      "Episode 1867 finished after 173 timesteps, total rewards 4.0, mean loss 0.017567178050918786\n",
      "Episode 1868 finished after 261 timesteps, total rewards 4.0, mean loss 0.016457522439170928\n",
      "Episode 1869 finished after 260 timesteps, total rewards 9.0, mean loss 0.014278333010868385\n",
      "Episode 1870 finished after 307 timesteps, total rewards 10.0, mean loss 0.01660282421807443\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 1871 finished after 89 timesteps, total rewards 0.0, mean loss 0.018499659997636055\n",
      "Episode 1872 finished after 238 timesteps, total rewards 6.0, mean loss 0.01683848087104554\n",
      "Episode 1873 finished after 158 timesteps, total rewards 5.0, mean loss 0.017061724796113144\n",
      "Episode 1874 finished after 170 timesteps, total rewards 8.0, mean loss 0.013657143648700132\n",
      "Episode 1875 finished after 146 timesteps, total rewards 4.0, mean loss 0.013581901664920598\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 1876 finished after 128 timesteps, total rewards 3.0, mean loss 0.014410213540941186\n",
      "Episode 1877 finished after 206 timesteps, total rewards 6.0, mean loss 0.018215239376340567\n",
      "Episode 1878 finished after 100 timesteps, total rewards 1.0, mean loss 0.014808788805967197\n",
      "Episode 1879 finished after 91 timesteps, total rewards 1.0, mean loss 0.01357370451512327\n",
      "Episode 1880 finished after 184 timesteps, total rewards 3.0, mean loss 0.019410381602305595\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 1881 finished after 94 timesteps, total rewards 3.0, mean loss 0.016994588003070292\n",
      "Episode 1882 finished after 128 timesteps, total rewards 0.0, mean loss 0.015909016157138467\n",
      "Episode 1883 finished after 104 timesteps, total rewards 2.0, mean loss 0.01411790013998353\n",
      "Episode 1884 finished after 137 timesteps, total rewards 5.0, mean loss 0.015783797542311347\n",
      "Episode 1885 finished after 272 timesteps, total rewards 3.0, mean loss 0.01677718911838276\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1886 finished after 143 timesteps, total rewards 3.0, mean loss 0.016066602870848004\n",
      "Episode 1887 finished after 87 timesteps, total rewards 0.0, mean loss 0.0167688888440112\n",
      "Episode 1888 finished after 158 timesteps, total rewards 2.0, mean loss 0.018212720003167662\n",
      "Episode 1889 finished after 113 timesteps, total rewards 1.0, mean loss 0.018677561940518166\n",
      "Episode 1890 finished after 155 timesteps, total rewards 3.0, mean loss 0.01791030734706099\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 1891 finished after 112 timesteps, total rewards 1.0, mean loss 0.0155100568645139\n",
      "Episode 1892 finished after 157 timesteps, total rewards 1.0, mean loss 0.01739279985773395\n",
      "Episode 1893 finished after 291 timesteps, total rewards 2.0, mean loss 0.018041285348750484\n",
      "Episode 1894 finished after 203 timesteps, total rewards 4.0, mean loss 0.01626878194774735\n",
      "Episode 1895 finished after 140 timesteps, total rewards 3.0, mean loss 0.017618932796176524\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 1896 finished after 373 timesteps, total rewards 8.0, mean loss 0.016354200539685382\n",
      "Episode 1897 finished after 206 timesteps, total rewards 3.0, mean loss 0.018638464229156545\n",
      "Episode 1898 finished after 177 timesteps, total rewards 3.0, mean loss 0.01642439129800499\n",
      "Episode 1899 finished after 277 timesteps, total rewards 5.0, mean loss 0.019682227061903406\n",
      "Episode 1900 finished after 169 timesteps, total rewards 2.0, mean loss 0.01607543982795478\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 1901 finished after 291 timesteps, total rewards 11.0, mean loss 0.015911811819504882\n",
      "Episode 1902 finished after 126 timesteps, total rewards 2.0, mean loss 0.017939309958213318\n",
      "Episode 1903 finished after 86 timesteps, total rewards 1.0, mean loss 0.013939167243607213\n",
      "Episode 1904 finished after 110 timesteps, total rewards 1.0, mean loss 0.018303083822617985\n",
      "Episode 1905 finished after 216 timesteps, total rewards 5.0, mean loss 0.017377072497361548\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1906 finished after 161 timesteps, total rewards 7.0, mean loss 0.015007579828589367\n",
      "Episode 1907 finished after 119 timesteps, total rewards 2.0, mean loss 0.017447511079533323\n",
      "Episode 1908 finished after 221 timesteps, total rewards 5.0, mean loss 0.018348154178883764\n",
      "Episode 1909 finished after 151 timesteps, total rewards 3.0, mean loss 0.0169821740935683\n",
      "Episode 1910 finished after 284 timesteps, total rewards 4.0, mean loss 0.015547722533675612\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 1911 finished after 196 timesteps, total rewards 1.0, mean loss 0.017417184442337554\n",
      "Episode 1912 finished after 140 timesteps, total rewards 3.0, mean loss 0.016103557986623076\n",
      "Episode 1913 finished after 141 timesteps, total rewards 1.0, mean loss 0.01684272901034511\n",
      "Episode 1914 finished after 167 timesteps, total rewards 4.0, mean loss 0.018848751304590305\n",
      "Episode 1915 finished after 264 timesteps, total rewards 7.0, mean loss 0.01639784510067936\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1916 finished after 123 timesteps, total rewards 1.0, mean loss 0.01480495210332849\n",
      "Episode 1917 finished after 126 timesteps, total rewards 3.0, mean loss 0.019804927230801524\n",
      "Episode 1918 finished after 91 timesteps, total rewards 1.0, mean loss 0.019704932772303715\n",
      "Episode 1919 finished after 122 timesteps, total rewards 2.0, mean loss 0.019037122734188727\n",
      "Episode 1920 finished after 272 timesteps, total rewards 2.0, mean loss 0.015487057265572195\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 1921 finished after 173 timesteps, total rewards 2.0, mean loss 0.014339050532263472\n",
      "Episode 1922 finished after 217 timesteps, total rewards 7.0, mean loss 0.016821558728346318\n",
      "Episode 1923 finished after 210 timesteps, total rewards 4.0, mean loss 0.017031858751683362\n",
      "Episode 1924 finished after 134 timesteps, total rewards 2.0, mean loss 0.014106624371393013\n",
      "Episode 1925 finished after 181 timesteps, total rewards 4.0, mean loss 0.017146248531203657\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 81.666667\n",
      "---------------------------------------\n",
      "Episode 1926 finished after 159 timesteps, total rewards 2.0, mean loss 0.017053310258560023\n",
      "Episode 1927 finished after 102 timesteps, total rewards 2.0, mean loss 0.014506564829121892\n",
      "Episode 1928 finished after 252 timesteps, total rewards 6.0, mean loss 0.015480427073184512\n",
      "Episode 1929 finished after 260 timesteps, total rewards 3.0, mean loss 0.017395053312961515\n",
      "Episode 1930 finished after 241 timesteps, total rewards 5.0, mean loss 0.015537326714807457\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 1931 finished after 130 timesteps, total rewards 2.0, mean loss 0.017233392106404958\n",
      "Episode 1932 finished after 218 timesteps, total rewards 3.0, mean loss 0.016839807393048557\n",
      "Episode 1933 finished after 119 timesteps, total rewards 2.0, mean loss 0.017017068047401253\n",
      "Episode 1934 finished after 97 timesteps, total rewards 1.0, mean loss 0.01651619088604384\n",
      "Episode 1935 finished after 316 timesteps, total rewards 6.0, mean loss 0.01701149996195086\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 1936 finished after 122 timesteps, total rewards 0.0, mean loss 0.017311076512207185\n",
      "Episode 1937 finished after 176 timesteps, total rewards 4.0, mean loss 0.01714691790402867\n",
      "Episode 1938 finished after 277 timesteps, total rewards 3.0, mean loss 0.014534811140353278\n",
      "Episode 1939 finished after 131 timesteps, total rewards 3.0, mean loss 0.01636119450967695\n",
      "Episode 1940 finished after 199 timesteps, total rewards 2.0, mean loss 0.01680856939444983\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 1941 finished after 130 timesteps, total rewards 2.0, mean loss 0.01431368145500668\n",
      "Episode 1942 finished after 255 timesteps, total rewards 6.0, mean loss 0.01369884124404623\n",
      "Episode 1943 finished after 219 timesteps, total rewards 4.0, mean loss 0.0181692135529582\n",
      "Episode 1944 finished after 370 timesteps, total rewards 8.0, mean loss 0.015085016467020143\n",
      "Episode 1945 finished after 307 timesteps, total rewards 7.0, mean loss 0.015139305703125815\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 1946 finished after 117 timesteps, total rewards 2.0, mean loss 0.017136446034145724\n",
      "Episode 1947 finished after 181 timesteps, total rewards 7.0, mean loss 0.014813847042612464\n",
      "Episode 1948 finished after 117 timesteps, total rewards 1.0, mean loss 0.01270707301212411\n",
      "Episode 1949 finished after 164 timesteps, total rewards 3.0, mean loss 0.015255975981722812\n",
      "Episode 1950 finished after 203 timesteps, total rewards 3.0, mean loss 0.015994870596223516\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 1951 finished after 85 timesteps, total rewards 1.0, mean loss 0.014614287466632531\n",
      "Episode 1952 finished after 96 timesteps, total rewards 1.0, mean loss 0.015087779162058723\n",
      "Episode 1953 finished after 108 timesteps, total rewards 2.0, mean loss 0.019530479817250226\n",
      "Episode 1954 finished after 174 timesteps, total rewards 5.0, mean loss 0.015902458428172395\n",
      "Episode 1955 finished after 207 timesteps, total rewards 4.0, mean loss 0.017643427502438157\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 1956 finished after 179 timesteps, total rewards 3.0, mean loss 0.017926111055782595\n",
      "Episode 1957 finished after 133 timesteps, total rewards 3.0, mean loss 0.014105297435124061\n",
      "Episode 1958 finished after 290 timesteps, total rewards 6.0, mean loss 0.015222155475479166\n",
      "Episode 1959 finished after 271 timesteps, total rewards 6.0, mean loss 0.01541160190457777\n",
      "Episode 1960 finished after 340 timesteps, total rewards 4.0, mean loss 0.015862637452493587\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 1961 finished after 213 timesteps, total rewards 5.0, mean loss 0.017686249977939637\n",
      "Episode 1962 finished after 162 timesteps, total rewards 5.0, mean loss 0.016097821242084014\n",
      "Episode 1963 finished after 195 timesteps, total rewards 6.0, mean loss 0.016682750260970818\n",
      "Episode 1964 finished after 271 timesteps, total rewards 6.0, mean loss 0.016822158891881208\n",
      "Episode 1965 finished after 222 timesteps, total rewards 5.0, mean loss 0.01809982777633998\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 10.000000\n",
      "---------------------------------------\n",
      "Episode 1966 finished after 81 timesteps, total rewards 0.0, mean loss 0.01740627896103916\n",
      "Episode 1967 finished after 131 timesteps, total rewards 2.0, mean loss 0.01544287798500563\n",
      "Episode 1968 finished after 99 timesteps, total rewards 2.0, mean loss 0.017624637596497333\n",
      "Episode 1969 finished after 274 timesteps, total rewards 5.0, mean loss 0.016093132860217598\n",
      "Episode 1970 finished after 181 timesteps, total rewards 3.0, mean loss 0.017208858354863563\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 1971 finished after 201 timesteps, total rewards 6.0, mean loss 0.017575206283991923\n",
      "Episode 1972 finished after 132 timesteps, total rewards 0.0, mean loss 0.014232138274096404\n",
      "Episode 1973 finished after 239 timesteps, total rewards 5.0, mean loss 0.018238362846710956\n",
      "Episode 1974 finished after 130 timesteps, total rewards 1.0, mean loss 0.018191914467248493\n",
      "Episode 1975 finished after 286 timesteps, total rewards 7.0, mean loss 0.016457390519456574\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1976 finished after 197 timesteps, total rewards 3.0, mean loss 0.015511316913274604\n",
      "Episode 1977 finished after 83 timesteps, total rewards 0.0, mean loss 0.014373025357939235\n",
      "Episode 1978 finished after 158 timesteps, total rewards 3.0, mean loss 0.01606950674742413\n",
      "Episode 1979 finished after 269 timesteps, total rewards 4.0, mean loss 0.016503993196535827\n",
      "Episode 1980 finished after 167 timesteps, total rewards 0.0, mean loss 0.017960201033591822\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 1981 finished after 275 timesteps, total rewards 4.0, mean loss 0.017642297119494867\n",
      "Episode 1982 finished after 90 timesteps, total rewards 0.0, mean loss 0.015703164963310378\n",
      "Episode 1983 finished after 303 timesteps, total rewards 3.0, mean loss 0.017004150306351065\n",
      "Episode 1984 finished after 158 timesteps, total rewards 3.0, mean loss 0.01711734217153921\n",
      "Episode 1985 finished after 243 timesteps, total rewards 2.0, mean loss 0.014094440910606855\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 1986 finished after 96 timesteps, total rewards 2.0, mean loss 0.020374778011804057\n",
      "Episode 1987 finished after 281 timesteps, total rewards 4.0, mean loss 0.015957804795087804\n",
      "Episode 1988 finished after 136 timesteps, total rewards 5.0, mean loss 0.01797519011554815\n",
      "Episode 1989 finished after 137 timesteps, total rewards 5.0, mean loss 0.017190317497554706\n",
      "Episode 1990 finished after 131 timesteps, total rewards 4.0, mean loss 0.016027245648340356\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 1991 finished after 96 timesteps, total rewards 0.0, mean loss 0.017040935148543213\n",
      "Episode 1992 finished after 268 timesteps, total rewards 10.0, mean loss 0.016378209889808204\n",
      "Episode 1993 finished after 208 timesteps, total rewards 3.0, mean loss 0.01828081613824067\n",
      "Episode 1994 finished after 160 timesteps, total rewards 2.0, mean loss 0.0157941793464488\n",
      "Episode 1995 finished after 111 timesteps, total rewards 2.0, mean loss 0.020907965016777854\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 1996 finished after 226 timesteps, total rewards 1.0, mean loss 0.016617223221975567\n",
      "Episode 1997 finished after 112 timesteps, total rewards 3.0, mean loss 0.014542258629912144\n",
      "Episode 1998 finished after 98 timesteps, total rewards 5.0, mean loss 0.013231206012055829\n",
      "Episode 1999 finished after 190 timesteps, total rewards 3.0, mean loss 0.016439578778533217\n",
      "Episode 2000 finished after 286 timesteps, total rewards 7.0, mean loss 0.016428402266416283\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 2001 finished after 84 timesteps, total rewards 1.0, mean loss 0.016078012080037125\n",
      "Episode 2002 finished after 120 timesteps, total rewards 1.0, mean loss 0.016465335576504002\n",
      "Episode 2003 finished after 236 timesteps, total rewards 3.0, mean loss 0.016009369988381168\n",
      "Episode 2004 finished after 150 timesteps, total rewards 3.0, mean loss 0.01586744299856946\n",
      "Episode 2005 finished after 247 timesteps, total rewards 6.0, mean loss 0.016107138562669435\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 110.000000\n",
      "---------------------------------------\n",
      "Episode 2006 finished after 181 timesteps, total rewards 6.0, mean loss 0.01745281157788733\n",
      "Episode 2007 finished after 205 timesteps, total rewards 9.0, mean loss 0.014994569439384178\n",
      "Episode 2008 finished after 155 timesteps, total rewards 1.0, mean loss 0.017013335274156903\n",
      "Episode 2009 finished after 178 timesteps, total rewards 7.0, mean loss 0.017341551143974274\n",
      "Episode 2010 finished after 274 timesteps, total rewards 2.0, mean loss 0.017040491639678943\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 2011 finished after 108 timesteps, total rewards 3.0, mean loss 0.017072031414028497\n",
      "Episode 2012 finished after 208 timesteps, total rewards 2.0, mean loss 0.016206206041225905\n",
      "Episode 2013 finished after 167 timesteps, total rewards 1.0, mean loss 0.01566737580627571\n",
      "Episode 2014 finished after 152 timesteps, total rewards 5.0, mean loss 0.015989402019170627\n",
      "Episode 2015 finished after 143 timesteps, total rewards 2.0, mean loss 0.017307777129931024\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 2016 finished after 95 timesteps, total rewards 4.0, mean loss 0.016505168084863967\n",
      "Episode 2017 finished after 292 timesteps, total rewards 5.0, mean loss 0.01800252032103033\n",
      "Episode 2018 finished after 236 timesteps, total rewards 3.0, mean loss 0.016366241568841547\n",
      "Episode 2019 finished after 173 timesteps, total rewards 2.0, mean loss 0.015201873509038368\n",
      "Episode 2020 finished after 130 timesteps, total rewards 1.0, mean loss 0.015133755954985435\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2021 finished after 123 timesteps, total rewards 1.0, mean loss 0.015545223199060126\n",
      "Episode 2022 finished after 158 timesteps, total rewards 4.0, mean loss 0.014522775167693631\n",
      "Episode 2023 finished after 157 timesteps, total rewards 4.0, mean loss 0.014412743051849294\n",
      "Episode 2024 finished after 235 timesteps, total rewards 3.0, mean loss 0.0159304681130031\n",
      "Episode 2025 finished after 165 timesteps, total rewards 6.0, mean loss 0.01359071422837477\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 2026 finished after 279 timesteps, total rewards 8.0, mean loss 0.018408520510011288\n",
      "Episode 2027 finished after 193 timesteps, total rewards 2.0, mean loss 0.013865523094129193\n",
      "Episode 2028 finished after 94 timesteps, total rewards 3.0, mean loss 0.01584673509313824\n",
      "Episode 2029 finished after 274 timesteps, total rewards 6.0, mean loss 0.017848966658084796\n",
      "Episode 2030 finished after 103 timesteps, total rewards 2.0, mean loss 0.0164912738696021\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 2031 finished after 252 timesteps, total rewards 5.0, mean loss 0.0160429115083331\n",
      "Episode 2032 finished after 173 timesteps, total rewards 2.0, mean loss 0.01706207522380871\n",
      "Episode 2033 finished after 174 timesteps, total rewards 2.0, mean loss 0.016605022385610728\n",
      "Episode 2034 finished after 241 timesteps, total rewards 2.0, mean loss 0.018450111456754466\n",
      "Episode 2035 finished after 140 timesteps, total rewards 2.0, mean loss 0.01647292344631361\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 2036 finished after 191 timesteps, total rewards 11.0, mean loss 0.014959489266450288\n",
      "Episode 2037 finished after 134 timesteps, total rewards 3.0, mean loss 0.016491218404879153\n",
      "Episode 2038 finished after 167 timesteps, total rewards 2.0, mean loss 0.01934715408868781\n",
      "Episode 2039 finished after 145 timesteps, total rewards 3.0, mean loss 0.0192484170166326\n",
      "Episode 2040 finished after 233 timesteps, total rewards 3.0, mean loss 0.01719009172038451\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 2041 finished after 138 timesteps, total rewards 0.0, mean loss 0.016727234863893777\n",
      "Episode 2042 finished after 179 timesteps, total rewards 4.0, mean loss 0.01775115858711427\n",
      "Episode 2043 finished after 122 timesteps, total rewards 0.0, mean loss 0.01749166872468777\n",
      "Episode 2044 finished after 88 timesteps, total rewards 1.0, mean loss 0.013087078417646064\n",
      "Episode 2045 finished after 163 timesteps, total rewards 1.0, mean loss 0.018454996305103614\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2046 finished after 96 timesteps, total rewards 2.0, mean loss 0.015041341879320195\n",
      "Episode 2047 finished after 92 timesteps, total rewards 4.0, mean loss 0.01736084642723891\n",
      "Episode 2048 finished after 153 timesteps, total rewards 3.0, mean loss 0.018112900342375306\n",
      "Episode 2049 finished after 261 timesteps, total rewards 5.0, mean loss 0.017657297254264376\n",
      "Episode 2050 finished after 122 timesteps, total rewards 2.0, mean loss 0.01752972912967022\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 95.000000\n",
      "---------------------------------------\n",
      "Episode 2051 finished after 146 timesteps, total rewards 2.0, mean loss 0.018023694088373156\n",
      "Episode 2052 finished after 93 timesteps, total rewards 4.0, mean loss 0.013559351444885294\n",
      "Episode 2053 finished after 114 timesteps, total rewards 2.0, mean loss 0.01717287322951546\n",
      "Episode 2054 finished after 134 timesteps, total rewards 0.0, mean loss 0.01628398617626161\n",
      "Episode 2055 finished after 261 timesteps, total rewards 5.0, mean loss 0.018839450071117987\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 2056 finished after 201 timesteps, total rewards 2.0, mean loss 0.017341719183894756\n",
      "Episode 2057 finished after 145 timesteps, total rewards 5.0, mean loss 0.015371621198598938\n",
      "Episode 2058 finished after 169 timesteps, total rewards 4.0, mean loss 0.017318352064277603\n",
      "Episode 2059 finished after 124 timesteps, total rewards 1.0, mean loss 0.015802960941190606\n",
      "Episode 2060 finished after 217 timesteps, total rewards 6.0, mean loss 0.01556199312401605\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 2061 finished after 102 timesteps, total rewards 0.0, mean loss 0.013759291185266977\n",
      "Episode 2062 finished after 166 timesteps, total rewards 2.0, mean loss 0.016590209588750703\n",
      "Episode 2063 finished after 146 timesteps, total rewards 3.0, mean loss 0.015273294770812667\n",
      "Episode 2064 finished after 222 timesteps, total rewards 6.0, mean loss 0.01683639221523704\n",
      "Episode 2065 finished after 91 timesteps, total rewards 1.0, mean loss 0.01667819417191792\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 2066 finished after 214 timesteps, total rewards 2.0, mean loss 0.01628562741288393\n",
      "Episode 2067 finished after 171 timesteps, total rewards 1.0, mean loss 0.01600988142938511\n",
      "Episode 2068 finished after 185 timesteps, total rewards 3.0, mean loss 0.016022477310459204\n",
      "Episode 2069 finished after 202 timesteps, total rewards 2.0, mean loss 0.017455488574918184\n",
      "Episode 2070 finished after 93 timesteps, total rewards 1.0, mean loss 0.017800378437612147\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 2071 finished after 255 timesteps, total rewards 6.0, mean loss 0.018727126854298382\n",
      "Episode 2072 finished after 149 timesteps, total rewards 1.0, mean loss 0.016733568648678008\n",
      "Episode 2073 finished after 96 timesteps, total rewards 2.0, mean loss 0.01511682110625164\n",
      "Episode 2074 finished after 258 timesteps, total rewards 6.0, mean loss 0.018697759890324225\n",
      "Episode 2075 finished after 102 timesteps, total rewards 2.0, mean loss 0.018452074628108748\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 2076 finished after 117 timesteps, total rewards 5.0, mean loss 0.01800043840592122\n",
      "Episode 2077 finished after 127 timesteps, total rewards 2.0, mean loss 0.0184059074759487\n",
      "Episode 2078 finished after 179 timesteps, total rewards 2.0, mean loss 0.016166852282453847\n",
      "Episode 2079 finished after 270 timesteps, total rewards 5.0, mean loss 0.016937765395872433\n",
      "Episode 2080 finished after 113 timesteps, total rewards 2.0, mean loss 0.016053569976001384\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 2081 finished after 101 timesteps, total rewards 0.0, mean loss 0.01987484468338495\n",
      "Episode 2082 finished after 276 timesteps, total rewards 5.0, mean loss 0.017056011900843958\n",
      "Episode 2083 finished after 286 timesteps, total rewards 3.0, mean loss 0.016602995465761963\n",
      "Episode 2084 finished after 177 timesteps, total rewards 6.0, mean loss 0.016641034241857817\n",
      "Episode 2085 finished after 120 timesteps, total rewards 1.0, mean loss 0.015459593143411136\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 2086 finished after 277 timesteps, total rewards 4.0, mean loss 0.01563602153611931\n",
      "Episode 2087 finished after 301 timesteps, total rewards 4.0, mean loss 0.014941413928228632\n",
      "Episode 2088 finished after 166 timesteps, total rewards 4.0, mean loss 0.014591762540421549\n",
      "Episode 2089 finished after 160 timesteps, total rewards 2.0, mean loss 0.017549367463470845\n",
      "Episode 2090 finished after 221 timesteps, total rewards 2.0, mean loss 0.013718991487168989\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 2091 finished after 125 timesteps, total rewards 1.0, mean loss 0.016685146080795677\n",
      "Episode 2092 finished after 86 timesteps, total rewards 0.0, mean loss 0.017934690050233893\n",
      "Episode 2093 finished after 99 timesteps, total rewards 2.0, mean loss 0.016248295748620436\n",
      "Episode 2094 finished after 125 timesteps, total rewards 4.0, mean loss 0.01740010681003332\n",
      "Episode 2095 finished after 269 timesteps, total rewards 8.0, mean loss 0.018152040034293656\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 2096 finished after 287 timesteps, total rewards 2.0, mean loss 0.015953162997573263\n",
      "Episode 2097 finished after 103 timesteps, total rewards 2.0, mean loss 0.01745122414957099\n",
      "Episode 2098 finished after 217 timesteps, total rewards 4.0, mean loss 0.01903564654787453\n",
      "Episode 2099 finished after 227 timesteps, total rewards 9.0, mean loss 0.015451365621300981\n",
      "Episode 2100 finished after 300 timesteps, total rewards 5.0, mean loss 0.01725123071053531\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 2101 finished after 151 timesteps, total rewards 3.0, mean loss 0.014000922232166254\n",
      "Episode 2102 finished after 156 timesteps, total rewards 4.0, mean loss 0.019293091588807352\n",
      "Episode 2103 finished after 150 timesteps, total rewards 1.0, mean loss 0.01812642048113048\n",
      "Episode 2104 finished after 177 timesteps, total rewards 4.0, mean loss 0.015220720586327488\n",
      "Episode 2105 finished after 229 timesteps, total rewards 4.0, mean loss 0.01713886730678941\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 2106 finished after 237 timesteps, total rewards 4.0, mean loss 0.01793459161943147\n",
      "Episode 2107 finished after 104 timesteps, total rewards 1.0, mean loss 0.016844542650617838\n",
      "Episode 2108 finished after 114 timesteps, total rewards 2.0, mean loss 0.0177730294812487\n",
      "Episode 2109 finished after 226 timesteps, total rewards 5.0, mean loss 0.017633832061088404\n",
      "Episode 2110 finished after 115 timesteps, total rewards 0.0, mean loss 0.015449090760565646\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 2111 finished after 128 timesteps, total rewards 2.0, mean loss 0.01603847739443154\n",
      "Episode 2112 finished after 98 timesteps, total rewards 1.0, mean loss 0.019354466534794633\n",
      "Episode 2113 finished after 236 timesteps, total rewards 5.0, mean loss 0.018443880168133083\n",
      "Episode 2114 finished after 239 timesteps, total rewards 5.0, mean loss 0.01719007467184699\n",
      "Episode 2115 finished after 146 timesteps, total rewards 2.0, mean loss 0.015277705769682915\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2116 finished after 314 timesteps, total rewards 5.0, mean loss 0.016818386261206072\n",
      "Episode 2117 finished after 189 timesteps, total rewards 6.0, mean loss 0.01755245088965499\n",
      "Episode 2118 finished after 81 timesteps, total rewards 4.0, mean loss 0.0158317556452397\n",
      "Episode 2119 finished after 139 timesteps, total rewards 3.0, mean loss 0.0173278161627743\n",
      "Episode 2120 finished after 224 timesteps, total rewards 1.0, mean loss 0.017363494051226423\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 2121 finished after 164 timesteps, total rewards 4.0, mean loss 0.015656895005898747\n",
      "Episode 2122 finished after 290 timesteps, total rewards 7.0, mean loss 0.01565710358313638\n",
      "Episode 2123 finished after 115 timesteps, total rewards 5.0, mean loss 0.016583906209005204\n",
      "Episode 2124 finished after 151 timesteps, total rewards 3.0, mean loss 0.017335507636141983\n",
      "Episode 2125 finished after 262 timesteps, total rewards 4.0, mean loss 0.016267608468179085\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 2126 finished after 137 timesteps, total rewards 2.0, mean loss 0.016181598687680425\n",
      "Episode 2127 finished after 100 timesteps, total rewards 1.0, mean loss 0.015026234179385938\n",
      "Episode 2128 finished after 178 timesteps, total rewards 2.0, mean loss 0.01663364948321584\n",
      "Episode 2129 finished after 128 timesteps, total rewards 3.0, mean loss 0.017866599404896988\n",
      "Episode 2130 finished after 178 timesteps, total rewards 0.0, mean loss 0.015462950005436713\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 2131 finished after 178 timesteps, total rewards 5.0, mean loss 0.018693006852526596\n",
      "Episode 2132 finished after 111 timesteps, total rewards 2.0, mean loss 0.01692728162690712\n",
      "Episode 2133 finished after 286 timesteps, total rewards 5.0, mean loss 0.018123164354543482\n",
      "Episode 2134 finished after 184 timesteps, total rewards 3.0, mean loss 0.01634364194330334\n",
      "Episode 2135 finished after 292 timesteps, total rewards 12.0, mean loss 0.01883940802598436\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 2136 finished after 123 timesteps, total rewards 2.0, mean loss 0.018411119804032358\n",
      "Episode 2137 finished after 83 timesteps, total rewards 1.0, mean loss 0.01642763969861532\n",
      "Episode 2138 finished after 303 timesteps, total rewards 5.0, mean loss 0.016794227483788376\n",
      "Episode 2139 finished after 84 timesteps, total rewards 2.0, mean loss 0.020870201265400585\n",
      "Episode 2140 finished after 97 timesteps, total rewards 1.0, mean loss 0.01873941670565568\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 2141 finished after 76 timesteps, total rewards 2.0, mean loss 0.01652888782739051\n",
      "Episode 2142 finished after 154 timesteps, total rewards 4.0, mean loss 0.015818218081469902\n",
      "Episode 2143 finished after 103 timesteps, total rewards 2.0, mean loss 0.016704339251482805\n",
      "Episode 2144 finished after 108 timesteps, total rewards 2.0, mean loss 0.01724206277867779\n",
      "Episode 2145 finished after 134 timesteps, total rewards 2.0, mean loss 0.015668842272568883\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 2146 finished after 291 timesteps, total rewards 7.0, mean loss 0.01526655821982592\n",
      "Episode 2147 finished after 299 timesteps, total rewards 5.0, mean loss 0.01514762408327059\n",
      "Episode 2148 finished after 317 timesteps, total rewards 8.0, mean loss 0.017009831955696806\n",
      "Episode 2149 finished after 101 timesteps, total rewards 1.0, mean loss 0.015446264108891904\n",
      "Episode 2150 finished after 123 timesteps, total rewards 1.0, mean loss 0.02014739083493416\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 2151 finished after 95 timesteps, total rewards 0.0, mean loss 0.014760961999356942\n",
      "Episode 2152 finished after 132 timesteps, total rewards 4.0, mean loss 0.016735341388883888\n",
      "Episode 2153 finished after 163 timesteps, total rewards 2.0, mean loss 0.017258369322063434\n",
      "Episode 2154 finished after 285 timesteps, total rewards 4.0, mean loss 0.015633896862262894\n",
      "Episode 2155 finished after 113 timesteps, total rewards 0.0, mean loss 0.014441709823811641\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 103.333333\n",
      "---------------------------------------\n",
      "Episode 2156 finished after 94 timesteps, total rewards 2.0, mean loss 0.01547489074143046\n",
      "Episode 2157 finished after 167 timesteps, total rewards 1.0, mean loss 0.01588236829328691\n",
      "Episode 2158 finished after 94 timesteps, total rewards 2.0, mean loss 0.01616928719685096\n",
      "Episode 2159 finished after 346 timesteps, total rewards 10.0, mean loss 0.015366368429100066\n",
      "Episode 2160 finished after 170 timesteps, total rewards 1.0, mean loss 0.01759358105071656\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 2161 finished after 121 timesteps, total rewards 3.0, mean loss 0.019991686325693456\n",
      "Episode 2162 finished after 292 timesteps, total rewards 9.0, mean loss 0.018434693207581565\n",
      "Episode 2163 finished after 170 timesteps, total rewards 2.0, mean loss 0.017908464581020835\n",
      "Episode 2164 finished after 172 timesteps, total rewards 2.0, mean loss 0.017263366071897192\n",
      "Episode 2165 finished after 236 timesteps, total rewards 2.0, mean loss 0.01686076061593929\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 2166 finished after 254 timesteps, total rewards 8.0, mean loss 0.017293177879420953\n",
      "Episode 2167 finished after 287 timesteps, total rewards 3.0, mean loss 0.01665621471681749\n",
      "Episode 2168 finished after 143 timesteps, total rewards 4.0, mean loss 0.02079927062467587\n",
      "Episode 2169 finished after 86 timesteps, total rewards 2.0, mean loss 0.017108193270423514\n",
      "Episode 2170 finished after 119 timesteps, total rewards 3.0, mean loss 0.016588598154211306\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 2171 finished after 137 timesteps, total rewards 2.0, mean loss 0.017420440604638335\n",
      "Episode 2172 finished after 141 timesteps, total rewards 2.0, mean loss 0.016308739205495394\n",
      "Episode 2173 finished after 167 timesteps, total rewards 2.0, mean loss 0.015834553457565136\n",
      "Episode 2174 finished after 80 timesteps, total rewards 1.0, mean loss 0.017444734909076942\n",
      "Episode 2175 finished after 342 timesteps, total rewards 9.0, mean loss 0.018768280141711866\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 2176 finished after 168 timesteps, total rewards 3.0, mean loss 0.0161695446631278\n",
      "Episode 2177 finished after 161 timesteps, total rewards 2.0, mean loss 0.018757239458807716\n",
      "Episode 2178 finished after 156 timesteps, total rewards 1.0, mean loss 0.01722697879431041\n",
      "Episode 2179 finished after 159 timesteps, total rewards 6.0, mean loss 0.019495332274225733\n",
      "Episode 2180 finished after 155 timesteps, total rewards 3.0, mean loss 0.018054667800184217\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2181 finished after 290 timesteps, total rewards 3.0, mean loss 0.018049750983080794\n",
      "Episode 2182 finished after 259 timesteps, total rewards 5.0, mean loss 0.018538836313930294\n",
      "Episode 2183 finished after 169 timesteps, total rewards 1.0, mean loss 0.016753636975979334\n",
      "Episode 2184 finished after 161 timesteps, total rewards 4.0, mean loss 0.015347414641526834\n",
      "Episode 2185 finished after 222 timesteps, total rewards 1.0, mean loss 0.0187936043272399\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2186 finished after 112 timesteps, total rewards 1.0, mean loss 0.018469152393663535\n",
      "Episode 2187 finished after 138 timesteps, total rewards 4.0, mean loss 0.014107655085525408\n",
      "Episode 2188 finished after 97 timesteps, total rewards 1.0, mean loss 0.015141275879079668\n",
      "Episode 2189 finished after 82 timesteps, total rewards 1.0, mean loss 0.020041382465534274\n",
      "Episode 2190 finished after 187 timesteps, total rewards 2.0, mean loss 0.016269660499886943\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 2191 finished after 119 timesteps, total rewards 3.0, mean loss 0.016642908040363686\n",
      "Episode 2192 finished after 259 timesteps, total rewards 5.0, mean loss 0.015868194542063802\n",
      "Episode 2193 finished after 180 timesteps, total rewards 2.0, mean loss 0.019071021125677767\n",
      "Episode 2194 finished after 225 timesteps, total rewards 5.0, mean loss 0.01667023863669278\n",
      "Episode 2195 finished after 274 timesteps, total rewards 2.0, mean loss 0.01860710049968342\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 115.000000\n",
      "---------------------------------------\n",
      "Episode 2196 finished after 168 timesteps, total rewards 3.0, mean loss 0.015354321038189699\n",
      "Episode 2197 finished after 202 timesteps, total rewards 2.0, mean loss 0.016142801968911016\n",
      "Episode 2198 finished after 97 timesteps, total rewards 1.0, mean loss 0.016803611186333\n",
      "Episode 2199 finished after 128 timesteps, total rewards 2.0, mean loss 0.020677832774254057\n",
      "Episode 2200 finished after 166 timesteps, total rewards 4.0, mean loss 0.01657178474159586\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 2201 finished after 144 timesteps, total rewards 2.0, mean loss 0.016939461701743614\n",
      "Episode 2202 finished after 112 timesteps, total rewards 1.0, mean loss 0.018015833453578774\n",
      "Episode 2203 finished after 132 timesteps, total rewards 1.0, mean loss 0.017241549963011603\n",
      "Episode 2204 finished after 169 timesteps, total rewards 3.0, mean loss 0.018232225031783452\n",
      "Episode 2205 finished after 90 timesteps, total rewards 2.0, mean loss 0.020370700751017365\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 2206 finished after 224 timesteps, total rewards 8.0, mean loss 0.01884204057854991\n",
      "Episode 2207 finished after 177 timesteps, total rewards 1.0, mean loss 0.01572626001513326\n",
      "Episode 2208 finished after 92 timesteps, total rewards 2.0, mean loss 0.017807179085059983\n",
      "Episode 2209 finished after 179 timesteps, total rewards 1.0, mean loss 0.0173209578429047\n",
      "Episode 2210 finished after 101 timesteps, total rewards 1.0, mean loss 0.019440018131033434\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 6.666667\n",
      "---------------------------------------\n",
      "Episode 2211 finished after 95 timesteps, total rewards 0.0, mean loss 0.013326975414039273\n",
      "Episode 2212 finished after 156 timesteps, total rewards 2.0, mean loss 0.018006960028940693\n",
      "Episode 2213 finished after 266 timesteps, total rewards 5.0, mean loss 0.01664018314690644\n",
      "Episode 2214 finished after 160 timesteps, total rewards 5.0, mean loss 0.017560740354383597\n",
      "Episode 2215 finished after 288 timesteps, total rewards 3.0, mean loss 0.017939472753621684\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2216 finished after 156 timesteps, total rewards 2.0, mean loss 0.014768350927251725\n",
      "Episode 2217 finished after 177 timesteps, total rewards 1.0, mean loss 0.01574026251532069\n",
      "Episode 2218 finished after 161 timesteps, total rewards 3.0, mean loss 0.018091777055589366\n",
      "Episode 2219 finished after 122 timesteps, total rewards 2.0, mean loss 0.014916307300512419\n",
      "Episode 2220 finished after 174 timesteps, total rewards 2.0, mean loss 0.017402720120106557\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 2221 finished after 125 timesteps, total rewards 3.0, mean loss 0.017697551540564745\n",
      "Episode 2222 finished after 134 timesteps, total rewards 1.0, mean loss 0.015805402016979574\n",
      "Episode 2223 finished after 259 timesteps, total rewards 6.0, mean loss 0.01761164196517901\n",
      "Episode 2224 finished after 259 timesteps, total rewards 4.0, mean loss 0.01807913665896152\n",
      "Episode 2225 finished after 198 timesteps, total rewards 5.0, mean loss 0.018965582920983905\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 2226 finished after 121 timesteps, total rewards 1.0, mean loss 0.019226427191448944\n",
      "Episode 2227 finished after 228 timesteps, total rewards 4.0, mean loss 0.017555789309992912\n",
      "Episode 2228 finished after 130 timesteps, total rewards 2.0, mean loss 0.019939719141872887\n",
      "Episode 2229 finished after 124 timesteps, total rewards 4.0, mean loss 0.019167885592680484\n",
      "Episode 2230 finished after 223 timesteps, total rewards 7.0, mean loss 0.017246017575779753\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 2231 finished after 128 timesteps, total rewards 1.0, mean loss 0.01735230118947584\n",
      "Episode 2232 finished after 223 timesteps, total rewards 3.0, mean loss 0.013927539380114826\n",
      "Episode 2233 finished after 102 timesteps, total rewards 1.0, mean loss 0.017417758646165477\n",
      "Episode 2234 finished after 137 timesteps, total rewards 1.0, mean loss 0.017857648533374668\n",
      "Episode 2235 finished after 214 timesteps, total rewards 4.0, mean loss 0.017008393869907202\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 2236 finished after 220 timesteps, total rewards 2.0, mean loss 0.019492455088766292\n",
      "Episode 2237 finished after 349 timesteps, total rewards 9.0, mean loss 0.01951881713108133\n",
      "Episode 2238 finished after 84 timesteps, total rewards 2.0, mean loss 0.0183704947689659\n",
      "Episode 2239 finished after 426 timesteps, total rewards 15.0, mean loss 0.01635599551813135\n",
      "Episode 2240 finished after 129 timesteps, total rewards 1.0, mean loss 0.018192742689956673\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 2241 finished after 117 timesteps, total rewards 5.0, mean loss 0.01688644956622241\n",
      "Episode 2242 finished after 140 timesteps, total rewards 2.0, mean loss 0.018102474842869143\n",
      "Episode 2243 finished after 199 timesteps, total rewards 6.0, mean loss 0.01728843686161516\n",
      "Episode 2244 finished after 235 timesteps, total rewards 4.0, mean loss 0.01940407524346077\n",
      "Episode 2245 finished after 242 timesteps, total rewards 1.0, mean loss 0.015187842880249647\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 2246 finished after 206 timesteps, total rewards 3.0, mean loss 0.01631003048285634\n",
      "Episode 2247 finished after 82 timesteps, total rewards 1.0, mean loss 0.0218140489383453\n",
      "Episode 2248 finished after 94 timesteps, total rewards 3.0, mean loss 0.014802207908979518\n",
      "Episode 2249 finished after 182 timesteps, total rewards 4.0, mean loss 0.01860428677141349\n",
      "Episode 2250 finished after 156 timesteps, total rewards 2.0, mean loss 0.018260719157973282\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2251 finished after 186 timesteps, total rewards 4.0, mean loss 0.01864951977697802\n",
      "Episode 2252 finished after 218 timesteps, total rewards 7.0, mean loss 0.0177052613460446\n",
      "Episode 2253 finished after 203 timesteps, total rewards 2.0, mean loss 0.01776740190273683\n",
      "Episode 2254 finished after 168 timesteps, total rewards 2.0, mean loss 0.01561564686042922\n",
      "Episode 2255 finished after 128 timesteps, total rewards 5.0, mean loss 0.018543267756285786\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 71.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2256 finished after 83 timesteps, total rewards 0.0, mean loss 0.02043110142456345\n",
      "Episode 2257 finished after 86 timesteps, total rewards 2.0, mean loss 0.01877880418472815\n",
      "Episode 2258 finished after 143 timesteps, total rewards 1.0, mean loss 0.018244331016309456\n",
      "Episode 2259 finished after 171 timesteps, total rewards 0.0, mean loss 0.018656372745883547\n",
      "Episode 2260 finished after 189 timesteps, total rewards 7.0, mean loss 0.012945541867151088\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 2261 finished after 229 timesteps, total rewards 7.0, mean loss 0.016969481264251206\n",
      "Episode 2262 finished after 183 timesteps, total rewards 6.0, mean loss 0.01716968271298629\n",
      "Episode 2263 finished after 171 timesteps, total rewards 6.0, mean loss 0.01869212342249175\n",
      "Episode 2264 finished after 90 timesteps, total rewards 1.0, mean loss 0.014825530332745984\n",
      "Episode 2265 finished after 218 timesteps, total rewards 7.0, mean loss 0.016911211933026415\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2266 finished after 216 timesteps, total rewards 3.0, mean loss 0.017304815983893437\n",
      "Episode 2267 finished after 98 timesteps, total rewards 1.0, mean loss 0.018104369171814308\n",
      "Episode 2268 finished after 164 timesteps, total rewards 2.0, mean loss 0.020539025228794257\n",
      "Episode 2269 finished after 102 timesteps, total rewards 3.0, mean loss 0.016505746227944745\n",
      "Episode 2270 finished after 194 timesteps, total rewards 3.0, mean loss 0.01732629019774247\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 2271 finished after 102 timesteps, total rewards 5.0, mean loss 0.021414671587419932\n",
      "Episode 2272 finished after 166 timesteps, total rewards 6.0, mean loss 0.018537740889692633\n",
      "Episode 2273 finished after 286 timesteps, total rewards 7.0, mean loss 0.017997857454514547\n",
      "Episode 2274 finished after 172 timesteps, total rewards 4.0, mean loss 0.017392376587024975\n",
      "Episode 2275 finished after 130 timesteps, total rewards 3.0, mean loss 0.01660070499772421\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 2276 finished after 200 timesteps, total rewards 2.0, mean loss 0.014604050952475517\n",
      "Episode 2277 finished after 166 timesteps, total rewards 4.0, mean loss 0.02092042362846221\n",
      "Episode 2278 finished after 210 timesteps, total rewards 4.0, mean loss 0.018132517098759612\n",
      "Episode 2279 finished after 131 timesteps, total rewards 5.0, mean loss 0.01819992290795532\n",
      "Episode 2280 finished after 92 timesteps, total rewards 2.0, mean loss 0.01905242170450156\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 2281 finished after 311 timesteps, total rewards 5.0, mean loss 0.018506069461571\n",
      "Episode 2282 finished after 165 timesteps, total rewards 3.0, mean loss 0.017019745555466436\n",
      "Episode 2283 finished after 135 timesteps, total rewards 2.0, mean loss 0.01653471463014958\n",
      "Episode 2284 finished after 290 timesteps, total rewards 7.0, mean loss 0.015028051686762222\n",
      "Episode 2285 finished after 101 timesteps, total rewards 1.0, mean loss 0.0162347937942169\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 2286 finished after 217 timesteps, total rewards 5.0, mean loss 0.016810184223942183\n",
      "Episode 2287 finished after 214 timesteps, total rewards 4.0, mean loss 0.01854338033900544\n",
      "Episode 2288 finished after 217 timesteps, total rewards 4.0, mean loss 0.017864532231981974\n",
      "Episode 2289 finished after 195 timesteps, total rewards 3.0, mean loss 0.019040516924817497\n",
      "Episode 2290 finished after 206 timesteps, total rewards 4.0, mean loss 0.01751448147793445\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 2291 finished after 301 timesteps, total rewards 6.0, mean loss 0.016389492793206707\n",
      "Episode 2292 finished after 93 timesteps, total rewards 1.0, mean loss 0.015078605319778886\n",
      "Episode 2293 finished after 215 timesteps, total rewards 3.0, mean loss 0.017840821243995844\n",
      "Episode 2294 finished after 212 timesteps, total rewards 7.0, mean loss 0.017704878611389092\n",
      "Episode 2295 finished after 124 timesteps, total rewards 0.0, mean loss 0.018032735959070945\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 2296 finished after 126 timesteps, total rewards 0.0, mean loss 0.018138048333293272\n",
      "Episode 2297 finished after 172 timesteps, total rewards 2.0, mean loss 0.017147611945780905\n",
      "Episode 2298 finished after 99 timesteps, total rewards 1.0, mean loss 0.020465551224076235\n",
      "Episode 2299 finished after 242 timesteps, total rewards 7.0, mean loss 0.019899276735343824\n",
      "Episode 2300 finished after 258 timesteps, total rewards 3.0, mean loss 0.01635265491890194\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 2301 finished after 397 timesteps, total rewards 9.0, mean loss 0.015095483473858676\n",
      "Episode 2302 finished after 149 timesteps, total rewards 2.0, mean loss 0.0171196010735686\n",
      "Episode 2303 finished after 208 timesteps, total rewards 4.0, mean loss 0.019836569938958677\n",
      "Episode 2304 finished after 246 timesteps, total rewards 8.0, mean loss 0.017340069340695288\n",
      "Episode 2305 finished after 203 timesteps, total rewards 6.0, mean loss 0.017134023673736072\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 2306 finished after 210 timesteps, total rewards 4.0, mean loss 0.018591230929762656\n",
      "Episode 2307 finished after 94 timesteps, total rewards 3.0, mean loss 0.01777878179997602\n",
      "Episode 2308 finished after 79 timesteps, total rewards 1.0, mean loss 0.019535254901481343\n",
      "Episode 2309 finished after 93 timesteps, total rewards 0.0, mean loss 0.020112499984617154\n",
      "Episode 2310 finished after 205 timesteps, total rewards 3.0, mean loss 0.01748161658122227\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 71.666667\n",
      "---------------------------------------\n",
      "Episode 2311 finished after 135 timesteps, total rewards 1.0, mean loss 0.017027490474369928\n",
      "Episode 2312 finished after 158 timesteps, total rewards 2.0, mean loss 0.01610213446267291\n",
      "Episode 2313 finished after 125 timesteps, total rewards 2.0, mean loss 0.015615652769804\n",
      "Episode 2314 finished after 131 timesteps, total rewards 1.0, mean loss 0.01779627520235554\n",
      "Episode 2315 finished after 167 timesteps, total rewards 5.0, mean loss 0.016534891132192833\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 2316 finished after 251 timesteps, total rewards 4.0, mean loss 0.018903259284408443\n",
      "Episode 2317 finished after 123 timesteps, total rewards 1.0, mean loss 0.013768188284834983\n",
      "Episode 2318 finished after 123 timesteps, total rewards 1.0, mean loss 0.01570180288375908\n",
      "Episode 2319 finished after 103 timesteps, total rewards 1.0, mean loss 0.012460085750417332\n",
      "Episode 2320 finished after 137 timesteps, total rewards 0.0, mean loss 0.01854889533936257\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 2321 finished after 125 timesteps, total rewards 5.0, mean loss 0.0178556824689731\n",
      "Episode 2322 finished after 228 timesteps, total rewards 3.0, mean loss 0.016353352931394494\n",
      "Episode 2323 finished after 94 timesteps, total rewards 1.0, mean loss 0.018722143641578905\n",
      "Episode 2324 finished after 136 timesteps, total rewards 5.0, mean loss 0.018554911700501928\n",
      "Episode 2325 finished after 270 timesteps, total rewards 6.0, mean loss 0.01757552157807233\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2326 finished after 122 timesteps, total rewards 2.0, mean loss 0.019039510448222798\n",
      "Episode 2327 finished after 158 timesteps, total rewards 1.0, mean loss 0.01563056553544512\n",
      "Episode 2328 finished after 267 timesteps, total rewards 5.0, mean loss 0.01973329805237091\n",
      "Episode 2329 finished after 125 timesteps, total rewards 1.0, mean loss 0.017956957952585072\n",
      "Episode 2330 finished after 125 timesteps, total rewards 2.0, mean loss 0.014392228679265828\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 2331 finished after 209 timesteps, total rewards 8.0, mean loss 0.02053326046435189\n",
      "Episode 2332 finished after 224 timesteps, total rewards 7.0, mean loss 0.017682831367242864\n",
      "Episode 2333 finished after 122 timesteps, total rewards 1.0, mean loss 0.020342002702460883\n",
      "Episode 2334 finished after 208 timesteps, total rewards 4.0, mean loss 0.018990289227916107\n",
      "Episode 2335 finished after 106 timesteps, total rewards 2.0, mean loss 0.01678607555629053\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 2336 finished after 195 timesteps, total rewards 3.0, mean loss 0.017917348600279253\n",
      "Episode 2337 finished after 217 timesteps, total rewards 6.0, mean loss 0.016068192559921007\n",
      "Episode 2338 finished after 120 timesteps, total rewards 4.0, mean loss 0.018402548072723828\n",
      "Episode 2339 finished after 126 timesteps, total rewards 5.0, mean loss 0.014944608386796308\n",
      "Episode 2340 finished after 97 timesteps, total rewards 2.0, mean loss 0.018251090545744933\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 2341 finished after 233 timesteps, total rewards 6.0, mean loss 0.01862053823324576\n",
      "Episode 2342 finished after 161 timesteps, total rewards 3.0, mean loss 0.015856520565637383\n",
      "Episode 2343 finished after 171 timesteps, total rewards 3.0, mean loss 0.01702816001165451\n",
      "Episode 2344 finished after 157 timesteps, total rewards 0.0, mean loss 0.017810551443724496\n",
      "Episode 2345 finished after 94 timesteps, total rewards 5.0, mean loss 0.015556389447970395\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 2346 finished after 103 timesteps, total rewards 1.0, mean loss 0.015788338091439178\n",
      "Episode 2347 finished after 323 timesteps, total rewards 5.0, mean loss 0.018626386756920788\n",
      "Episode 2348 finished after 336 timesteps, total rewards 10.0, mean loss 0.019256932870034472\n",
      "Episode 2349 finished after 121 timesteps, total rewards 1.0, mean loss 0.017794479958497544\n",
      "Episode 2350 finished after 236 timesteps, total rewards 3.0, mean loss 0.019071124861326654\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 2351 finished after 99 timesteps, total rewards 1.0, mean loss 0.01754047705279193\n",
      "Episode 2352 finished after 108 timesteps, total rewards 0.0, mean loss 0.01806664107425604\n",
      "Episode 2353 finished after 130 timesteps, total rewards 2.0, mean loss 0.01849315152816976\n",
      "Episode 2354 finished after 247 timesteps, total rewards 4.0, mean loss 0.02091463031845831\n",
      "Episode 2355 finished after 189 timesteps, total rewards 2.0, mean loss 0.017304745559129746\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 2356 finished after 79 timesteps, total rewards 2.0, mean loss 0.018992617681380834\n",
      "Episode 2357 finished after 125 timesteps, total rewards 1.0, mean loss 0.016440507176332175\n",
      "Episode 2358 finished after 157 timesteps, total rewards 5.0, mean loss 0.01992740172182453\n",
      "Episode 2359 finished after 263 timesteps, total rewards 4.0, mean loss 0.01871140208095312\n",
      "Episode 2360 finished after 88 timesteps, total rewards 1.0, mean loss 0.013738064010712233\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 2361 finished after 86 timesteps, total rewards 1.0, mean loss 0.01969997300549822\n",
      "Episode 2362 finished after 230 timesteps, total rewards 3.0, mean loss 0.019389580313430126\n",
      "Episode 2363 finished after 103 timesteps, total rewards 2.0, mean loss 0.01459832827999139\n",
      "Episode 2364 finished after 109 timesteps, total rewards 1.0, mean loss 0.017876062450349468\n",
      "Episode 2365 finished after 225 timesteps, total rewards 5.0, mean loss 0.01911691795905224\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 2366 finished after 203 timesteps, total rewards 2.0, mean loss 0.013970446924557406\n",
      "Episode 2367 finished after 98 timesteps, total rewards 2.0, mean loss 0.016586550783451493\n",
      "Episode 2368 finished after 105 timesteps, total rewards 0.0, mean loss 0.013734148176098686\n",
      "Episode 2369 finished after 129 timesteps, total rewards 3.0, mean loss 0.014992440606254449\n",
      "Episode 2370 finished after 119 timesteps, total rewards 2.0, mean loss 0.017050203375181795\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 2371 finished after 173 timesteps, total rewards 1.0, mean loss 0.01614130821921513\n",
      "Episode 2372 finished after 266 timesteps, total rewards 5.0, mean loss 0.01810619577194767\n",
      "Episode 2373 finished after 234 timesteps, total rewards 1.0, mean loss 0.017839924285076875\n",
      "Episode 2374 finished after 117 timesteps, total rewards 0.0, mean loss 0.016786513013793077\n",
      "Episode 2375 finished after 95 timesteps, total rewards 0.0, mean loss 0.015106590253938186\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 2376 finished after 83 timesteps, total rewards 2.0, mean loss 0.01647587015548811\n",
      "Episode 2377 finished after 144 timesteps, total rewards 2.0, mean loss 0.015247576908425091\n",
      "Episode 2378 finished after 79 timesteps, total rewards 0.0, mean loss 0.016978234319381794\n",
      "Episode 2379 finished after 140 timesteps, total rewards 3.0, mean loss 0.01444737733670211\n",
      "Episode 2380 finished after 246 timesteps, total rewards 6.0, mean loss 0.014228869741593252\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 2381 finished after 95 timesteps, total rewards 0.0, mean loss 0.018568415378563498\n",
      "Episode 2382 finished after 164 timesteps, total rewards 3.0, mean loss 0.016233920633278946\n",
      "Episode 2383 finished after 211 timesteps, total rewards 4.0, mean loss 0.015402872381584004\n",
      "Episode 2384 finished after 114 timesteps, total rewards 0.0, mean loss 0.018892864894791785\n",
      "Episode 2385 finished after 244 timesteps, total rewards 5.0, mean loss 0.016890342361517974\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 2386 finished after 282 timesteps, total rewards 8.0, mean loss 0.016415966636912423\n",
      "Episode 2387 finished after 106 timesteps, total rewards 1.0, mean loss 0.015971682943241177\n",
      "Episode 2388 finished after 121 timesteps, total rewards 1.0, mean loss 0.019647962970789985\n",
      "Episode 2389 finished after 175 timesteps, total rewards 2.0, mean loss 0.01667974359755005\n",
      "Episode 2390 finished after 150 timesteps, total rewards 0.0, mean loss 0.015107755648205056\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 2391 finished after 222 timesteps, total rewards 6.0, mean loss 0.01798103904867595\n",
      "Episode 2392 finished after 85 timesteps, total rewards 5.0, mean loss 0.016215811569161492\n",
      "Episode 2393 finished after 163 timesteps, total rewards 2.0, mean loss 0.019050553824218344\n",
      "Episode 2394 finished after 132 timesteps, total rewards 1.0, mean loss 0.017341876357518646\n",
      "Episode 2395 finished after 144 timesteps, total rewards 1.0, mean loss 0.018766937477873096\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2396 finished after 109 timesteps, total rewards 2.0, mean loss 0.015034443090046081\n",
      "Episode 2397 finished after 179 timesteps, total rewards 3.0, mean loss 0.016171152858250885\n",
      "Episode 2398 finished after 324 timesteps, total rewards 7.0, mean loss 0.018475744810406647\n",
      "Episode 2399 finished after 162 timesteps, total rewards 2.0, mean loss 0.01549071824608549\n",
      "Episode 2400 finished after 225 timesteps, total rewards 4.0, mean loss 0.019259451283141972\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2401 finished after 263 timesteps, total rewards 5.0, mean loss 0.017496632580280645\n",
      "Episode 2402 finished after 151 timesteps, total rewards 3.0, mean loss 0.017947088973288308\n",
      "Episode 2403 finished after 131 timesteps, total rewards 0.0, mean loss 0.01861617690017127\n",
      "Episode 2404 finished after 233 timesteps, total rewards 6.0, mean loss 0.014984225782080125\n",
      "Episode 2405 finished after 165 timesteps, total rewards 3.0, mean loss 0.019336847890746977\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 2406 finished after 301 timesteps, total rewards 7.0, mean loss 0.015432040431409876\n",
      "Episode 2407 finished after 156 timesteps, total rewards 3.0, mean loss 0.015461030248657037\n",
      "Episode 2408 finished after 94 timesteps, total rewards 4.0, mean loss 0.016387288108160918\n",
      "Episode 2409 finished after 167 timesteps, total rewards 2.0, mean loss 0.015940658639751176\n",
      "Episode 2410 finished after 204 timesteps, total rewards 3.0, mean loss 0.01618794849463145\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 2411 finished after 206 timesteps, total rewards 7.0, mean loss 0.0148917410824475\n",
      "Episode 2412 finished after 109 timesteps, total rewards 0.0, mean loss 0.01644982216097927\n",
      "Episode 2413 finished after 161 timesteps, total rewards 3.0, mean loss 0.016075675024344406\n",
      "Episode 2414 finished after 309 timesteps, total rewards 5.0, mean loss 0.018119555628258722\n",
      "Episode 2415 finished after 166 timesteps, total rewards 0.0, mean loss 0.016235029078733892\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2416 finished after 199 timesteps, total rewards 7.0, mean loss 0.017239217137468597\n",
      "Episode 2417 finished after 140 timesteps, total rewards 0.0, mean loss 0.015971843450928905\n",
      "Episode 2418 finished after 130 timesteps, total rewards 1.0, mean loss 0.015793836811252942\n",
      "Episode 2419 finished after 206 timesteps, total rewards 4.0, mean loss 0.0161385376673775\n",
      "Episode 2420 finished after 134 timesteps, total rewards 1.0, mean loss 0.016603488914248076\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 2421 finished after 287 timesteps, total rewards 5.0, mean loss 0.015564494308847642\n",
      "Episode 2422 finished after 162 timesteps, total rewards 3.0, mean loss 0.013346986887713515\n",
      "Episode 2423 finished after 209 timesteps, total rewards 1.0, mean loss 0.015899056311681172\n",
      "Episode 2424 finished after 270 timesteps, total rewards 4.0, mean loss 0.017077064900486556\n",
      "Episode 2425 finished after 119 timesteps, total rewards 4.0, mean loss 0.015892897877830447\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 2426 finished after 158 timesteps, total rewards 4.0, mean loss 0.020055292561779955\n",
      "Episode 2427 finished after 451 timesteps, total rewards 9.0, mean loss 0.016759868639602868\n",
      "Episode 2428 finished after 137 timesteps, total rewards 3.0, mean loss 0.017051119853599663\n",
      "Episode 2429 finished after 124 timesteps, total rewards 1.0, mean loss 0.020285423356859435\n",
      "Episode 2430 finished after 182 timesteps, total rewards 3.0, mean loss 0.01528620575829716\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 2431 finished after 165 timesteps, total rewards 1.0, mean loss 0.019121228179993163\n",
      "Episode 2432 finished after 126 timesteps, total rewards 2.0, mean loss 0.018072569291920415\n",
      "Episode 2433 finished after 260 timesteps, total rewards 7.0, mean loss 0.016118010618419457\n",
      "Episode 2434 finished after 77 timesteps, total rewards 2.0, mean loss 0.023192429437314147\n",
      "Episode 2435 finished after 93 timesteps, total rewards 3.0, mean loss 0.018009822658558567\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 2436 finished after 166 timesteps, total rewards 0.0, mean loss 0.016821627852966987\n",
      "Episode 2437 finished after 323 timesteps, total rewards 7.0, mean loss 0.018592349416504242\n",
      "Episode 2438 finished after 153 timesteps, total rewards 1.0, mean loss 0.01660816136251502\n",
      "Episode 2439 finished after 148 timesteps, total rewards 1.0, mean loss 0.017315693064734364\n",
      "Episode 2440 finished after 181 timesteps, total rewards 1.0, mean loss 0.017207248954845912\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2441 finished after 205 timesteps, total rewards 3.0, mean loss 0.016290820451033097\n",
      "Episode 2442 finished after 286 timesteps, total rewards 6.0, mean loss 0.015701135086836748\n",
      "Episode 2443 finished after 105 timesteps, total rewards 1.0, mean loss 0.01647409491152281\n",
      "Episode 2444 finished after 94 timesteps, total rewards 1.0, mean loss 0.016701889843580887\n",
      "Episode 2445 finished after 158 timesteps, total rewards 4.0, mean loss 0.015932820373379827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 2446 finished after 280 timesteps, total rewards 7.0, mean loss 0.016698434643746752\n",
      "Episode 2447 finished after 234 timesteps, total rewards 4.0, mean loss 0.015783660436192386\n",
      "Episode 2448 finished after 123 timesteps, total rewards 0.0, mean loss 0.015476095085306759\n",
      "Episode 2449 finished after 132 timesteps, total rewards 0.0, mean loss 0.014119728839211872\n",
      "Episode 2450 finished after 252 timesteps, total rewards 3.0, mean loss 0.01584288548416687\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 2451 finished after 126 timesteps, total rewards 2.0, mean loss 0.016633033679912812\n",
      "Episode 2452 finished after 121 timesteps, total rewards 3.0, mean loss 0.018232323352462988\n",
      "Episode 2453 finished after 252 timesteps, total rewards 4.0, mean loss 0.016447634241702626\n",
      "Episode 2454 finished after 239 timesteps, total rewards 4.0, mean loss 0.016715957776343285\n",
      "Episode 2455 finished after 249 timesteps, total rewards 6.0, mean loss 0.0141903388412832\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 103.333333\n",
      "---------------------------------------\n",
      "Episode 2456 finished after 132 timesteps, total rewards 1.0, mean loss 0.017184322995352417\n",
      "Episode 2457 finished after 176 timesteps, total rewards 1.0, mean loss 0.018459658832977188\n",
      "Episode 2458 finished after 149 timesteps, total rewards 3.0, mean loss 0.020101348096004768\n",
      "Episode 2459 finished after 134 timesteps, total rewards 1.0, mean loss 0.015118989378471039\n",
      "Episode 2460 finished after 281 timesteps, total rewards 7.0, mean loss 0.016285408201032515\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 2461 finished after 93 timesteps, total rewards 1.0, mean loss 0.01801879548152248\n",
      "Episode 2462 finished after 282 timesteps, total rewards 2.0, mean loss 0.015524192689670289\n",
      "Episode 2463 finished after 161 timesteps, total rewards 2.0, mean loss 0.018578485859433617\n",
      "Episode 2464 finished after 168 timesteps, total rewards 2.0, mean loss 0.018094226749478064\n",
      "Episode 2465 finished after 274 timesteps, total rewards 4.0, mean loss 0.01767750575192728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 2466 finished after 148 timesteps, total rewards 3.0, mean loss 0.017662073332168213\n",
      "Episode 2467 finished after 152 timesteps, total rewards 1.0, mean loss 0.01580696295231495\n",
      "Episode 2468 finished after 120 timesteps, total rewards 5.0, mean loss 0.017166557348779556\n",
      "Episode 2469 finished after 145 timesteps, total rewards 3.0, mean loss 0.01713835282054537\n",
      "Episode 2470 finished after 224 timesteps, total rewards 5.0, mean loss 0.017029445540174493\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 2471 finished after 232 timesteps, total rewards 6.0, mean loss 0.015683286855195985\n",
      "Episode 2472 finished after 92 timesteps, total rewards 2.0, mean loss 0.016575456207805393\n",
      "Episode 2473 finished after 158 timesteps, total rewards 4.0, mean loss 0.014823566628929486\n",
      "Episode 2474 finished after 218 timesteps, total rewards 0.0, mean loss 0.014486340991725478\n",
      "Episode 2475 finished after 124 timesteps, total rewards 0.0, mean loss 0.018388081865850836\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2476 finished after 152 timesteps, total rewards 4.0, mean loss 0.01631553525160263\n",
      "Episode 2477 finished after 155 timesteps, total rewards 4.0, mean loss 0.01504960237972198\n",
      "Episode 2478 finished after 133 timesteps, total rewards 1.0, mean loss 0.016904871249518925\n",
      "Episode 2479 finished after 196 timesteps, total rewards 3.0, mean loss 0.016702662828813154\n",
      "Episode 2480 finished after 127 timesteps, total rewards 0.0, mean loss 0.017273675714524126\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 81.666667\n",
      "---------------------------------------\n",
      "Episode 2481 finished after 166 timesteps, total rewards 5.0, mean loss 0.019055935991465125\n",
      "Episode 2482 finished after 216 timesteps, total rewards 2.0, mean loss 0.017383381523864552\n",
      "Episode 2483 finished after 304 timesteps, total rewards 8.0, mean loss 0.016766800660996905\n",
      "Episode 2484 finished after 206 timesteps, total rewards 5.0, mean loss 0.0175975391152993\n",
      "Episode 2485 finished after 278 timesteps, total rewards 6.0, mean loss 0.015132062710409568\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 2486 finished after 149 timesteps, total rewards 4.0, mean loss 0.015471154998912908\n",
      "Episode 2487 finished after 170 timesteps, total rewards 1.0, mean loss 0.015460302976562696\n",
      "Episode 2488 finished after 128 timesteps, total rewards 1.0, mean loss 0.017014677202496387\n",
      "Episode 2489 finished after 173 timesteps, total rewards 1.0, mean loss 0.01577532387404525\n",
      "Episode 2490 finished after 237 timesteps, total rewards 5.0, mean loss 0.017078445352936157\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 2491 finished after 189 timesteps, total rewards 2.0, mean loss 0.015392532859184833\n",
      "Episode 2492 finished after 119 timesteps, total rewards 1.0, mean loss 0.0186878020788089\n",
      "Episode 2493 finished after 122 timesteps, total rewards 0.0, mean loss 0.01642586169029647\n",
      "Episode 2494 finished after 172 timesteps, total rewards 2.0, mean loss 0.01652938448387431\n",
      "Episode 2495 finished after 287 timesteps, total rewards 7.0, mean loss 0.014508787396758184\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2496 finished after 139 timesteps, total rewards 1.0, mean loss 0.014204911172822144\n",
      "Episode 2497 finished after 232 timesteps, total rewards 7.0, mean loss 0.015943721553726233\n",
      "Episode 2498 finished after 82 timesteps, total rewards 2.0, mean loss 0.01548931352998607\n",
      "Episode 2499 finished after 159 timesteps, total rewards 5.0, mean loss 0.01501071136576598\n",
      "Episode 2500 finished after 185 timesteps, total rewards 6.0, mean loss 0.015954392993935963\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 2501 finished after 157 timesteps, total rewards 2.0, mean loss 0.016687439964820222\n",
      "Episode 2502 finished after 130 timesteps, total rewards 0.0, mean loss 0.015053603796807763\n",
      "Episode 2503 finished after 90 timesteps, total rewards 1.0, mean loss 0.013025334549860821\n",
      "Episode 2504 finished after 98 timesteps, total rewards 3.0, mean loss 0.018924130798003883\n",
      "Episode 2505 finished after 292 timesteps, total rewards 5.0, mean loss 0.017070431904931082\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 2506 finished after 86 timesteps, total rewards 0.0, mean loss 0.015342142190621777\n",
      "Episode 2507 finished after 258 timesteps, total rewards 4.0, mean loss 0.015486606490814538\n",
      "Episode 2508 finished after 243 timesteps, total rewards 4.0, mean loss 0.017265674895741077\n",
      "Episode 2509 finished after 194 timesteps, total rewards 5.0, mean loss 0.015216662749508876\n",
      "Episode 2510 finished after 243 timesteps, total rewards 4.0, mean loss 0.01768552511690928\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2511 finished after 120 timesteps, total rewards 1.0, mean loss 0.014519093199730075\n",
      "Episode 2512 finished after 236 timesteps, total rewards 4.0, mean loss 0.015722299577600237\n",
      "Episode 2513 finished after 238 timesteps, total rewards 3.0, mean loss 0.017103401475934265\n",
      "Episode 2514 finished after 206 timesteps, total rewards 3.0, mean loss 0.016553634259164857\n",
      "Episode 2515 finished after 165 timesteps, total rewards 4.0, mean loss 0.018033611128868705\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 130.000000\n",
      "---------------------------------------\n",
      "Episode 2516 finished after 252 timesteps, total rewards 5.0, mean loss 0.017042372198562906\n",
      "Episode 2517 finished after 86 timesteps, total rewards 2.0, mean loss 0.02094778550658808\n",
      "Episode 2518 finished after 216 timesteps, total rewards 1.0, mean loss 0.01729485409628574\n",
      "Episode 2519 finished after 103 timesteps, total rewards 1.0, mean loss 0.015296749776292556\n",
      "Episode 2520 finished after 237 timesteps, total rewards 7.0, mean loss 0.014195344741416914\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 2521 finished after 120 timesteps, total rewards 2.0, mean loss 0.015776267784531228\n",
      "Episode 2522 finished after 197 timesteps, total rewards 3.0, mean loss 0.01744043751711943\n",
      "Episode 2523 finished after 259 timesteps, total rewards 1.0, mean loss 0.015789741337610387\n",
      "Episode 2524 finished after 100 timesteps, total rewards 2.0, mean loss 0.017916398855741134\n",
      "Episode 2525 finished after 142 timesteps, total rewards 3.0, mean loss 0.01595736771877724\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 2526 finished after 205 timesteps, total rewards 4.0, mean loss 0.013229684536941577\n",
      "Episode 2527 finished after 163 timesteps, total rewards 5.0, mean loss 0.01686783839660651\n",
      "Episode 2528 finished after 134 timesteps, total rewards 0.0, mean loss 0.019473604731515413\n",
      "Episode 2529 finished after 127 timesteps, total rewards 1.0, mean loss 0.016642071574827378\n",
      "Episode 2530 finished after 346 timesteps, total rewards 11.0, mean loss 0.015657095977265355\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 98.333333\n",
      "---------------------------------------\n",
      "Episode 2531 finished after 130 timesteps, total rewards 1.0, mean loss 0.018867101193441507\n",
      "Episode 2532 finished after 199 timesteps, total rewards 7.0, mean loss 0.016541309195193907\n",
      "Episode 2533 finished after 130 timesteps, total rewards 0.0, mean loss 0.01905803469529089\n",
      "Episode 2534 finished after 244 timesteps, total rewards 6.0, mean loss 0.01752295247808702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2535 finished after 109 timesteps, total rewards 0.0, mean loss 0.012629962374067464\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 2536 finished after 175 timesteps, total rewards 4.0, mean loss 0.016935473836492746\n",
      "Episode 2537 finished after 229 timesteps, total rewards 4.0, mean loss 0.018983446097450826\n",
      "Episode 2538 finished after 150 timesteps, total rewards 2.0, mean loss 0.016424835176828006\n",
      "Episode 2539 finished after 137 timesteps, total rewards 2.0, mean loss 0.01456201635173651\n",
      "Episode 2540 finished after 159 timesteps, total rewards 4.0, mean loss 0.019962645845660508\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 2541 finished after 300 timesteps, total rewards 8.0, mean loss 0.014414355407546585\n",
      "Episode 2542 finished after 203 timesteps, total rewards 9.0, mean loss 0.0166070212462683\n",
      "Episode 2543 finished after 350 timesteps, total rewards 5.0, mean loss 0.018170537671207316\n",
      "Episode 2544 finished after 91 timesteps, total rewards 2.0, mean loss 0.016495740250300536\n",
      "Episode 2545 finished after 173 timesteps, total rewards 0.0, mean loss 0.018183130864794176\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 2546 finished after 231 timesteps, total rewards 6.0, mean loss 0.016766576772481123\n",
      "Episode 2547 finished after 203 timesteps, total rewards 4.0, mean loss 0.018269903305516547\n",
      "Episode 2548 finished after 140 timesteps, total rewards 3.0, mean loss 0.017109344507584216\n",
      "Episode 2549 finished after 88 timesteps, total rewards 3.0, mean loss 0.014513315560179763\n",
      "Episode 2550 finished after 221 timesteps, total rewards 5.0, mean loss 0.017251700474469757\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 3.333333\n",
      "---------------------------------------\n",
      "Episode 2551 finished after 222 timesteps, total rewards 2.0, mean loss 0.015804680758092955\n",
      "Episode 2552 finished after 100 timesteps, total rewards 1.0, mean loss 0.017755333844106644\n",
      "Episode 2553 finished after 157 timesteps, total rewards 4.0, mean loss 0.02011619900185674\n",
      "Episode 2554 finished after 90 timesteps, total rewards 1.0, mean loss 0.016492627408458953\n",
      "Episode 2555 finished after 264 timesteps, total rewards 8.0, mean loss 0.01634793144021938\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 2556 finished after 125 timesteps, total rewards 3.0, mean loss 0.016082109114155172\n",
      "Episode 2557 finished after 211 timesteps, total rewards 4.0, mean loss 0.014404707625221409\n",
      "Episode 2558 finished after 160 timesteps, total rewards 4.0, mean loss 0.018573817336073262\n",
      "Episode 2559 finished after 170 timesteps, total rewards 1.0, mean loss 0.015289891281452797\n",
      "Episode 2560 finished after 130 timesteps, total rewards 1.0, mean loss 0.01868432409246452\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 2561 finished after 181 timesteps, total rewards 5.0, mean loss 0.016515751548101146\n",
      "Episode 2562 finished after 201 timesteps, total rewards 3.0, mean loss 0.01824544441761143\n",
      "Episode 2563 finished after 103 timesteps, total rewards 0.0, mean loss 0.017301055875118567\n",
      "Episode 2564 finished after 84 timesteps, total rewards 0.0, mean loss 0.016527432640335366\n",
      "Episode 2565 finished after 128 timesteps, total rewards 2.0, mean loss 0.01633864175346389\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 2566 finished after 206 timesteps, total rewards 8.0, mean loss 0.01819249645052975\n",
      "Episode 2567 finished after 288 timesteps, total rewards 5.0, mean loss 0.018205297065732238\n",
      "Episode 2568 finished after 261 timesteps, total rewards 3.0, mean loss 0.019944739211761032\n",
      "Episode 2569 finished after 157 timesteps, total rewards 2.0, mean loss 0.01902839520339599\n",
      "Episode 2570 finished after 123 timesteps, total rewards 1.0, mean loss 0.017702475484986827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 2571 finished after 93 timesteps, total rewards 1.0, mean loss 0.01648977400750042\n",
      "Episode 2572 finished after 161 timesteps, total rewards 3.0, mean loss 0.014706568712562392\n",
      "Episode 2573 finished after 152 timesteps, total rewards 2.0, mean loss 0.01569808474902713\n",
      "Episode 2574 finished after 141 timesteps, total rewards 3.0, mean loss 0.019444116655033716\n",
      "Episode 2575 finished after 130 timesteps, total rewards 4.0, mean loss 0.01613332987986863\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 2576 finished after 161 timesteps, total rewards 4.0, mean loss 0.017568225542123873\n",
      "Episode 2577 finished after 175 timesteps, total rewards 2.0, mean loss 0.017475984233564563\n",
      "Episode 2578 finished after 204 timesteps, total rewards 3.0, mean loss 0.01704595836710331\n",
      "Episode 2579 finished after 236 timesteps, total rewards 4.0, mean loss 0.015507806380825922\n",
      "Episode 2580 finished after 176 timesteps, total rewards 5.0, mean loss 0.01814526690842203\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2581 finished after 123 timesteps, total rewards 1.0, mean loss 0.01520874558681077\n",
      "Episode 2582 finished after 305 timesteps, total rewards 3.0, mean loss 0.01604712012311688\n",
      "Episode 2583 finished after 159 timesteps, total rewards 5.0, mean loss 0.01515540943016831\n",
      "Episode 2584 finished after 202 timesteps, total rewards 4.0, mean loss 0.014454824707901917\n",
      "Episode 2585 finished after 135 timesteps, total rewards 1.0, mean loss 0.019974848809565796\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 2586 finished after 201 timesteps, total rewards 4.0, mean loss 0.017999125733179275\n",
      "Episode 2587 finished after 227 timesteps, total rewards 2.0, mean loss 0.01638913342020462\n",
      "Episode 2588 finished after 182 timesteps, total rewards 3.0, mean loss 0.015468671057593322\n",
      "Episode 2589 finished after 170 timesteps, total rewards 1.0, mean loss 0.017310029873624445\n",
      "Episode 2590 finished after 133 timesteps, total rewards 0.0, mean loss 0.01627754971439271\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2591 finished after 183 timesteps, total rewards 2.0, mean loss 0.017844185505706125\n",
      "Episode 2592 finished after 182 timesteps, total rewards 0.0, mean loss 0.0174450323970382\n",
      "Episode 2593 finished after 135 timesteps, total rewards 0.0, mean loss 0.018114847654511254\n",
      "Episode 2594 finished after 310 timesteps, total rewards 3.0, mean loss 0.015125572236044512\n",
      "Episode 2595 finished after 87 timesteps, total rewards 0.0, mean loss 0.016038992398033112\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 2596 finished after 267 timesteps, total rewards 7.0, mean loss 0.01536959904130053\n",
      "Episode 2597 finished after 81 timesteps, total rewards 1.0, mean loss 0.017004299388600538\n",
      "Episode 2598 finished after 271 timesteps, total rewards 9.0, mean loss 0.01639300142651135\n",
      "Episode 2599 finished after 222 timesteps, total rewards 3.0, mean loss 0.017212449907281158\n",
      "Episode 2600 finished after 130 timesteps, total rewards 0.0, mean loss 0.01730272438009986\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 2601 finished after 238 timesteps, total rewards 3.0, mean loss 0.017004335240926594\n",
      "Episode 2602 finished after 130 timesteps, total rewards 2.0, mean loss 0.016092196869878814\n",
      "Episode 2603 finished after 263 timesteps, total rewards 4.0, mean loss 0.015445212094368194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2604 finished after 258 timesteps, total rewards 4.0, mean loss 0.01497157041413002\n",
      "Episode 2605 finished after 165 timesteps, total rewards 4.0, mean loss 0.016008771518174786\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2606 finished after 236 timesteps, total rewards 3.0, mean loss 0.017409879985187494\n",
      "Episode 2607 finished after 151 timesteps, total rewards 1.0, mean loss 0.014972007058604022\n",
      "Episode 2608 finished after 286 timesteps, total rewards 5.0, mean loss 0.01688296181354118\n",
      "Episode 2609 finished after 212 timesteps, total rewards 3.0, mean loss 0.01914534349964255\n",
      "Episode 2610 finished after 307 timesteps, total rewards 5.0, mean loss 0.0167047313330331\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 2611 finished after 122 timesteps, total rewards 2.0, mean loss 0.014409822274426946\n",
      "Episode 2612 finished after 212 timesteps, total rewards 5.0, mean loss 0.016318595252946495\n",
      "Episode 2613 finished after 203 timesteps, total rewards 5.0, mean loss 0.014651071884969865\n",
      "Episode 2614 finished after 126 timesteps, total rewards 1.0, mean loss 0.01656543442495315\n",
      "Episode 2615 finished after 172 timesteps, total rewards 4.0, mean loss 0.016821964292280743\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 2616 finished after 244 timesteps, total rewards 7.0, mean loss 0.017773668229609508\n",
      "Episode 2617 finished after 173 timesteps, total rewards 5.0, mean loss 0.01784365653764252\n",
      "Episode 2618 finished after 216 timesteps, total rewards 6.0, mean loss 0.01708392547395003\n",
      "Episode 2619 finished after 247 timesteps, total rewards 3.0, mean loss 0.017370052835743815\n",
      "Episode 2620 finished after 235 timesteps, total rewards 5.0, mean loss 0.015296860051123386\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 2621 finished after 239 timesteps, total rewards 6.0, mean loss 0.015511483462307228\n",
      "Episode 2622 finished after 140 timesteps, total rewards 2.0, mean loss 0.014192844097436007\n",
      "Episode 2623 finished after 129 timesteps, total rewards 0.0, mean loss 0.01807058293755029\n",
      "Episode 2624 finished after 172 timesteps, total rewards 1.0, mean loss 0.01580571292948281\n",
      "Episode 2625 finished after 262 timesteps, total rewards 5.0, mean loss 0.01753825031466185\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 2626 finished after 245 timesteps, total rewards 6.0, mean loss 0.014980211618774551\n",
      "Episode 2627 finished after 117 timesteps, total rewards 3.0, mean loss 0.02010592016437226\n",
      "Episode 2628 finished after 230 timesteps, total rewards 2.0, mean loss 0.016033933202118572\n",
      "Episode 2629 finished after 166 timesteps, total rewards 1.0, mean loss 0.016487801857769536\n",
      "Episode 2630 finished after 135 timesteps, total rewards 2.0, mean loss 0.015907762481192887\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 2631 finished after 149 timesteps, total rewards 4.0, mean loss 0.020419936373223545\n",
      "Episode 2632 finished after 120 timesteps, total rewards 1.0, mean loss 0.01632088558690157\n",
      "Episode 2633 finished after 101 timesteps, total rewards 2.0, mean loss 0.013944569151585757\n",
      "Episode 2634 finished after 90 timesteps, total rewards 0.0, mean loss 0.018654336537777757\n",
      "Episode 2635 finished after 127 timesteps, total rewards 4.0, mean loss 0.01595326208768002\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 71.666667\n",
      "---------------------------------------\n",
      "Episode 2636 finished after 188 timesteps, total rewards 2.0, mean loss 0.018306105505387756\n",
      "Episode 2637 finished after 125 timesteps, total rewards 0.0, mean loss 0.015381161455996335\n",
      "Episode 2638 finished after 168 timesteps, total rewards 2.0, mean loss 0.016506092540415313\n",
      "Episode 2639 finished after 117 timesteps, total rewards 0.0, mean loss 0.017718469669333953\n",
      "Episode 2640 finished after 233 timesteps, total rewards 6.0, mean loss 0.01542530033678464\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 2641 finished after 269 timesteps, total rewards 4.0, mean loss 0.016286806817310186\n",
      "Episode 2642 finished after 154 timesteps, total rewards 6.0, mean loss 0.018971420363402117\n",
      "Episode 2643 finished after 95 timesteps, total rewards 1.0, mean loss 0.0168081565907127\n",
      "Episode 2644 finished after 196 timesteps, total rewards 1.0, mean loss 0.017923279451883437\n",
      "Episode 2645 finished after 106 timesteps, total rewards 3.0, mean loss 0.018181159092390136\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 108.333333\n",
      "---------------------------------------\n",
      "Episode 2646 finished after 85 timesteps, total rewards 1.0, mean loss 0.015828750505769516\n",
      "Episode 2647 finished after 199 timesteps, total rewards 5.0, mean loss 0.014127692977556183\n",
      "Episode 2648 finished after 222 timesteps, total rewards 3.0, mean loss 0.01495087206225238\n",
      "Episode 2649 finished after 151 timesteps, total rewards 6.0, mean loss 0.02042356361781512\n",
      "Episode 2650 finished after 246 timesteps, total rewards 5.0, mean loss 0.01782451904408645\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 2651 finished after 135 timesteps, total rewards 4.0, mean loss 0.016532582802163368\n",
      "Episode 2652 finished after 256 timesteps, total rewards 3.0, mean loss 0.015192119261200787\n",
      "Episode 2653 finished after 165 timesteps, total rewards 2.0, mean loss 0.01699281657664952\n",
      "Episode 2654 finished after 131 timesteps, total rewards 3.0, mean loss 0.01544536882633777\n",
      "Episode 2655 finished after 239 timesteps, total rewards 5.0, mean loss 0.016971584302558327\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 113.333333\n",
      "---------------------------------------\n",
      "Episode 2656 finished after 180 timesteps, total rewards 6.0, mean loss 0.014959524776269164\n",
      "Episode 2657 finished after 272 timesteps, total rewards 5.0, mean loss 0.01669356706449488\n",
      "Episode 2658 finished after 185 timesteps, total rewards 4.0, mean loss 0.014877804614779716\n",
      "Episode 2659 finished after 158 timesteps, total rewards 2.0, mean loss 0.016823432313962074\n",
      "Episode 2660 finished after 115 timesteps, total rewards 2.0, mean loss 0.017156308820046\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 2661 finished after 88 timesteps, total rewards 3.0, mean loss 0.015806381273168055\n",
      "Episode 2662 finished after 172 timesteps, total rewards 2.0, mean loss 0.017864837982288433\n",
      "Episode 2663 finished after 184 timesteps, total rewards 3.0, mean loss 0.016911912659808244\n",
      "Episode 2664 finished after 214 timesteps, total rewards 3.0, mean loss 0.015574637575240406\n",
      "Episode 2665 finished after 230 timesteps, total rewards 5.0, mean loss 0.016147514255008783\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 6.666667\n",
      "---------------------------------------\n",
      "Episode 2666 finished after 232 timesteps, total rewards 2.0, mean loss 0.013368127452849876\n",
      "Episode 2667 finished after 171 timesteps, total rewards 5.0, mean loss 0.016275100060738623\n",
      "Episode 2668 finished after 120 timesteps, total rewards 0.0, mean loss 0.015774934292130637\n",
      "Episode 2669 finished after 220 timesteps, total rewards 0.0, mean loss 0.01760375121375546\n",
      "Episode 2670 finished after 269 timesteps, total rewards 5.0, mean loss 0.016960757359564583\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2671 finished after 166 timesteps, total rewards 4.0, mean loss 0.01727621393181462\n",
      "Episode 2672 finished after 138 timesteps, total rewards 4.0, mean loss 0.01909015939209907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2673 finished after 179 timesteps, total rewards 3.0, mean loss 0.016296247830068143\n",
      "Episode 2674 finished after 150 timesteps, total rewards 2.0, mean loss 0.015159707726367438\n",
      "Episode 2675 finished after 327 timesteps, total rewards 7.0, mean loss 0.017061797710810574\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 2676 finished after 89 timesteps, total rewards 0.0, mean loss 0.013871753870200869\n",
      "Episode 2677 finished after 140 timesteps, total rewards 6.0, mean loss 0.0166561406384322\n",
      "Episode 2678 finished after 136 timesteps, total rewards 3.0, mean loss 0.015787278320417767\n",
      "Episode 2679 finished after 133 timesteps, total rewards 5.0, mean loss 0.015983862355544016\n",
      "Episode 2680 finished after 87 timesteps, total rewards 0.0, mean loss 0.016947427634738558\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 2681 finished after 201 timesteps, total rewards 5.0, mean loss 0.016743506488230413\n",
      "Episode 2682 finished after 180 timesteps, total rewards 8.0, mean loss 0.019613028807281\n",
      "Episode 2683 finished after 257 timesteps, total rewards 6.0, mean loss 0.018160615449296563\n",
      "Episode 2684 finished after 243 timesteps, total rewards 5.0, mean loss 0.01903559155129816\n",
      "Episode 2685 finished after 88 timesteps, total rewards 1.0, mean loss 0.016882143291613003\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 2686 finished after 252 timesteps, total rewards 8.0, mean loss 0.01993697798129038\n",
      "Episode 2687 finished after 163 timesteps, total rewards 4.0, mean loss 0.014738453428212415\n",
      "Episode 2688 finished after 104 timesteps, total rewards 2.0, mean loss 0.01578178627823945\n",
      "Episode 2689 finished after 169 timesteps, total rewards 6.0, mean loss 0.016245617752360213\n",
      "Episode 2690 finished after 183 timesteps, total rewards 1.0, mean loss 0.019786443125871342\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 2691 finished after 306 timesteps, total rewards 3.0, mean loss 0.01702259109993511\n",
      "Episode 2692 finished after 100 timesteps, total rewards 2.0, mean loss 0.013830531382700429\n",
      "Episode 2693 finished after 171 timesteps, total rewards 2.0, mean loss 0.01683012784863087\n",
      "Episode 2694 finished after 305 timesteps, total rewards 5.0, mean loss 0.016713360663060648\n",
      "Episode 2695 finished after 116 timesteps, total rewards 3.0, mean loss 0.016648209300032687\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2696 finished after 233 timesteps, total rewards 4.0, mean loss 0.017193846768412794\n",
      "Episode 2697 finished after 91 timesteps, total rewards 0.0, mean loss 0.017392778492808996\n",
      "Episode 2698 finished after 157 timesteps, total rewards 5.0, mean loss 0.018693363687370543\n",
      "Episode 2699 finished after 155 timesteps, total rewards 4.0, mean loss 0.01569968083903434\n",
      "Episode 2700 finished after 95 timesteps, total rewards 0.0, mean loss 0.019088057739856213\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 2701 finished after 132 timesteps, total rewards 4.0, mean loss 0.017743771676605596\n",
      "Episode 2702 finished after 419 timesteps, total rewards 10.0, mean loss 0.016427130190553342\n",
      "Episode 2703 finished after 203 timesteps, total rewards 6.0, mean loss 0.015318197052549627\n",
      "Episode 2704 finished after 178 timesteps, total rewards 2.0, mean loss 0.014934304119624051\n",
      "Episode 2705 finished after 127 timesteps, total rewards 3.0, mean loss 0.01691105190754362\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 2706 finished after 286 timesteps, total rewards 8.0, mean loss 0.014230280307404879\n",
      "Episode 2707 finished after 123 timesteps, total rewards 3.0, mean loss 0.013631088328850644\n",
      "Episode 2708 finished after 161 timesteps, total rewards 3.0, mean loss 0.016120408967886444\n",
      "Episode 2709 finished after 164 timesteps, total rewards 3.0, mean loss 0.015347092800699855\n",
      "Episode 2710 finished after 163 timesteps, total rewards 3.0, mean loss 0.018006453151668943\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 2711 finished after 95 timesteps, total rewards 1.0, mean loss 0.015717104392273255\n",
      "Episode 2712 finished after 186 timesteps, total rewards 7.0, mean loss 0.014971351916638894\n",
      "Episode 2713 finished after 156 timesteps, total rewards 5.0, mean loss 0.014678390474634795\n",
      "Episode 2714 finished after 229 timesteps, total rewards 6.0, mean loss 0.017331375651250647\n",
      "Episode 2715 finished after 232 timesteps, total rewards 3.0, mean loss 0.018387843569959435\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 2716 finished after 369 timesteps, total rewards 6.0, mean loss 0.017255581393560298\n",
      "Episode 2717 finished after 129 timesteps, total rewards 3.0, mean loss 0.013568135514916752\n",
      "Episode 2718 finished after 309 timesteps, total rewards 1.0, mean loss 0.01627095292831325\n",
      "Episode 2719 finished after 111 timesteps, total rewards 2.0, mean loss 0.015342823546988101\n",
      "Episode 2720 finished after 174 timesteps, total rewards 1.0, mean loss 0.017092689355278665\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 2721 finished after 256 timesteps, total rewards 3.0, mean loss 0.017554704437316104\n",
      "Episode 2722 finished after 243 timesteps, total rewards 1.0, mean loss 0.016995277166425984\n",
      "Episode 2723 finished after 155 timesteps, total rewards 4.0, mean loss 0.018071135819228666\n",
      "Episode 2724 finished after 122 timesteps, total rewards 0.0, mean loss 0.014574357406732429\n",
      "Episode 2725 finished after 87 timesteps, total rewards 1.0, mean loss 0.018918544278833372\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 2726 finished after 117 timesteps, total rewards 0.0, mean loss 0.014226495453482892\n",
      "Episode 2727 finished after 155 timesteps, total rewards 1.0, mean loss 0.014103923254315892\n",
      "Episode 2728 finished after 182 timesteps, total rewards 7.0, mean loss 0.014121502315162446\n",
      "Episode 2729 finished after 283 timesteps, total rewards 5.0, mean loss 0.01717819431516944\n",
      "Episode 2730 finished after 149 timesteps, total rewards 1.0, mean loss 0.01618555395812384\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 2731 finished after 147 timesteps, total rewards 2.0, mean loss 0.014771347431874625\n",
      "Episode 2732 finished after 135 timesteps, total rewards 1.0, mean loss 0.0156262978251713\n",
      "Episode 2733 finished after 166 timesteps, total rewards 3.0, mean loss 0.01592482871541484\n",
      "Episode 2734 finished after 102 timesteps, total rewards 0.0, mean loss 0.018298202103538\n",
      "Episode 2735 finished after 260 timesteps, total rewards 6.0, mean loss 0.0189024595839258\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 2736 finished after 258 timesteps, total rewards 6.0, mean loss 0.016085428500037222\n",
      "Episode 2737 finished after 212 timesteps, total rewards 7.0, mean loss 0.013215057177080351\n",
      "Episode 2738 finished after 232 timesteps, total rewards 6.0, mean loss 0.016516983443831\n",
      "Episode 2739 finished after 218 timesteps, total rewards 6.0, mean loss 0.014983112375367296\n",
      "Episode 2740 finished after 191 timesteps, total rewards 3.0, mean loss 0.015118575141721293\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 2741 finished after 152 timesteps, total rewards 2.0, mean loss 0.015731041610706598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2742 finished after 300 timesteps, total rewards 5.0, mean loss 0.016604214743128978\n",
      "Episode 2743 finished after 208 timesteps, total rewards 5.0, mean loss 0.01543166544126087\n",
      "Episode 2744 finished after 99 timesteps, total rewards 1.0, mean loss 0.015534862028105617\n",
      "Episode 2745 finished after 149 timesteps, total rewards 2.0, mean loss 0.017665886323298984\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 2746 finished after 172 timesteps, total rewards 1.0, mean loss 0.018227631557830283\n",
      "Episode 2747 finished after 169 timesteps, total rewards 0.0, mean loss 0.01447469717995181\n",
      "Episode 2748 finished after 94 timesteps, total rewards 2.0, mean loss 0.013711543572927885\n",
      "Episode 2749 finished after 240 timesteps, total rewards 6.0, mean loss 0.016015456229797564\n",
      "Episode 2750 finished after 177 timesteps, total rewards 2.0, mean loss 0.016757355629016597\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 2751 finished after 324 timesteps, total rewards 7.0, mean loss 0.014228872642428385\n",
      "Episode 2752 finished after 184 timesteps, total rewards 5.0, mean loss 0.014355442307018877\n",
      "Episode 2753 finished after 92 timesteps, total rewards 0.0, mean loss 0.01593079475645462\n",
      "Episode 2754 finished after 295 timesteps, total rewards 3.0, mean loss 0.017854222378267323\n",
      "Episode 2755 finished after 147 timesteps, total rewards 2.0, mean loss 0.01537578111850232\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 2756 finished after 167 timesteps, total rewards 4.0, mean loss 0.017687232280289476\n",
      "Episode 2757 finished after 283 timesteps, total rewards 10.0, mean loss 0.014629672875655848\n",
      "Episode 2758 finished after 126 timesteps, total rewards 4.0, mean loss 0.015782119447572365\n",
      "Episode 2759 finished after 143 timesteps, total rewards 2.0, mean loss 0.017138185318220745\n",
      "Episode 2760 finished after 250 timesteps, total rewards 4.0, mean loss 0.015220449042273685\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 2761 finished after 119 timesteps, total rewards 3.0, mean loss 0.017251315811832882\n",
      "Episode 2762 finished after 117 timesteps, total rewards 2.0, mean loss 0.015460304241790635\n",
      "Episode 2763 finished after 157 timesteps, total rewards 3.0, mean loss 0.016474755565416378\n",
      "Episode 2764 finished after 112 timesteps, total rewards 0.0, mean loss 0.016368856033555597\n",
      "Episode 2765 finished after 277 timesteps, total rewards 6.0, mean loss 0.015612323666310832\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 2766 finished after 98 timesteps, total rewards 2.0, mean loss 0.01483552411735552\n",
      "Episode 2767 finished after 214 timesteps, total rewards 1.0, mean loss 0.016629532037224518\n",
      "Episode 2768 finished after 232 timesteps, total rewards 2.0, mean loss 0.014198910855941056\n",
      "Episode 2769 finished after 313 timesteps, total rewards 3.0, mean loss 0.01650951622249618\n",
      "Episode 2770 finished after 226 timesteps, total rewards 4.0, mean loss 0.015615422117412585\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 2771 finished after 173 timesteps, total rewards 3.0, mean loss 0.017439243118911228\n",
      "Episode 2772 finished after 179 timesteps, total rewards 4.0, mean loss 0.016542542256563174\n",
      "Episode 2773 finished after 131 timesteps, total rewards 2.0, mean loss 0.015756912093133246\n",
      "Episode 2774 finished after 179 timesteps, total rewards 7.0, mean loss 0.015378863088638318\n",
      "Episode 2775 finished after 196 timesteps, total rewards 7.0, mean loss 0.01671296517763819\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 2776 finished after 205 timesteps, total rewards 3.0, mean loss 0.014924347264090235\n",
      "Episode 2777 finished after 100 timesteps, total rewards 0.0, mean loss 0.012230240529752336\n",
      "Episode 2778 finished after 212 timesteps, total rewards 2.0, mean loss 0.01773043209027921\n",
      "Episode 2779 finished after 104 timesteps, total rewards 3.0, mean loss 0.016931160556291267\n",
      "Episode 2780 finished after 194 timesteps, total rewards 8.0, mean loss 0.01706773211811093\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 2781 finished after 199 timesteps, total rewards 8.0, mean loss 0.016720474206833123\n",
      "Episode 2782 finished after 129 timesteps, total rewards 2.0, mean loss 0.018812273658790388\n",
      "Episode 2783 finished after 90 timesteps, total rewards 1.0, mean loss 0.01690323382176252\n",
      "Episode 2784 finished after 168 timesteps, total rewards 1.0, mean loss 0.016978628151783987\n",
      "Episode 2785 finished after 258 timesteps, total rewards 5.0, mean loss 0.01684840364641575\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 2786 finished after 94 timesteps, total rewards 0.0, mean loss 0.015234335375815986\n",
      "Episode 2787 finished after 237 timesteps, total rewards 6.0, mean loss 0.015989406872835434\n",
      "Episode 2788 finished after 158 timesteps, total rewards 4.0, mean loss 0.0172428599701711\n",
      "Episode 2789 finished after 166 timesteps, total rewards 5.0, mean loss 0.017780061075898697\n",
      "Episode 2790 finished after 105 timesteps, total rewards 2.0, mean loss 0.01724376635337692\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 2791 finished after 288 timesteps, total rewards 8.0, mean loss 0.018973468088612815\n",
      "Episode 2792 finished after 86 timesteps, total rewards 1.0, mean loss 0.019758689064981936\n",
      "Episode 2793 finished after 271 timesteps, total rewards 2.0, mean loss 0.015092222420825207\n",
      "Episode 2794 finished after 173 timesteps, total rewards 5.0, mean loss 0.014835400509209281\n",
      "Episode 2795 finished after 179 timesteps, total rewards 1.0, mean loss 0.02018800098871729\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 2796 finished after 287 timesteps, total rewards 5.0, mean loss 0.018513577636120392\n",
      "Episode 2797 finished after 266 timesteps, total rewards 7.0, mean loss 0.016764629102019677\n",
      "Episode 2798 finished after 169 timesteps, total rewards 2.0, mean loss 0.016539538371481397\n",
      "Episode 2799 finished after 256 timesteps, total rewards 4.0, mean loss 0.018611127707799824\n",
      "Episode 2800 finished after 259 timesteps, total rewards 5.0, mean loss 0.017316658298833776\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 2801 finished after 118 timesteps, total rewards 1.0, mean loss 0.017600488500724848\n",
      "Episode 2802 finished after 163 timesteps, total rewards 2.0, mean loss 0.016715385157977762\n",
      "Episode 2803 finished after 131 timesteps, total rewards 1.0, mean loss 0.015316393297922776\n",
      "Episode 2804 finished after 257 timesteps, total rewards 1.0, mean loss 0.017350747219670986\n",
      "Episode 2805 finished after 129 timesteps, total rewards 0.0, mean loss 0.02196380566596234\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 2806 finished after 262 timesteps, total rewards 1.0, mean loss 0.01773324949650862\n",
      "Episode 2807 finished after 235 timesteps, total rewards 3.0, mean loss 0.01574478843769534\n",
      "Episode 2808 finished after 104 timesteps, total rewards 1.0, mean loss 0.01675439871570024\n",
      "Episode 2809 finished after 132 timesteps, total rewards 2.0, mean loss 0.01567273666230242\n",
      "Episode 2810 finished after 277 timesteps, total rewards 4.0, mean loss 0.016925882085794684\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2811 finished after 164 timesteps, total rewards 5.0, mean loss 0.016567473977707643\n",
      "Episode 2812 finished after 212 timesteps, total rewards 3.0, mean loss 0.015736518039220927\n",
      "Episode 2813 finished after 249 timesteps, total rewards 9.0, mean loss 0.01710313187475432\n",
      "Episode 2814 finished after 223 timesteps, total rewards 4.0, mean loss 0.016719787871786664\n",
      "Episode 2815 finished after 303 timesteps, total rewards 6.0, mean loss 0.0160713029111714\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 2816 finished after 239 timesteps, total rewards 5.0, mean loss 0.015629220638223477\n",
      "Episode 2817 finished after 306 timesteps, total rewards 7.0, mean loss 0.017533080144846214\n",
      "Episode 2818 finished after 331 timesteps, total rewards 9.0, mean loss 0.01712182536650888\n",
      "Episode 2819 finished after 248 timesteps, total rewards 11.0, mean loss 0.015610194572570523\n",
      "Episode 2820 finished after 116 timesteps, total rewards 4.0, mean loss 0.01876033953057827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 2821 finished after 237 timesteps, total rewards 6.0, mean loss 0.01707221384952041\n",
      "Episode 2822 finished after 106 timesteps, total rewards 3.0, mean loss 0.0193202336898671\n",
      "Episode 2823 finished after 98 timesteps, total rewards 0.0, mean loss 0.015796713792357822\n",
      "Episode 2824 finished after 189 timesteps, total rewards 1.0, mean loss 0.014516883077612393\n",
      "Episode 2825 finished after 123 timesteps, total rewards 1.0, mean loss 0.012218970054859007\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 2826 finished after 264 timesteps, total rewards 6.0, mean loss 0.017310570879172617\n",
      "Episode 2827 finished after 155 timesteps, total rewards 3.0, mean loss 0.018810495898698366\n",
      "Episode 2828 finished after 271 timesteps, total rewards 5.0, mean loss 0.018132152905725493\n",
      "Episode 2829 finished after 319 timesteps, total rewards 7.0, mean loss 0.016528938988918513\n",
      "Episode 2830 finished after 131 timesteps, total rewards 2.0, mean loss 0.014456764216808502\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 141.666667\n",
      "---------------------------------------\n",
      "Episode 2831 finished after 85 timesteps, total rewards 1.0, mean loss 0.02021011272308362\n",
      "Episode 2832 finished after 179 timesteps, total rewards 1.0, mean loss 0.015512617286060567\n",
      "Episode 2833 finished after 268 timesteps, total rewards 6.0, mean loss 0.017465425474107014\n",
      "Episode 2834 finished after 231 timesteps, total rewards 5.0, mean loss 0.01738954950890048\n",
      "Episode 2835 finished after 241 timesteps, total rewards 4.0, mean loss 0.01572491614634936\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 2836 finished after 152 timesteps, total rewards 2.0, mean loss 0.014176158843359812\n",
      "Episode 2837 finished after 288 timesteps, total rewards 4.0, mean loss 0.01757439186379391\n",
      "Episode 2838 finished after 157 timesteps, total rewards 5.0, mean loss 0.019083002199579008\n",
      "Episode 2839 finished after 155 timesteps, total rewards 5.0, mean loss 0.015313312184861712\n",
      "Episode 2840 finished after 199 timesteps, total rewards 3.0, mean loss 0.01590732312772947\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 2841 finished after 109 timesteps, total rewards 0.0, mean loss 0.013040435601905877\n",
      "Episode 2842 finished after 125 timesteps, total rewards 1.0, mean loss 0.019383228627964853\n",
      "Episode 2843 finished after 250 timesteps, total rewards 3.0, mean loss 0.016128908031620086\n",
      "Episode 2844 finished after 126 timesteps, total rewards 0.0, mean loss 0.012840123309399046\n",
      "Episode 2845 finished after 272 timesteps, total rewards 6.0, mean loss 0.01536576033472289\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 2846 finished after 167 timesteps, total rewards 3.0, mean loss 0.016234921375122256\n",
      "Episode 2847 finished after 189 timesteps, total rewards 3.0, mean loss 0.017630172961159908\n",
      "Episode 2848 finished after 139 timesteps, total rewards 2.0, mean loss 0.016890264808438155\n",
      "Episode 2849 finished after 140 timesteps, total rewards 0.0, mean loss 0.01903169814114725\n",
      "Episode 2850 finished after 237 timesteps, total rewards 4.0, mean loss 0.018021849151071995\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 2851 finished after 339 timesteps, total rewards 6.0, mean loss 0.016866017638322427\n",
      "Episode 2852 finished after 123 timesteps, total rewards 0.0, mean loss 0.012368759907482995\n",
      "Episode 2853 finished after 174 timesteps, total rewards 6.0, mean loss 0.01680540464799744\n",
      "Episode 2854 finished after 137 timesteps, total rewards 0.0, mean loss 0.020227465407249866\n",
      "Episode 2855 finished after 125 timesteps, total rewards 5.0, mean loss 0.018069819064810873\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 2856 finished after 113 timesteps, total rewards 1.0, mean loss 0.015194529786487563\n",
      "Episode 2857 finished after 134 timesteps, total rewards 1.0, mean loss 0.018232561553630337\n",
      "Episode 2858 finished after 220 timesteps, total rewards 5.0, mean loss 0.017595315307103607\n",
      "Episode 2859 finished after 127 timesteps, total rewards 2.0, mean loss 0.019261919891491064\n",
      "Episode 2860 finished after 275 timesteps, total rewards 3.0, mean loss 0.014887743025848811\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 2861 finished after 157 timesteps, total rewards 5.0, mean loss 0.01764127758263379\n",
      "Episode 2862 finished after 171 timesteps, total rewards 3.0, mean loss 0.01741577298564248\n",
      "Episode 2863 finished after 158 timesteps, total rewards 3.0, mean loss 0.014981309094177442\n",
      "Episode 2864 finished after 251 timesteps, total rewards 4.0, mean loss 0.018034005325669757\n",
      "Episode 2865 finished after 260 timesteps, total rewards 5.0, mean loss 0.015614674927648873\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 86.666667\n",
      "---------------------------------------\n",
      "Episode 2866 finished after 157 timesteps, total rewards 4.0, mean loss 0.019184347114579125\n",
      "Episode 2867 finished after 285 timesteps, total rewards 9.0, mean loss 0.018133301793989774\n",
      "Episode 2868 finished after 93 timesteps, total rewards 2.0, mean loss 0.015157580792513345\n",
      "Episode 2869 finished after 189 timesteps, total rewards 2.0, mean loss 0.019134266292750245\n",
      "Episode 2870 finished after 189 timesteps, total rewards 2.0, mean loss 0.019546345587716334\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 2871 finished after 173 timesteps, total rewards 3.0, mean loss 0.013492022475801042\n",
      "Episode 2872 finished after 75 timesteps, total rewards 3.0, mean loss 0.01827436520718038\n",
      "Episode 2873 finished after 215 timesteps, total rewards 7.0, mean loss 0.016808809438547075\n",
      "Episode 2874 finished after 240 timesteps, total rewards 3.0, mean loss 0.016440638509811832\n",
      "Episode 2875 finished after 95 timesteps, total rewards 2.0, mean loss 0.017814969126208637\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 2876 finished after 227 timesteps, total rewards 3.0, mean loss 0.01851727435004278\n",
      "Episode 2877 finished after 270 timesteps, total rewards 4.0, mean loss 0.017323614999488066\n",
      "Episode 2878 finished after 293 timesteps, total rewards 6.0, mean loss 0.01746816684948058\n",
      "Episode 2879 finished after 232 timesteps, total rewards 4.0, mean loss 0.018062581333526458\n",
      "Episode 2880 finished after 278 timesteps, total rewards 6.0, mean loss 0.017527962528736993\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2881 finished after 251 timesteps, total rewards 5.0, mean loss 0.01679369326670695\n",
      "Episode 2882 finished after 178 timesteps, total rewards 3.0, mean loss 0.01422086643843828\n",
      "Episode 2883 finished after 287 timesteps, total rewards 7.0, mean loss 0.01608971808671964\n",
      "Episode 2884 finished after 300 timesteps, total rewards 2.0, mean loss 0.016320117367625547\n",
      "Episode 2885 finished after 127 timesteps, total rewards 0.0, mean loss 0.014588999982710606\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 2886 finished after 142 timesteps, total rewards 1.0, mean loss 0.015289233970000001\n",
      "Episode 2887 finished after 214 timesteps, total rewards 3.0, mean loss 0.01622965201331851\n",
      "Episode 2888 finished after 165 timesteps, total rewards 2.0, mean loss 0.017304557990141665\n",
      "Episode 2889 finished after 233 timesteps, total rewards 7.0, mean loss 0.017203325238588463\n",
      "Episode 2890 finished after 228 timesteps, total rewards 5.0, mean loss 0.019863147710055814\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 2891 finished after 93 timesteps, total rewards 3.0, mean loss 0.017412518954745704\n",
      "Episode 2892 finished after 229 timesteps, total rewards 4.0, mean loss 0.018188233606519347\n",
      "Episode 2893 finished after 94 timesteps, total rewards 1.0, mean loss 0.012972668833792843\n",
      "Episode 2894 finished after 127 timesteps, total rewards 3.0, mean loss 0.017660101679498403\n",
      "Episode 2895 finished after 87 timesteps, total rewards 1.0, mean loss 0.01850151099617882\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 2896 finished after 279 timesteps, total rewards 4.0, mean loss 0.01635247240791024\n",
      "Episode 2897 finished after 173 timesteps, total rewards 4.0, mean loss 0.016703401100041204\n",
      "Episode 2898 finished after 110 timesteps, total rewards 1.0, mean loss 0.016679086816094986\n",
      "Episode 2899 finished after 137 timesteps, total rewards 0.0, mean loss 0.015379912463818968\n",
      "Episode 2900 finished after 150 timesteps, total rewards 4.0, mean loss 0.01656536210561171\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 2901 finished after 301 timesteps, total rewards 7.0, mean loss 0.017015703487947918\n",
      "Episode 2902 finished after 159 timesteps, total rewards 4.0, mean loss 0.015588707151111075\n",
      "Episode 2903 finished after 122 timesteps, total rewards 1.0, mean loss 0.015942451811288713\n",
      "Episode 2904 finished after 161 timesteps, total rewards 2.0, mean loss 0.018766339840468667\n",
      "Episode 2905 finished after 268 timesteps, total rewards 5.0, mean loss 0.017437215804760067\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 2906 finished after 173 timesteps, total rewards 2.0, mean loss 0.01727880171979266\n",
      "Episode 2907 finished after 235 timesteps, total rewards 2.0, mean loss 0.01695762253886564\n",
      "Episode 2908 finished after 78 timesteps, total rewards 0.0, mean loss 0.01561368726647626\n",
      "Episode 2909 finished after 162 timesteps, total rewards 4.0, mean loss 0.01737766921193983\n",
      "Episode 2910 finished after 277 timesteps, total rewards 10.0, mean loss 0.01803688309911894\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 2911 finished after 128 timesteps, total rewards 0.0, mean loss 0.014788073359795817\n",
      "Episode 2912 finished after 208 timesteps, total rewards 4.0, mean loss 0.014184333160441244\n",
      "Episode 2913 finished after 136 timesteps, total rewards 2.0, mean loss 0.017648941049735772\n",
      "Episode 2914 finished after 134 timesteps, total rewards 5.0, mean loss 0.015721319179022825\n",
      "Episode 2915 finished after 235 timesteps, total rewards 2.0, mean loss 0.014835119187247324\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 2916 finished after 154 timesteps, total rewards 1.0, mean loss 0.014568302764514969\n",
      "Episode 2917 finished after 206 timesteps, total rewards 4.0, mean loss 0.01723291354312661\n",
      "Episode 2918 finished after 128 timesteps, total rewards 3.0, mean loss 0.01544609694610699\n",
      "Episode 2919 finished after 216 timesteps, total rewards 3.0, mean loss 0.01656548423434961\n",
      "Episode 2920 finished after 184 timesteps, total rewards 4.0, mean loss 0.015705216855364953\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 2921 finished after 307 timesteps, total rewards 4.0, mean loss 0.01652134204031566\n",
      "Episode 2922 finished after 172 timesteps, total rewards 4.0, mean loss 0.016834538493597836\n",
      "Episode 2923 finished after 118 timesteps, total rewards 1.0, mean loss 0.01795492405900574\n",
      "Episode 2924 finished after 85 timesteps, total rewards 1.0, mean loss 0.018548687371452724\n",
      "Episode 2925 finished after 132 timesteps, total rewards 3.0, mean loss 0.01660080393423524\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 2926 finished after 204 timesteps, total rewards 2.0, mean loss 0.016598979853762443\n",
      "Episode 2927 finished after 167 timesteps, total rewards 2.0, mean loss 0.018595311248362688\n",
      "Episode 2928 finished after 236 timesteps, total rewards 9.0, mean loss 0.017158025045488502\n",
      "Episode 2929 finished after 132 timesteps, total rewards 0.0, mean loss 0.016628914015723927\n",
      "Episode 2930 finished after 255 timesteps, total rewards 8.0, mean loss 0.01705961709695996\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 2931 finished after 132 timesteps, total rewards 0.0, mean loss 0.015459877131489162\n",
      "Episode 2932 finished after 246 timesteps, total rewards 4.0, mean loss 0.016084314372704734\n",
      "Episode 2933 finished after 268 timesteps, total rewards 8.0, mean loss 0.016599633160382578\n",
      "Episode 2934 finished after 168 timesteps, total rewards 5.0, mean loss 0.013733552353403952\n",
      "Episode 2935 finished after 91 timesteps, total rewards 0.0, mean loss 0.013620882865635085\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 2936 finished after 244 timesteps, total rewards 3.0, mean loss 0.015228005828666043\n",
      "Episode 2937 finished after 188 timesteps, total rewards 6.0, mean loss 0.018132772939118677\n",
      "Episode 2938 finished after 108 timesteps, total rewards 2.0, mean loss 0.014530574729347049\n",
      "Episode 2939 finished after 174 timesteps, total rewards 2.0, mean loss 0.01608460102412293\n",
      "Episode 2940 finished after 182 timesteps, total rewards 6.0, mean loss 0.01760958834963206\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 2941 finished after 165 timesteps, total rewards 1.0, mean loss 0.01627623004641271\n",
      "Episode 2942 finished after 258 timesteps, total rewards 8.0, mean loss 0.01653190115430616\n",
      "Episode 2943 finished after 118 timesteps, total rewards 3.0, mean loss 0.01555932793022386\n",
      "Episode 2944 finished after 118 timesteps, total rewards 1.0, mean loss 0.016398741598280493\n",
      "Episode 2945 finished after 205 timesteps, total rewards 4.0, mean loss 0.016675244929359817\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 2946 finished after 135 timesteps, total rewards 5.0, mean loss 0.015834599760516235\n",
      "Episode 2947 finished after 273 timesteps, total rewards 4.0, mean loss 0.01716590365083656\n",
      "Episode 2948 finished after 111 timesteps, total rewards 1.0, mean loss 0.016891306807886943\n",
      "Episode 2949 finished after 220 timesteps, total rewards 1.0, mean loss 0.016542882650603795\n",
      "Episode 2950 finished after 121 timesteps, total rewards 2.0, mean loss 0.015414737213079718\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2951 finished after 138 timesteps, total rewards 2.0, mean loss 0.01612600568130248\n",
      "Episode 2952 finished after 164 timesteps, total rewards 2.0, mean loss 0.015683353071458773\n",
      "Episode 2953 finished after 160 timesteps, total rewards 2.0, mean loss 0.016073180701278035\n",
      "Episode 2954 finished after 168 timesteps, total rewards 0.0, mean loss 0.01725937073933892\n",
      "Episode 2955 finished after 174 timesteps, total rewards 5.0, mean loss 0.014655020935982787\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 2956 finished after 174 timesteps, total rewards 1.0, mean loss 0.016427004977795777\n",
      "Episode 2957 finished after 84 timesteps, total rewards 0.0, mean loss 0.01516030779956574\n",
      "Episode 2958 finished after 146 timesteps, total rewards 1.0, mean loss 0.017059417393645398\n",
      "Episode 2959 finished after 219 timesteps, total rewards 3.0, mean loss 0.01588277346358877\n",
      "Episode 2960 finished after 130 timesteps, total rewards 2.0, mean loss 0.016946553508751093\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 2961 finished after 181 timesteps, total rewards 3.0, mean loss 0.016569082250237177\n",
      "Episode 2962 finished after 157 timesteps, total rewards 2.0, mean loss 0.015380853162351759\n",
      "Episode 2963 finished after 210 timesteps, total rewards 3.0, mean loss 0.0147727585112166\n",
      "Episode 2964 finished after 126 timesteps, total rewards 2.0, mean loss 0.017019703864611477\n",
      "Episode 2965 finished after 167 timesteps, total rewards 1.0, mean loss 0.01620511441110929\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 2966 finished after 142 timesteps, total rewards 3.0, mean loss 0.01779810770031091\n",
      "Episode 2967 finished after 124 timesteps, total rewards 0.0, mean loss 0.016274827252228504\n",
      "Episode 2968 finished after 153 timesteps, total rewards 1.0, mean loss 0.01889327289064847\n",
      "Episode 2969 finished after 156 timesteps, total rewards 5.0, mean loss 0.016442129325393278\n",
      "Episode 2970 finished after 217 timesteps, total rewards 6.0, mean loss 0.019242275186249994\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 2971 finished after 92 timesteps, total rewards 0.0, mean loss 0.016579145587773226\n",
      "Episode 2972 finished after 265 timesteps, total rewards 5.0, mean loss 0.014801235053521353\n",
      "Episode 2973 finished after 96 timesteps, total rewards 1.0, mean loss 0.01764451346389251\n",
      "Episode 2974 finished after 171 timesteps, total rewards 4.0, mean loss 0.01645119107166327\n",
      "Episode 2975 finished after 192 timesteps, total rewards 4.0, mean loss 0.01651319388626386\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 2976 finished after 140 timesteps, total rewards 4.0, mean loss 0.015385474693279581\n",
      "Episode 2977 finished after 157 timesteps, total rewards 6.0, mean loss 0.01640334554093706\n",
      "Episode 2978 finished after 122 timesteps, total rewards 1.0, mean loss 0.01883632029677726\n",
      "Episode 2979 finished after 176 timesteps, total rewards 1.0, mean loss 0.015866661277951524\n",
      "Episode 2980 finished after 269 timesteps, total rewards 6.0, mean loss 0.017070368485833383\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 2981 finished after 168 timesteps, total rewards 3.0, mean loss 0.016783659488802578\n",
      "Episode 2982 finished after 131 timesteps, total rewards 1.0, mean loss 0.016557035883284308\n",
      "Episode 2983 finished after 303 timesteps, total rewards 9.0, mean loss 0.01745097598955404\n",
      "Episode 2984 finished after 113 timesteps, total rewards 3.0, mean loss 0.016805665994030054\n",
      "Episode 2985 finished after 347 timesteps, total rewards 5.0, mean loss 0.01713978525996611\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 2986 finished after 214 timesteps, total rewards 2.0, mean loss 0.014990111022249787\n",
      "Episode 2987 finished after 248 timesteps, total rewards 6.0, mean loss 0.017890964126103602\n",
      "Episode 2988 finished after 238 timesteps, total rewards 5.0, mean loss 0.015455727690944308\n",
      "Episode 2989 finished after 91 timesteps, total rewards 2.0, mean loss 0.01651634315954992\n",
      "Episode 2990 finished after 100 timesteps, total rewards 2.0, mean loss 0.021238981841597705\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 2991 finished after 183 timesteps, total rewards 2.0, mean loss 0.017595885789067654\n",
      "Episode 2992 finished after 188 timesteps, total rewards 1.0, mean loss 0.016600801569825793\n",
      "Episode 2993 finished after 125 timesteps, total rewards 1.0, mean loss 0.017789497965946793\n",
      "Episode 2994 finished after 122 timesteps, total rewards 0.0, mean loss 0.01517224509639238\n",
      "Episode 2995 finished after 201 timesteps, total rewards 6.0, mean loss 0.01781716991583145\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 2996 finished after 88 timesteps, total rewards 0.0, mean loss 0.01722621880815661\n",
      "Episode 2997 finished after 287 timesteps, total rewards 4.0, mean loss 0.015318542256876696\n",
      "Episode 2998 finished after 200 timesteps, total rewards 3.0, mean loss 0.015655418939422815\n",
      "Episode 2999 finished after 99 timesteps, total rewards 0.0, mean loss 0.019989705364680803\n",
      "Episode 3000 finished after 324 timesteps, total rewards 9.0, mean loss 0.017683591304151456\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 3001 finished after 123 timesteps, total rewards 2.0, mean loss 0.01392207160872233\n",
      "Episode 3002 finished after 262 timesteps, total rewards 3.0, mean loss 0.01540735777212442\n",
      "Episode 3003 finished after 312 timesteps, total rewards 4.0, mean loss 0.015952178674454514\n",
      "Episode 3004 finished after 257 timesteps, total rewards 3.0, mean loss 0.018129124819592786\n",
      "Episode 3005 finished after 104 timesteps, total rewards 2.0, mean loss 0.015849076204176527\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 3006 finished after 260 timesteps, total rewards 5.0, mean loss 0.017293929368203793\n",
      "Episode 3007 finished after 186 timesteps, total rewards 2.0, mean loss 0.016198640086522866\n",
      "Episode 3008 finished after 136 timesteps, total rewards 1.0, mean loss 0.01616990151524078\n",
      "Episode 3009 finished after 158 timesteps, total rewards 4.0, mean loss 0.01822810323947239\n",
      "Episode 3010 finished after 158 timesteps, total rewards 3.0, mean loss 0.01897056410722788\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 3011 finished after 168 timesteps, total rewards 1.0, mean loss 0.01641805958656949\n",
      "Episode 3012 finished after 122 timesteps, total rewards 1.0, mean loss 0.015613722826781699\n",
      "Episode 3013 finished after 236 timesteps, total rewards 4.0, mean loss 0.01784822440551302\n",
      "Episode 3014 finished after 228 timesteps, total rewards 8.0, mean loss 0.01762367756639019\n",
      "Episode 3015 finished after 215 timesteps, total rewards 6.0, mean loss 0.015659364035385554\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 3016 finished after 273 timesteps, total rewards 5.0, mean loss 0.016495653006185065\n",
      "Episode 3017 finished after 169 timesteps, total rewards 5.0, mean loss 0.015379726284351641\n",
      "Episode 3018 finished after 84 timesteps, total rewards 1.0, mean loss 0.018395660015466136\n",
      "Episode 3019 finished after 159 timesteps, total rewards 2.0, mean loss 0.01700452830695178\n",
      "Episode 3020 finished after 249 timesteps, total rewards 3.0, mean loss 0.01558459794123556\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3021 finished after 120 timesteps, total rewards 1.0, mean loss 0.017637170378293376\n",
      "Episode 3022 finished after 199 timesteps, total rewards 2.0, mean loss 0.01898609752352607\n",
      "Episode 3023 finished after 260 timesteps, total rewards 7.0, mean loss 0.017559172474116518\n",
      "Episode 3024 finished after 158 timesteps, total rewards 3.0, mean loss 0.017669866865801423\n",
      "Episode 3025 finished after 156 timesteps, total rewards 4.0, mean loss 0.016489791204385724\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3026 finished after 84 timesteps, total rewards 0.0, mean loss 0.018488974905700332\n",
      "Episode 3027 finished after 181 timesteps, total rewards 3.0, mean loss 0.01504975748286989\n",
      "Episode 3028 finished after 113 timesteps, total rewards 1.0, mean loss 0.01771299638970862\n",
      "Episode 3029 finished after 77 timesteps, total rewards 1.0, mean loss 0.013996069922789255\n",
      "Episode 3030 finished after 208 timesteps, total rewards 1.0, mean loss 0.01586577529479445\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 3031 finished after 143 timesteps, total rewards 1.0, mean loss 0.014270326944875655\n",
      "Episode 3032 finished after 124 timesteps, total rewards 4.0, mean loss 0.01656937087330246\n",
      "Episode 3033 finished after 101 timesteps, total rewards 1.0, mean loss 0.014049299869289862\n",
      "Episode 3034 finished after 180 timesteps, total rewards 6.0, mean loss 0.014892060174154014\n",
      "Episode 3035 finished after 196 timesteps, total rewards 1.0, mean loss 0.01742082751624533\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 3036 finished after 137 timesteps, total rewards 2.0, mean loss 0.017483151258210768\n",
      "Episode 3037 finished after 245 timesteps, total rewards 9.0, mean loss 0.01673840869393922\n",
      "Episode 3038 finished after 104 timesteps, total rewards 1.0, mean loss 0.016222579068898294\n",
      "Episode 3039 finished after 154 timesteps, total rewards 4.0, mean loss 0.01763729531281664\n",
      "Episode 3040 finished after 161 timesteps, total rewards 5.0, mean loss 0.015251935878696256\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 3041 finished after 159 timesteps, total rewards 2.0, mean loss 0.01638606150997744\n",
      "Episode 3042 finished after 128 timesteps, total rewards 0.0, mean loss 0.018472160493274714\n",
      "Episode 3043 finished after 218 timesteps, total rewards 3.0, mean loss 0.018596605145705675\n",
      "Episode 3044 finished after 192 timesteps, total rewards 8.0, mean loss 0.016780127432563557\n",
      "Episode 3045 finished after 168 timesteps, total rewards 4.0, mean loss 0.019937521252792238\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 3046 finished after 156 timesteps, total rewards 3.0, mean loss 0.016060590179851994\n",
      "Episode 3047 finished after 95 timesteps, total rewards 3.0, mean loss 0.017562242271378637\n",
      "Episode 3048 finished after 281 timesteps, total rewards 10.0, mean loss 0.017118273112961022\n",
      "Episode 3049 finished after 197 timesteps, total rewards 4.0, mean loss 0.015595049274373838\n",
      "Episode 3050 finished after 96 timesteps, total rewards 0.0, mean loss 0.015105702599006085\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 3051 finished after 97 timesteps, total rewards 3.0, mean loss 0.01839349076713522\n",
      "Episode 3052 finished after 130 timesteps, total rewards 2.0, mean loss 0.014663529232627927\n",
      "Episode 3053 finished after 231 timesteps, total rewards 8.0, mean loss 0.018818281837124216\n",
      "Episode 3054 finished after 211 timesteps, total rewards 4.0, mean loss 0.016837059454804824\n",
      "Episode 3055 finished after 246 timesteps, total rewards 5.0, mean loss 0.018040821355165597\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 11.666667\n",
      "---------------------------------------\n",
      "Episode 3056 finished after 128 timesteps, total rewards 2.0, mean loss 0.016092422308247478\n",
      "Episode 3057 finished after 274 timesteps, total rewards 6.0, mean loss 0.01981703237860708\n",
      "Episode 3058 finished after 220 timesteps, total rewards 4.0, mean loss 0.016547201542099092\n",
      "Episode 3059 finished after 103 timesteps, total rewards 2.0, mean loss 0.016121012721554313\n",
      "Episode 3060 finished after 150 timesteps, total rewards 2.0, mean loss 0.016604609256610274\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 3061 finished after 334 timesteps, total rewards 8.0, mean loss 0.018102275763574184\n",
      "Episode 3062 finished after 290 timesteps, total rewards 6.0, mean loss 0.01672897360664953\n",
      "Episode 3063 finished after 155 timesteps, total rewards 2.0, mean loss 0.014453848276752978\n",
      "Episode 3064 finished after 95 timesteps, total rewards 4.0, mean loss 0.01343687976845295\n",
      "Episode 3065 finished after 217 timesteps, total rewards 2.0, mean loss 0.015400287521679023\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 3066 finished after 239 timesteps, total rewards 3.0, mean loss 0.016508089539216963\n",
      "Episode 3067 finished after 273 timesteps, total rewards 7.0, mean loss 0.017684596356161403\n",
      "Episode 3068 finished after 129 timesteps, total rewards 2.0, mean loss 0.013669412767211364\n",
      "Episode 3069 finished after 173 timesteps, total rewards 4.0, mean loss 0.01640190092195364\n",
      "Episode 3070 finished after 127 timesteps, total rewards 3.0, mean loss 0.01868785386281748\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 86.666667\n",
      "---------------------------------------\n",
      "Episode 3071 finished after 160 timesteps, total rewards 2.0, mean loss 0.01762138268604758\n",
      "Episode 3072 finished after 167 timesteps, total rewards 5.0, mean loss 0.016665027903819535\n",
      "Episode 3073 finished after 311 timesteps, total rewards 7.0, mean loss 0.015925560538114985\n",
      "Episode 3074 finished after 123 timesteps, total rewards 2.0, mean loss 0.01637074288891295\n",
      "Episode 3075 finished after 139 timesteps, total rewards 1.0, mean loss 0.014250444167916723\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 85.000000\n",
      "---------------------------------------\n",
      "Episode 3076 finished after 202 timesteps, total rewards 7.0, mean loss 0.01687108576764574\n",
      "Episode 3077 finished after 258 timesteps, total rewards 2.0, mean loss 0.015552470810682506\n",
      "Episode 3078 finished after 269 timesteps, total rewards 3.0, mean loss 0.015896667216317623\n",
      "Episode 3079 finished after 195 timesteps, total rewards 2.0, mean loss 0.017454015985370063\n",
      "Episode 3080 finished after 174 timesteps, total rewards 2.0, mean loss 0.018204448448932976\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 3081 finished after 173 timesteps, total rewards 5.0, mean loss 0.01703502633205125\n",
      "Episode 3082 finished after 309 timesteps, total rewards 7.0, mean loss 0.01835192758480726\n",
      "Episode 3083 finished after 110 timesteps, total rewards 3.0, mean loss 0.016573099496732042\n",
      "Episode 3084 finished after 139 timesteps, total rewards 4.0, mean loss 0.016913473535757916\n",
      "Episode 3085 finished after 219 timesteps, total rewards 1.0, mean loss 0.01837648460125874\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 3086 finished after 221 timesteps, total rewards 4.0, mean loss 0.01842993094187601\n",
      "Episode 3087 finished after 252 timesteps, total rewards 7.0, mean loss 0.018220361025582645\n",
      "Episode 3088 finished after 125 timesteps, total rewards 3.0, mean loss 0.0141000982997939\n",
      "Episode 3089 finished after 98 timesteps, total rewards 2.0, mean loss 0.017582978987923766\n",
      "Episode 3090 finished after 318 timesteps, total rewards 11.0, mean loss 0.01668867815919982\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3091 finished after 165 timesteps, total rewards 1.0, mean loss 0.01672640111240925\n",
      "Episode 3092 finished after 230 timesteps, total rewards 5.0, mean loss 0.0192741919121624\n",
      "Episode 3093 finished after 103 timesteps, total rewards 1.0, mean loss 0.016017458518600834\n",
      "Episode 3094 finished after 213 timesteps, total rewards 5.0, mean loss 0.01858695502203841\n",
      "Episode 3095 finished after 256 timesteps, total rewards 5.0, mean loss 0.015964420432283077\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 3096 finished after 170 timesteps, total rewards 3.0, mean loss 0.019131566332790124\n",
      "Episode 3097 finished after 122 timesteps, total rewards 2.0, mean loss 0.01703797478503624\n",
      "Episode 3098 finished after 178 timesteps, total rewards 2.0, mean loss 0.018001366778815774\n",
      "Episode 3099 finished after 110 timesteps, total rewards 1.0, mean loss 0.015223874549635433\n",
      "Episode 3100 finished after 191 timesteps, total rewards 5.0, mean loss 0.018086815902022206\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 131.666667\n",
      "---------------------------------------\n",
      "Episode 3101 finished after 181 timesteps, total rewards 4.0, mean loss 0.01735201221665635\n",
      "Episode 3102 finished after 296 timesteps, total rewards 5.0, mean loss 0.015137827859025737\n",
      "Episode 3103 finished after 203 timesteps, total rewards 2.0, mean loss 0.017950617998157135\n",
      "Episode 3104 finished after 245 timesteps, total rewards 10.0, mean loss 0.018852571851089215\n",
      "Episode 3105 finished after 264 timesteps, total rewards 2.0, mean loss 0.018242951256979723\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 128.333333\n",
      "---------------------------------------\n",
      "Episode 3106 finished after 169 timesteps, total rewards 4.0, mean loss 0.016624448898788433\n",
      "Episode 3107 finished after 252 timesteps, total rewards 4.0, mean loss 0.01645262362153661\n",
      "Episode 3108 finished after 158 timesteps, total rewards 2.0, mean loss 0.01757807438880724\n",
      "Episode 3109 finished after 172 timesteps, total rewards 0.0, mean loss 0.015975229942976215\n",
      "Episode 3110 finished after 120 timesteps, total rewards 1.0, mean loss 0.018060858608805574\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 3111 finished after 166 timesteps, total rewards 4.0, mean loss 0.01860175112925795\n",
      "Episode 3112 finished after 120 timesteps, total rewards 1.0, mean loss 0.019676426387741232\n",
      "Episode 3113 finished after 156 timesteps, total rewards 1.0, mean loss 0.017598013494698066\n",
      "Episode 3114 finished after 282 timesteps, total rewards 7.0, mean loss 0.01775037237948322\n",
      "Episode 3115 finished after 172 timesteps, total rewards 1.0, mean loss 0.015439218344359096\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 3116 finished after 166 timesteps, total rewards 2.0, mean loss 0.018179161780224887\n",
      "Episode 3117 finished after 377 timesteps, total rewards 4.0, mean loss 0.016629963419375452\n",
      "Episode 3118 finished after 130 timesteps, total rewards 0.0, mean loss 0.017083711300690013\n",
      "Episode 3119 finished after 90 timesteps, total rewards 2.0, mean loss 0.01609079204370371\n",
      "Episode 3120 finished after 293 timesteps, total rewards 6.0, mean loss 0.018548354680718276\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 3121 finished after 131 timesteps, total rewards 1.0, mean loss 0.014079066179603878\n",
      "Episode 3122 finished after 168 timesteps, total rewards 2.0, mean loss 0.0158467244566834\n",
      "Episode 3123 finished after 139 timesteps, total rewards 0.0, mean loss 0.01809079477416097\n",
      "Episode 3124 finished after 228 timesteps, total rewards 6.0, mean loss 0.016235775785549265\n",
      "Episode 3125 finished after 137 timesteps, total rewards 4.0, mean loss 0.01633035609274287\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3126 finished after 101 timesteps, total rewards 3.0, mean loss 0.015369291154501757\n",
      "Episode 3127 finished after 232 timesteps, total rewards 5.0, mean loss 0.015274767955847422\n",
      "Episode 3128 finished after 171 timesteps, total rewards 4.0, mean loss 0.016160136569027206\n",
      "Episode 3129 finished after 172 timesteps, total rewards 4.0, mean loss 0.016563513477300403\n",
      "Episode 3130 finished after 151 timesteps, total rewards 4.0, mean loss 0.01618606029602619\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 81.666667\n",
      "---------------------------------------\n",
      "Episode 3131 finished after 236 timesteps, total rewards 5.0, mean loss 0.018137066930402228\n",
      "Episode 3132 finished after 249 timesteps, total rewards 5.0, mean loss 0.016007563555575758\n",
      "Episode 3133 finished after 134 timesteps, total rewards 0.0, mean loss 0.016938214582120026\n",
      "Episode 3134 finished after 163 timesteps, total rewards 1.0, mean loss 0.015613838115727037\n",
      "Episode 3135 finished after 104 timesteps, total rewards 3.0, mean loss 0.016903816976152414\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 3136 finished after 204 timesteps, total rewards 4.0, mean loss 0.015579417852374414\n",
      "Episode 3137 finished after 128 timesteps, total rewards 0.0, mean loss 0.01732952617931005\n",
      "Episode 3138 finished after 196 timesteps, total rewards 2.0, mean loss 0.017039882069351437\n",
      "Episode 3139 finished after 82 timesteps, total rewards 0.0, mean loss 0.021244974622343916\n",
      "Episode 3140 finished after 181 timesteps, total rewards 2.0, mean loss 0.015674022164965017\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 3141 finished after 224 timesteps, total rewards 4.0, mean loss 0.016877477823072695\n",
      "Episode 3142 finished after 137 timesteps, total rewards 2.0, mean loss 0.019757046822261363\n",
      "Episode 3143 finished after 130 timesteps, total rewards 2.0, mean loss 0.017011196591640607\n",
      "Episode 3144 finished after 297 timesteps, total rewards 11.0, mean loss 0.017414434944665193\n",
      "Episode 3145 finished after 252 timesteps, total rewards 5.0, mean loss 0.016365212615623716\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 3146 finished after 97 timesteps, total rewards 1.0, mean loss 0.01962350072944057\n",
      "Episode 3147 finished after 218 timesteps, total rewards 7.0, mean loss 0.018152848846778924\n",
      "Episode 3148 finished after 157 timesteps, total rewards 4.0, mean loss 0.01793611605479649\n",
      "Episode 3149 finished after 83 timesteps, total rewards 2.0, mean loss 0.016913046464267893\n",
      "Episode 3150 finished after 130 timesteps, total rewards 0.0, mean loss 0.01585386276486903\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 3151 finished after 163 timesteps, total rewards 2.0, mean loss 0.015030463500394046\n",
      "Episode 3152 finished after 123 timesteps, total rewards 1.0, mean loss 0.01776306049063484\n",
      "Episode 3153 finished after 208 timesteps, total rewards 8.0, mean loss 0.01655706390402674\n",
      "Episode 3154 finished after 222 timesteps, total rewards 4.0, mean loss 0.016286792721796513\n",
      "Episode 3155 finished after 122 timesteps, total rewards 2.0, mean loss 0.01796914758752329\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 3156 finished after 95 timesteps, total rewards 1.0, mean loss 0.01592150496711072\n",
      "Episode 3157 finished after 193 timesteps, total rewards 1.0, mean loss 0.01747057373138444\n",
      "Episode 3158 finished after 101 timesteps, total rewards 1.0, mean loss 0.0187723980110028\n",
      "Episode 3159 finished after 166 timesteps, total rewards 0.0, mean loss 0.016958903791298753\n",
      "Episode 3160 finished after 264 timesteps, total rewards 4.0, mean loss 0.01874707954244293\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3161 finished after 203 timesteps, total rewards 8.0, mean loss 0.01621743744890678\n",
      "Episode 3162 finished after 251 timesteps, total rewards 8.0, mean loss 0.015539024686821147\n",
      "Episode 3163 finished after 239 timesteps, total rewards 5.0, mean loss 0.01787267274478505\n",
      "Episode 3164 finished after 221 timesteps, total rewards 5.0, mean loss 0.018053785379716865\n",
      "Episode 3165 finished after 93 timesteps, total rewards 2.0, mean loss 0.01278028061728604\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 3166 finished after 203 timesteps, total rewards 5.0, mean loss 0.017101207398236017\n",
      "Episode 3167 finished after 139 timesteps, total rewards 3.0, mean loss 0.017424217894298138\n",
      "Episode 3168 finished after 155 timesteps, total rewards 5.0, mean loss 0.017675767559933688\n",
      "Episode 3169 finished after 172 timesteps, total rewards 1.0, mean loss 0.01685806538274804\n",
      "Episode 3170 finished after 347 timesteps, total rewards 6.0, mean loss 0.017767937839003038\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 3171 finished after 157 timesteps, total rewards 3.0, mean loss 0.017397207047814018\n",
      "Episode 3172 finished after 122 timesteps, total rewards 2.0, mean loss 0.017543958217577368\n",
      "Episode 3173 finished after 159 timesteps, total rewards 4.0, mean loss 0.0183956982540816\n",
      "Episode 3174 finished after 166 timesteps, total rewards 5.0, mean loss 0.016413092584116384\n",
      "Episode 3175 finished after 138 timesteps, total rewards 0.0, mean loss 0.015186786460538355\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 123.333333\n",
      "---------------------------------------\n",
      "Episode 3176 finished after 265 timesteps, total rewards 4.0, mean loss 0.01484228098822125\n",
      "Episode 3177 finished after 236 timesteps, total rewards 5.0, mean loss 0.017655158556023044\n",
      "Episode 3178 finished after 84 timesteps, total rewards 2.0, mean loss 0.019155526236448037\n",
      "Episode 3179 finished after 161 timesteps, total rewards 2.0, mean loss 0.017036635938436025\n",
      "Episode 3180 finished after 136 timesteps, total rewards 1.0, mean loss 0.015166572637099307\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 136.666667\n",
      "---------------------------------------\n",
      "Episode 3181 finished after 168 timesteps, total rewards 2.0, mean loss 0.017430565791506696\n",
      "Episode 3182 finished after 249 timesteps, total rewards 5.0, mean loss 0.01631371375808238\n",
      "Episode 3183 finished after 133 timesteps, total rewards 2.0, mean loss 0.01424005226043046\n",
      "Episode 3184 finished after 92 timesteps, total rewards 2.0, mean loss 0.015919247272667355\n",
      "Episode 3185 finished after 123 timesteps, total rewards 2.0, mean loss 0.01865073453589547\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 3186 finished after 175 timesteps, total rewards 4.0, mean loss 0.014501655461000545\n",
      "Episode 3187 finished after 174 timesteps, total rewards 0.0, mean loss 0.015304938301272776\n",
      "Episode 3188 finished after 161 timesteps, total rewards 3.0, mean loss 0.020259089360840803\n",
      "Episode 3189 finished after 291 timesteps, total rewards 5.0, mean loss 0.01815251693412777\n",
      "Episode 3190 finished after 197 timesteps, total rewards 3.0, mean loss 0.016248527603746312\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 3191 finished after 159 timesteps, total rewards 2.0, mean loss 0.014961334998256661\n",
      "Episode 3192 finished after 128 timesteps, total rewards 1.0, mean loss 0.016327718339653075\n",
      "Episode 3193 finished after 137 timesteps, total rewards 3.0, mean loss 0.017433664714747592\n",
      "Episode 3194 finished after 125 timesteps, total rewards 2.0, mean loss 0.014146880431100727\n",
      "Episode 3195 finished after 266 timesteps, total rewards 4.0, mean loss 0.016905399921474475\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 3196 finished after 268 timesteps, total rewards 9.0, mean loss 0.015126500133843744\n",
      "Episode 3197 finished after 155 timesteps, total rewards 4.0, mean loss 0.014911024125232812\n",
      "Episode 3198 finished after 127 timesteps, total rewards 2.0, mean loss 0.01715048228979008\n",
      "Episode 3199 finished after 284 timesteps, total rewards 4.0, mean loss 0.018140001490369477\n",
      "Episode 3200 finished after 206 timesteps, total rewards 5.0, mean loss 0.017477737917413357\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 3201 finished after 224 timesteps, total rewards 6.0, mean loss 0.01721976413318771\n",
      "Episode 3202 finished after 277 timesteps, total rewards 4.0, mean loss 0.015188620651026495\n",
      "Episode 3203 finished after 170 timesteps, total rewards 4.0, mean loss 0.015251131096567191\n",
      "Episode 3204 finished after 142 timesteps, total rewards 0.0, mean loss 0.015554667969437604\n",
      "Episode 3205 finished after 320 timesteps, total rewards 8.0, mean loss 0.016433055828747455\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 3206 finished after 159 timesteps, total rewards 3.0, mean loss 0.018317973087819985\n",
      "Episode 3207 finished after 126 timesteps, total rewards 3.0, mean loss 0.01512557929169224\n",
      "Episode 3208 finished after 121 timesteps, total rewards 1.0, mean loss 0.016316079282998455\n",
      "Episode 3209 finished after 296 timesteps, total rewards 12.0, mean loss 0.017503845779936307\n",
      "Episode 3210 finished after 311 timesteps, total rewards 10.0, mean loss 0.015961798160244078\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 3211 finished after 230 timesteps, total rewards 5.0, mean loss 0.01485097330144566\n",
      "Episode 3212 finished after 134 timesteps, total rewards 8.0, mean loss 0.01929594053836442\n",
      "Episode 3213 finished after 128 timesteps, total rewards 2.0, mean loss 0.016341636782271962\n",
      "Episode 3214 finished after 125 timesteps, total rewards 1.0, mean loss 0.017240727258846164\n",
      "Episode 3215 finished after 121 timesteps, total rewards 1.0, mean loss 0.013622073077573633\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 88.333333\n",
      "---------------------------------------\n",
      "Episode 3216 finished after 219 timesteps, total rewards 7.0, mean loss 0.0188181604770093\n",
      "Episode 3217 finished after 262 timesteps, total rewards 5.0, mean loss 0.016564092754594313\n",
      "Episode 3218 finished after 200 timesteps, total rewards 4.0, mean loss 0.015900163836195132\n",
      "Episode 3219 finished after 210 timesteps, total rewards 7.0, mean loss 0.01677071241290486\n",
      "Episode 3220 finished after 124 timesteps, total rewards 3.0, mean loss 0.015286995761749906\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 3221 finished after 199 timesteps, total rewards 6.0, mean loss 0.015572244641909273\n",
      "Episode 3222 finished after 129 timesteps, total rewards 2.0, mean loss 0.019172573179663557\n",
      "Episode 3223 finished after 236 timesteps, total rewards 8.0, mean loss 0.016595979497898213\n",
      "Episode 3224 finished after 120 timesteps, total rewards 2.0, mean loss 0.017118881611774366\n",
      "Episode 3225 finished after 148 timesteps, total rewards 3.0, mean loss 0.01699402354220306\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 3226 finished after 204 timesteps, total rewards 4.0, mean loss 0.017072836573431997\n",
      "Episode 3227 finished after 135 timesteps, total rewards 3.0, mean loss 0.015839829976687692\n",
      "Episode 3228 finished after 191 timesteps, total rewards 3.0, mean loss 0.016895057417145146\n",
      "Episode 3229 finished after 185 timesteps, total rewards 1.0, mean loss 0.016889334024823698\n",
      "Episode 3230 finished after 172 timesteps, total rewards 3.0, mean loss 0.018534447962613127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 3231 finished after 157 timesteps, total rewards 2.0, mean loss 0.019738188560660574\n",
      "Episode 3232 finished after 178 timesteps, total rewards 6.0, mean loss 0.01587450632130926\n",
      "Episode 3233 finished after 134 timesteps, total rewards 0.0, mean loss 0.0172677994935961\n",
      "Episode 3234 finished after 252 timesteps, total rewards 4.0, mean loss 0.018659510546722376\n",
      "Episode 3235 finished after 129 timesteps, total rewards 2.0, mean loss 0.014998070238692355\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 3236 finished after 249 timesteps, total rewards 1.0, mean loss 0.017387399828644942\n",
      "Episode 3237 finished after 156 timesteps, total rewards 6.0, mean loss 0.01653661471545302\n",
      "Episode 3238 finished after 171 timesteps, total rewards 4.0, mean loss 0.01613713078791744\n",
      "Episode 3239 finished after 124 timesteps, total rewards 2.0, mean loss 0.016770349280352915\n",
      "Episode 3240 finished after 172 timesteps, total rewards 7.0, mean loss 0.016949048094575966\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 3241 finished after 339 timesteps, total rewards 9.0, mean loss 0.018006081289369997\n",
      "Episode 3242 finished after 154 timesteps, total rewards 2.0, mean loss 0.015214082568372879\n",
      "Episode 3243 finished after 192 timesteps, total rewards 10.0, mean loss 0.01755389956876267\n",
      "Episode 3244 finished after 164 timesteps, total rewards 3.0, mean loss 0.017067193799004777\n",
      "Episode 3245 finished after 134 timesteps, total rewards 1.0, mean loss 0.017021391538159448\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3246 finished after 158 timesteps, total rewards 5.0, mean loss 0.015091382665191246\n",
      "Episode 3247 finished after 150 timesteps, total rewards 5.0, mean loss 0.015298986904478321\n",
      "Episode 3248 finished after 161 timesteps, total rewards 4.0, mean loss 0.01538272149085721\n",
      "Episode 3249 finished after 172 timesteps, total rewards 3.0, mean loss 0.016337253141817963\n",
      "Episode 3250 finished after 230 timesteps, total rewards 6.0, mean loss 0.017361934016883858\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 110.000000\n",
      "---------------------------------------\n",
      "Episode 3251 finished after 330 timesteps, total rewards 5.0, mean loss 0.01663692517939842\n",
      "Episode 3252 finished after 133 timesteps, total rewards 5.0, mean loss 0.014527225417445618\n",
      "Episode 3253 finished after 176 timesteps, total rewards 7.0, mean loss 0.014755559811998286\n",
      "Episode 3254 finished after 179 timesteps, total rewards 3.0, mean loss 0.017313350596225838\n",
      "Episode 3255 finished after 150 timesteps, total rewards 3.0, mean loss 0.017014248036624244\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 3256 finished after 97 timesteps, total rewards 3.0, mean loss 0.01518992750098948\n",
      "Episode 3257 finished after 280 timesteps, total rewards 6.0, mean loss 0.01606271078635473\n",
      "Episode 3258 finished after 160 timesteps, total rewards 4.0, mean loss 0.017126814481162\n",
      "Episode 3259 finished after 212 timesteps, total rewards 3.0, mean loss 0.01878274097447811\n",
      "Episode 3260 finished after 150 timesteps, total rewards 5.0, mean loss 0.01798761196822549\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 3261 finished after 181 timesteps, total rewards 2.0, mean loss 0.01821711033940295\n",
      "Episode 3262 finished after 126 timesteps, total rewards 1.0, mean loss 0.01797260168481559\n",
      "Episode 3263 finished after 271 timesteps, total rewards 4.0, mean loss 0.020762637200012694\n",
      "Episode 3264 finished after 127 timesteps, total rewards 2.0, mean loss 0.01782433918262942\n",
      "Episode 3265 finished after 93 timesteps, total rewards 1.0, mean loss 0.01307033787491501\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 3266 finished after 193 timesteps, total rewards 3.0, mean loss 0.01697399852878216\n",
      "Episode 3267 finished after 225 timesteps, total rewards 6.0, mean loss 0.0178144225009924\n",
      "Episode 3268 finished after 244 timesteps, total rewards 4.0, mean loss 0.01807427056611813\n",
      "Episode 3269 finished after 92 timesteps, total rewards 1.0, mean loss 0.02047524030216585\n",
      "Episode 3270 finished after 242 timesteps, total rewards 4.0, mean loss 0.016151479646478863\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 200.000000\n",
      "---------------------------------------\n",
      "Episode 3271 finished after 88 timesteps, total rewards 1.0, mean loss 0.018814834899702957\n",
      "Episode 3272 finished after 173 timesteps, total rewards 2.0, mean loss 0.01761953551201096\n",
      "Episode 3273 finished after 89 timesteps, total rewards 2.0, mean loss 0.015539505444414747\n",
      "Episode 3274 finished after 173 timesteps, total rewards 2.0, mean loss 0.015734199478931597\n",
      "Episode 3275 finished after 266 timesteps, total rewards 6.0, mean loss 0.018228119940766573\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 3276 finished after 219 timesteps, total rewards 2.0, mean loss 0.015792458897858398\n",
      "Episode 3277 finished after 173 timesteps, total rewards 4.0, mean loss 0.017215031820599722\n",
      "Episode 3278 finished after 200 timesteps, total rewards 3.0, mean loss 0.01686038608138915\n",
      "Episode 3279 finished after 137 timesteps, total rewards 0.0, mean loss 0.020292233705459448\n",
      "Episode 3280 finished after 121 timesteps, total rewards 1.0, mean loss 0.01695550326354155\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 3281 finished after 104 timesteps, total rewards 1.0, mean loss 0.015822176088896904\n",
      "Episode 3282 finished after 165 timesteps, total rewards 5.0, mean loss 0.014792130072248366\n",
      "Episode 3283 finished after 362 timesteps, total rewards 8.0, mean loss 0.017086707538917393\n",
      "Episode 3284 finished after 128 timesteps, total rewards 0.0, mean loss 0.01746053059650876\n",
      "Episode 3285 finished after 290 timesteps, total rewards 8.0, mean loss 0.016530178324921572\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 3286 finished after 140 timesteps, total rewards 0.0, mean loss 0.01694078246863293\n",
      "Episode 3287 finished after 129 timesteps, total rewards 3.0, mean loss 0.017688533150439344\n",
      "Episode 3288 finished after 121 timesteps, total rewards 3.0, mean loss 0.01882594639833724\n",
      "Episode 3289 finished after 219 timesteps, total rewards 4.0, mean loss 0.017223741355794377\n",
      "Episode 3290 finished after 287 timesteps, total rewards 6.0, mean loss 0.020248879072133075\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 83.333333\n",
      "---------------------------------------\n",
      "Episode 3291 finished after 157 timesteps, total rewards 1.0, mean loss 0.016114358000893288\n",
      "Episode 3292 finished after 186 timesteps, total rewards 7.0, mean loss 0.016580589386316576\n",
      "Episode 3293 finished after 164 timesteps, total rewards 4.0, mean loss 0.017206258667212707\n",
      "Episode 3294 finished after 109 timesteps, total rewards 1.0, mean loss 0.01935300214211841\n",
      "Episode 3295 finished after 132 timesteps, total rewards 3.0, mean loss 0.020208254231626845\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 3296 finished after 124 timesteps, total rewards 1.0, mean loss 0.017682196414698998\n",
      "Episode 3297 finished after 331 timesteps, total rewards 9.0, mean loss 0.017024937372273424\n",
      "Episode 3298 finished after 203 timesteps, total rewards 5.0, mean loss 0.01780589889698281\n",
      "Episode 3299 finished after 189 timesteps, total rewards 2.0, mean loss 0.01692598343644508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 finished after 173 timesteps, total rewards 3.0, mean loss 0.018032737365638502\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 3301 finished after 167 timesteps, total rewards 1.0, mean loss 0.01687603325145263\n",
      "Episode 3302 finished after 154 timesteps, total rewards 5.0, mean loss 0.015396706677909771\n",
      "Episode 3303 finished after 151 timesteps, total rewards 3.0, mean loss 0.017835847346500698\n",
      "Episode 3304 finished after 249 timesteps, total rewards 8.0, mean loss 0.01634920345961554\n",
      "Episode 3305 finished after 261 timesteps, total rewards 7.0, mean loss 0.01669936414524356\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 3306 finished after 172 timesteps, total rewards 1.0, mean loss 0.01753893836068401\n",
      "Episode 3307 finished after 190 timesteps, total rewards 4.0, mean loss 0.016882299329766905\n",
      "Episode 3308 finished after 244 timesteps, total rewards 9.0, mean loss 0.016779761809615237\n",
      "Episode 3309 finished after 205 timesteps, total rewards 4.0, mean loss 0.01737351807504438\n",
      "Episode 3310 finished after 324 timesteps, total rewards 10.0, mean loss 0.01781812922800546\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 3311 finished after 227 timesteps, total rewards 3.0, mean loss 0.01703236803247302\n",
      "Episode 3312 finished after 157 timesteps, total rewards 3.0, mean loss 0.019163946033111376\n",
      "Episode 3313 finished after 226 timesteps, total rewards 3.0, mean loss 0.01663374903161631\n",
      "Episode 3314 finished after 284 timesteps, total rewards 3.0, mean loss 0.0161637834701228\n",
      "Episode 3315 finished after 125 timesteps, total rewards 2.0, mean loss 0.01535347656533122\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 3316 finished after 230 timesteps, total rewards 8.0, mean loss 0.017915213057447386\n",
      "Episode 3317 finished after 209 timesteps, total rewards 5.0, mean loss 0.017492836608348692\n",
      "Episode 3318 finished after 216 timesteps, total rewards 6.0, mean loss 0.015791320052812807\n",
      "Episode 3319 finished after 128 timesteps, total rewards 2.0, mean loss 0.015872107090672216\n",
      "Episode 3320 finished after 164 timesteps, total rewards 3.0, mean loss 0.01829809007283141\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 83.333333\n",
      "---------------------------------------\n",
      "Episode 3321 finished after 86 timesteps, total rewards 1.0, mean loss 0.018214681566162252\n",
      "Episode 3322 finished after 296 timesteps, total rewards 8.0, mean loss 0.018358176111202763\n",
      "Episode 3323 finished after 162 timesteps, total rewards 2.0, mean loss 0.01942844804625004\n",
      "Episode 3324 finished after 170 timesteps, total rewards 4.0, mean loss 0.018004743158406413\n",
      "Episode 3325 finished after 218 timesteps, total rewards 1.0, mean loss 0.017626992850820748\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 3326 finished after 253 timesteps, total rewards 7.0, mean loss 0.018758716918491267\n",
      "Episode 3327 finished after 196 timesteps, total rewards 2.0, mean loss 0.017069126717503925\n",
      "Episode 3328 finished after 140 timesteps, total rewards 4.0, mean loss 0.019277602940564974\n",
      "Episode 3329 finished after 157 timesteps, total rewards 4.0, mean loss 0.018564691255462302\n",
      "Episode 3330 finished after 225 timesteps, total rewards 2.0, mean loss 0.017451378987170755\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 3331 finished after 152 timesteps, total rewards 5.0, mean loss 0.01800796479265524\n",
      "Episode 3332 finished after 229 timesteps, total rewards 3.0, mean loss 0.018166264600942002\n",
      "Episode 3333 finished after 211 timesteps, total rewards 5.0, mean loss 0.015646452768882338\n",
      "Episode 3334 finished after 180 timesteps, total rewards 2.0, mean loss 0.018308367950440798\n",
      "Episode 3335 finished after 188 timesteps, total rewards 4.0, mean loss 0.015309035826345628\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 3336 finished after 266 timesteps, total rewards 6.0, mean loss 0.019280751745209546\n",
      "Episode 3337 finished after 170 timesteps, total rewards 5.0, mean loss 0.018182475288288997\n",
      "Episode 3338 finished after 221 timesteps, total rewards 8.0, mean loss 0.016425303694459176\n",
      "Episode 3339 finished after 91 timesteps, total rewards 3.0, mean loss 0.013869849468612081\n",
      "Episode 3340 finished after 141 timesteps, total rewards 2.0, mean loss 0.01686240326743639\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 3341 finished after 124 timesteps, total rewards 4.0, mean loss 0.016860398049879398\n",
      "Episode 3342 finished after 88 timesteps, total rewards 0.0, mean loss 0.0168575130373938\n",
      "Episode 3343 finished after 174 timesteps, total rewards 2.0, mean loss 0.016524952050196755\n",
      "Episode 3344 finished after 167 timesteps, total rewards 4.0, mean loss 0.019965847728294347\n",
      "Episode 3345 finished after 173 timesteps, total rewards 5.0, mean loss 0.01997115766909512\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 3346 finished after 119 timesteps, total rewards 1.0, mean loss 0.016111828878858446\n",
      "Episode 3347 finished after 256 timesteps, total rewards 4.0, mean loss 0.016756409791469196\n",
      "Episode 3348 finished after 257 timesteps, total rewards 7.0, mean loss 0.016928089866849527\n",
      "Episode 3349 finished after 99 timesteps, total rewards 2.0, mean loss 0.016396075314277725\n",
      "Episode 3350 finished after 221 timesteps, total rewards 4.0, mean loss 0.01772734908175305\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 120.000000\n",
      "---------------------------------------\n",
      "Episode 3351 finished after 215 timesteps, total rewards 4.0, mean loss 0.01828662410086064\n",
      "Episode 3352 finished after 166 timesteps, total rewards 6.0, mean loss 0.016716307213569218\n",
      "Episode 3353 finished after 141 timesteps, total rewards 1.0, mean loss 0.019616027668131687\n",
      "Episode 3354 finished after 156 timesteps, total rewards 1.0, mean loss 0.019624059079946257\n",
      "Episode 3355 finished after 130 timesteps, total rewards 2.0, mean loss 0.018280003710578266\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 3356 finished after 134 timesteps, total rewards 3.0, mean loss 0.01689039931611728\n",
      "Episode 3357 finished after 87 timesteps, total rewards 3.0, mean loss 0.014188647917177442\n",
      "Episode 3358 finished after 192 timesteps, total rewards 2.0, mean loss 0.01938920777441429\n",
      "Episode 3359 finished after 306 timesteps, total rewards 7.0, mean loss 0.019417368316508563\n",
      "Episode 3360 finished after 230 timesteps, total rewards 5.0, mean loss 0.01758211306599981\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 83.333333\n",
      "---------------------------------------\n",
      "Episode 3361 finished after 116 timesteps, total rewards 1.0, mean loss 0.020054394185567145\n",
      "Episode 3362 finished after 95 timesteps, total rewards 0.0, mean loss 0.017103559971100798\n",
      "Episode 3363 finished after 228 timesteps, total rewards 3.0, mean loss 0.018456914321735927\n",
      "Episode 3364 finished after 161 timesteps, total rewards 2.0, mean loss 0.016710559475981854\n",
      "Episode 3365 finished after 164 timesteps, total rewards 4.0, mean loss 0.01854645475928342\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 3366 finished after 295 timesteps, total rewards 4.0, mean loss 0.018698590620957568\n",
      "Episode 3367 finished after 119 timesteps, total rewards 4.0, mean loss 0.015178591553454421\n",
      "Episode 3368 finished after 151 timesteps, total rewards 3.0, mean loss 0.016673044337033732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3369 finished after 134 timesteps, total rewards 4.0, mean loss 0.020780792112337118\n",
      "Episode 3370 finished after 122 timesteps, total rewards 2.0, mean loss 0.014580846722161428\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 3371 finished after 131 timesteps, total rewards 1.0, mean loss 0.015039627777967062\n",
      "Episode 3372 finished after 234 timesteps, total rewards 8.0, mean loss 0.01808671210693498\n",
      "Episode 3373 finished after 92 timesteps, total rewards 4.0, mean loss 0.020066824756846156\n",
      "Episode 3374 finished after 173 timesteps, total rewards 5.0, mean loss 0.01733035464889526\n",
      "Episode 3375 finished after 162 timesteps, total rewards 2.0, mean loss 0.019337750389959295\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 3376 finished after 166 timesteps, total rewards 5.0, mean loss 0.017721259829038417\n",
      "Episode 3377 finished after 123 timesteps, total rewards 1.0, mean loss 0.01730844753603186\n",
      "Episode 3378 finished after 194 timesteps, total rewards 5.0, mean loss 0.018485520657910436\n",
      "Episode 3379 finished after 133 timesteps, total rewards 4.0, mean loss 0.021055800248553653\n",
      "Episode 3380 finished after 185 timesteps, total rewards 8.0, mean loss 0.017192376189160388\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 3381 finished after 124 timesteps, total rewards 2.0, mean loss 0.014018792107949153\n",
      "Episode 3382 finished after 82 timesteps, total rewards 2.0, mean loss 0.015225705185269073\n",
      "Episode 3383 finished after 114 timesteps, total rewards 2.0, mean loss 0.01786701940255354\n",
      "Episode 3384 finished after 128 timesteps, total rewards 5.0, mean loss 0.016415414983384835\n",
      "Episode 3385 finished after 218 timesteps, total rewards 7.0, mean loss 0.01886740475204141\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 103.333333\n",
      "---------------------------------------\n",
      "Episode 3386 finished after 125 timesteps, total rewards 2.0, mean loss 0.015111084997653962\n",
      "Episode 3387 finished after 261 timesteps, total rewards 5.0, mean loss 0.017987706491516695\n",
      "Episode 3388 finished after 224 timesteps, total rewards 10.0, mean loss 0.017903148593437175\n",
      "Episode 3389 finished after 112 timesteps, total rewards 0.0, mean loss 0.01790730519340806\n",
      "Episode 3390 finished after 291 timesteps, total rewards 5.0, mean loss 0.019963684642433005\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 3391 finished after 267 timesteps, total rewards 6.0, mean loss 0.019208776783756213\n",
      "Episode 3392 finished after 200 timesteps, total rewards 1.0, mean loss 0.01661773044470465\n",
      "Episode 3393 finished after 219 timesteps, total rewards 3.0, mean loss 0.01892096355565947\n",
      "Episode 3394 finished after 101 timesteps, total rewards 0.0, mean loss 0.018772683401435317\n",
      "Episode 3395 finished after 192 timesteps, total rewards 3.0, mean loss 0.018908451382837182\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 3396 finished after 97 timesteps, total rewards 2.0, mean loss 0.01812435518806205\n",
      "Episode 3397 finished after 134 timesteps, total rewards 2.0, mean loss 0.01936082540813194\n",
      "Episode 3398 finished after 132 timesteps, total rewards 5.0, mean loss 0.018658589225996173\n",
      "Episode 3399 finished after 122 timesteps, total rewards 2.0, mean loss 0.01856617547464786\n",
      "Episode 3400 finished after 236 timesteps, total rewards 7.0, mean loss 0.018651405027751828\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 3401 finished after 143 timesteps, total rewards 4.0, mean loss 0.02056897598908159\n",
      "Episode 3402 finished after 143 timesteps, total rewards 4.0, mean loss 0.021381192738060037\n",
      "Episode 3403 finished after 174 timesteps, total rewards 4.0, mean loss 0.01759061284414118\n",
      "Episode 3404 finished after 199 timesteps, total rewards 3.0, mean loss 0.017316899000287954\n",
      "Episode 3405 finished after 139 timesteps, total rewards 2.0, mean loss 0.017634845233234248\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 86.666667\n",
      "---------------------------------------\n",
      "Episode 3406 finished after 247 timesteps, total rewards 5.0, mean loss 0.0180231568413911\n",
      "Episode 3407 finished after 90 timesteps, total rewards 1.0, mean loss 0.021296248789359297\n",
      "Episode 3408 finished after 123 timesteps, total rewards 1.0, mean loss 0.02055774687607659\n",
      "Episode 3409 finished after 254 timesteps, total rewards 4.0, mean loss 0.019229033885351344\n",
      "Episode 3410 finished after 92 timesteps, total rewards 3.0, mean loss 0.01865364986588247\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3411 finished after 135 timesteps, total rewards 0.0, mean loss 0.019170314030445838\n",
      "Episode 3412 finished after 110 timesteps, total rewards 2.0, mean loss 0.019326869813217358\n",
      "Episode 3413 finished after 210 timesteps, total rewards 7.0, mean loss 0.018899011558442865\n",
      "Episode 3414 finished after 110 timesteps, total rewards 2.0, mean loss 0.019177577238191256\n",
      "Episode 3415 finished after 196 timesteps, total rewards 4.0, mean loss 0.01520314747146901\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 3416 finished after 170 timesteps, total rewards 5.0, mean loss 0.01710609241443522\n",
      "Episode 3417 finished after 145 timesteps, total rewards 1.0, mean loss 0.015045784257256008\n",
      "Episode 3418 finished after 198 timesteps, total rewards 6.0, mean loss 0.01931834640922119\n",
      "Episode 3419 finished after 135 timesteps, total rewards 3.0, mean loss 0.01934706816725709\n",
      "Episode 3420 finished after 207 timesteps, total rewards 4.0, mean loss 0.01862710385846541\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 3421 finished after 159 timesteps, total rewards 4.0, mean loss 0.017730750695585547\n",
      "Episode 3422 finished after 138 timesteps, total rewards 1.0, mean loss 0.015965748035460307\n",
      "Episode 3423 finished after 166 timesteps, total rewards 5.0, mean loss 0.018794287804721754\n",
      "Episode 3424 finished after 301 timesteps, total rewards 5.0, mean loss 0.01728758528214443\n",
      "Episode 3425 finished after 144 timesteps, total rewards 4.0, mean loss 0.019973318042401742\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 110.000000\n",
      "---------------------------------------\n",
      "Episode 3426 finished after 204 timesteps, total rewards 5.0, mean loss 0.020119660305128635\n",
      "Episode 3427 finished after 239 timesteps, total rewards 5.0, mean loss 0.01713682765873718\n",
      "Episode 3428 finished after 165 timesteps, total rewards 6.0, mean loss 0.015928703353909605\n",
      "Episode 3429 finished after 127 timesteps, total rewards 2.0, mean loss 0.018007679743413614\n",
      "Episode 3430 finished after 164 timesteps, total rewards 2.0, mean loss 0.016568601095591808\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 3431 finished after 143 timesteps, total rewards 5.0, mean loss 0.01793323607359837\n",
      "Episode 3432 finished after 99 timesteps, total rewards 3.0, mean loss 0.016855084235577685\n",
      "Episode 3433 finished after 173 timesteps, total rewards 5.0, mean loss 0.01811571235531629\n",
      "Episode 3434 finished after 220 timesteps, total rewards 2.0, mean loss 0.019685922269921073\n",
      "Episode 3435 finished after 89 timesteps, total rewards 2.0, mean loss 0.02062613164006701\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 3436 finished after 147 timesteps, total rewards 2.0, mean loss 0.01690840401819774\n",
      "Episode 3437 finished after 231 timesteps, total rewards 1.0, mean loss 0.018657164588959652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3438 finished after 233 timesteps, total rewards 6.0, mean loss 0.021463784424172388\n",
      "Episode 3439 finished after 98 timesteps, total rewards 1.0, mean loss 0.01804553041334397\n",
      "Episode 3440 finished after 154 timesteps, total rewards 2.0, mean loss 0.016525219159439667\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 3441 finished after 274 timesteps, total rewards 5.0, mean loss 0.018686857306480027\n",
      "Episode 3442 finished after 111 timesteps, total rewards 0.0, mean loss 0.01913779277274771\n",
      "Episode 3443 finished after 192 timesteps, total rewards 4.0, mean loss 0.017065598323218484\n",
      "Episode 3444 finished after 275 timesteps, total rewards 4.0, mean loss 0.017217902571640232\n",
      "Episode 3445 finished after 252 timesteps, total rewards 4.0, mean loss 0.01965608288273437\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 3446 finished after 82 timesteps, total rewards 3.0, mean loss 0.01878263321052101\n",
      "Episode 3447 finished after 271 timesteps, total rewards 8.0, mean loss 0.01714462193710269\n",
      "Episode 3448 finished after 270 timesteps, total rewards 2.0, mean loss 0.017472865739276772\n",
      "Episode 3449 finished after 90 timesteps, total rewards 4.0, mean loss 0.014597124997009006\n",
      "Episode 3450 finished after 115 timesteps, total rewards 4.0, mean loss 0.01724080500172694\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 115.000000\n",
      "---------------------------------------\n",
      "Episode 3451 finished after 296 timesteps, total rewards 6.0, mean loss 0.017063374280681002\n",
      "Episode 3452 finished after 109 timesteps, total rewards 1.0, mean loss 0.01949197961948812\n",
      "Episode 3453 finished after 282 timesteps, total rewards 5.0, mean loss 0.018526225248417745\n",
      "Episode 3454 finished after 178 timesteps, total rewards 5.0, mean loss 0.01731063274528015\n",
      "Episode 3455 finished after 294 timesteps, total rewards 5.0, mean loss 0.018599800464576807\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 3456 finished after 346 timesteps, total rewards 9.0, mean loss 0.017536656138341808\n",
      "Episode 3457 finished after 216 timesteps, total rewards 2.0, mean loss 0.020885013963884883\n",
      "Episode 3458 finished after 273 timesteps, total rewards 8.0, mean loss 0.018798029740194316\n",
      "Episode 3459 finished after 94 timesteps, total rewards 0.0, mean loss 0.01597016892381052\n",
      "Episode 3460 finished after 99 timesteps, total rewards 0.0, mean loss 0.017598977940970786\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 3461 finished after 222 timesteps, total rewards 5.0, mean loss 0.017581468497335842\n",
      "Episode 3462 finished after 120 timesteps, total rewards 0.0, mean loss 0.01795078298212805\n",
      "Episode 3463 finished after 147 timesteps, total rewards 2.0, mean loss 0.016551833676624106\n",
      "Episode 3464 finished after 199 timesteps, total rewards 6.0, mean loss 0.016760978750631813\n",
      "Episode 3465 finished after 117 timesteps, total rewards 2.0, mean loss 0.018871235976127\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 128.333333\n",
      "---------------------------------------\n",
      "Episode 3466 finished after 136 timesteps, total rewards 1.0, mean loss 0.01819868056319298\n",
      "Episode 3467 finished after 116 timesteps, total rewards 5.0, mean loss 0.017535556198081708\n",
      "Episode 3468 finished after 96 timesteps, total rewards 2.0, mean loss 0.015882110791911448\n",
      "Episode 3469 finished after 217 timesteps, total rewards 5.0, mean loss 0.019563061083250673\n",
      "Episode 3470 finished after 187 timesteps, total rewards 3.0, mean loss 0.015773829864275786\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 3471 finished after 203 timesteps, total rewards 6.0, mean loss 0.019274794793942382\n",
      "Episode 3472 finished after 228 timesteps, total rewards 7.0, mean loss 0.01930729200677505\n",
      "Episode 3473 finished after 121 timesteps, total rewards 2.0, mean loss 0.020614756835039612\n",
      "Episode 3474 finished after 113 timesteps, total rewards 2.0, mean loss 0.01961166621391884\n",
      "Episode 3475 finished after 167 timesteps, total rewards 3.0, mean loss 0.016662433603360224\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3476 finished after 174 timesteps, total rewards 4.0, mean loss 0.018767577922506922\n",
      "Episode 3477 finished after 198 timesteps, total rewards 3.0, mean loss 0.020269308733104757\n",
      "Episode 3478 finished after 235 timesteps, total rewards 3.0, mean loss 0.01742612300539746\n",
      "Episode 3479 finished after 210 timesteps, total rewards 8.0, mean loss 0.02186847171230641\n",
      "Episode 3480 finished after 175 timesteps, total rewards 1.0, mean loss 0.017692759174720518\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 3481 finished after 253 timesteps, total rewards 2.0, mean loss 0.017042301967003834\n",
      "Episode 3482 finished after 136 timesteps, total rewards 5.0, mean loss 0.016063706532483648\n",
      "Episode 3483 finished after 89 timesteps, total rewards 2.0, mean loss 0.019833736902552794\n",
      "Episode 3484 finished after 157 timesteps, total rewards 1.0, mean loss 0.019347405468208633\n",
      "Episode 3485 finished after 177 timesteps, total rewards 6.0, mean loss 0.01919021555038411\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 3486 finished after 148 timesteps, total rewards 4.0, mean loss 0.017194805767487834\n",
      "Episode 3487 finished after 180 timesteps, total rewards 5.0, mean loss 0.01699921400997684\n",
      "Episode 3488 finished after 136 timesteps, total rewards 3.0, mean loss 0.01710332596249988\n",
      "Episode 3489 finished after 107 timesteps, total rewards 3.0, mean loss 0.02149120308828723\n",
      "Episode 3490 finished after 183 timesteps, total rewards 4.0, mean loss 0.017450089088661466\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 3491 finished after 102 timesteps, total rewards 2.0, mean loss 0.0186977235819487\n",
      "Episode 3492 finished after 128 timesteps, total rewards 1.0, mean loss 0.0196384816963473\n",
      "Episode 3493 finished after 206 timesteps, total rewards 7.0, mean loss 0.02005646465169899\n",
      "Episode 3494 finished after 150 timesteps, total rewards 3.0, mean loss 0.019726623604850224\n",
      "Episode 3495 finished after 152 timesteps, total rewards 2.0, mean loss 0.01875554472912642\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 3496 finished after 141 timesteps, total rewards 3.0, mean loss 0.02030638999021645\n",
      "Episode 3497 finished after 128 timesteps, total rewards 5.0, mean loss 0.020274156360756024\n",
      "Episode 3498 finished after 126 timesteps, total rewards 2.0, mean loss 0.017716498976196385\n",
      "Episode 3499 finished after 134 timesteps, total rewards 2.0, mean loss 0.019865879784650934\n",
      "Episode 3500 finished after 182 timesteps, total rewards 2.0, mean loss 0.018878964173791746\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 3501 finished after 168 timesteps, total rewards 5.0, mean loss 0.01839067339425951\n",
      "Episode 3502 finished after 265 timesteps, total rewards 8.0, mean loss 0.018334559796219868\n",
      "Episode 3503 finished after 106 timesteps, total rewards 2.0, mean loss 0.016635462285331363\n",
      "Episode 3504 finished after 199 timesteps, total rewards 2.0, mean loss 0.016867241185701373\n",
      "Episode 3505 finished after 161 timesteps, total rewards 2.0, mean loss 0.017901761841277668\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 3506 finished after 141 timesteps, total rewards 2.0, mean loss 0.016806055622419727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3507 finished after 267 timesteps, total rewards 3.0, mean loss 0.017462214930480075\n",
      "Episode 3508 finished after 159 timesteps, total rewards 4.0, mean loss 0.020774251287920202\n",
      "Episode 3509 finished after 166 timesteps, total rewards 4.0, mean loss 0.017360069083487504\n",
      "Episode 3510 finished after 249 timesteps, total rewards 1.0, mean loss 0.017332194989451755\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 3511 finished after 213 timesteps, total rewards 5.0, mean loss 0.017512174393159684\n",
      "Episode 3512 finished after 148 timesteps, total rewards 5.0, mean loss 0.019242672644149687\n",
      "Episode 3513 finished after 158 timesteps, total rewards 1.0, mean loss 0.019332455287693374\n",
      "Episode 3514 finished after 382 timesteps, total rewards 8.0, mean loss 0.018683506571443957\n",
      "Episode 3515 finished after 210 timesteps, total rewards 3.0, mean loss 0.018679147928542944\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 3516 finished after 141 timesteps, total rewards 3.0, mean loss 0.01955752779980659\n",
      "Episode 3517 finished after 131 timesteps, total rewards 2.0, mean loss 0.016418250856160378\n",
      "Episode 3518 finished after 223 timesteps, total rewards 6.0, mean loss 0.018725735297992475\n",
      "Episode 3519 finished after 167 timesteps, total rewards 4.0, mean loss 0.017304503718602978\n",
      "Episode 3520 finished after 160 timesteps, total rewards 5.0, mean loss 0.017030503166461132\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 3521 finished after 281 timesteps, total rewards 4.0, mean loss 0.018440974264793443\n",
      "Episode 3522 finished after 306 timesteps, total rewards 4.0, mean loss 0.018831930513603285\n",
      "Episode 3523 finished after 87 timesteps, total rewards 3.0, mean loss 0.018904152886952734\n",
      "Episode 3524 finished after 201 timesteps, total rewards 2.0, mean loss 0.018162725027071416\n",
      "Episode 3525 finished after 161 timesteps, total rewards 4.0, mean loss 0.015264646482063885\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 3526 finished after 149 timesteps, total rewards 1.0, mean loss 0.017216752165297184\n",
      "Episode 3527 finished after 278 timesteps, total rewards 5.0, mean loss 0.018741205740023217\n",
      "Episode 3528 finished after 129 timesteps, total rewards 1.0, mean loss 0.012644944268968245\n",
      "Episode 3529 finished after 274 timesteps, total rewards 3.0, mean loss 0.01845423635571216\n",
      "Episode 3530 finished after 130 timesteps, total rewards 1.0, mean loss 0.013748827232764318\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 3531 finished after 127 timesteps, total rewards 2.0, mean loss 0.01770035674938507\n",
      "Episode 3532 finished after 233 timesteps, total rewards 6.0, mean loss 0.018516050839033217\n",
      "Episode 3533 finished after 311 timesteps, total rewards 4.0, mean loss 0.01890205289162336\n",
      "Episode 3534 finished after 324 timesteps, total rewards 12.0, mean loss 0.016689964502555443\n",
      "Episode 3535 finished after 296 timesteps, total rewards 10.0, mean loss 0.016918483877324853\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 3536 finished after 89 timesteps, total rewards 3.0, mean loss 0.016379511949156274\n",
      "Episode 3537 finished after 170 timesteps, total rewards 0.0, mean loss 0.018267078649060912\n",
      "Episode 3538 finished after 125 timesteps, total rewards 1.0, mean loss 0.02092484924569726\n",
      "Episode 3539 finished after 86 timesteps, total rewards 1.0, mean loss 0.01547464088341871\n",
      "Episode 3540 finished after 93 timesteps, total rewards 1.0, mean loss 0.014796023637617147\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 131.666667\n",
      "---------------------------------------\n",
      "Episode 3541 finished after 182 timesteps, total rewards 2.0, mean loss 0.017242589121131777\n",
      "Episode 3542 finished after 197 timesteps, total rewards 3.0, mean loss 0.016892089286871102\n",
      "Episode 3543 finished after 160 timesteps, total rewards 3.0, mean loss 0.01864329222444212\n",
      "Episode 3544 finished after 131 timesteps, total rewards 2.0, mean loss 0.01762038203336668\n",
      "Episode 3545 finished after 130 timesteps, total rewards 5.0, mean loss 0.02021039826885009\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 3546 finished after 228 timesteps, total rewards 5.0, mean loss 0.017989162331152903\n",
      "Episode 3547 finished after 197 timesteps, total rewards 0.0, mean loss 0.01745822473764798\n",
      "Episode 3548 finished after 123 timesteps, total rewards 1.0, mean loss 0.016105393223214623\n",
      "Episode 3549 finished after 184 timesteps, total rewards 3.0, mean loss 0.016047210828970572\n",
      "Episode 3550 finished after 164 timesteps, total rewards 2.0, mean loss 0.01910963101119439\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 3551 finished after 166 timesteps, total rewards 3.0, mean loss 0.017713897328905156\n",
      "Episode 3552 finished after 205 timesteps, total rewards 4.0, mean loss 0.01839392527188288\n",
      "Episode 3553 finished after 184 timesteps, total rewards 3.0, mean loss 0.017831091917722242\n",
      "Episode 3554 finished after 174 timesteps, total rewards 4.0, mean loss 0.0173430242100677\n",
      "Episode 3555 finished after 189 timesteps, total rewards 7.0, mean loss 0.014691690058464667\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 3556 finished after 302 timesteps, total rewards 4.0, mean loss 0.017843532243385814\n",
      "Episode 3557 finished after 88 timesteps, total rewards 1.0, mean loss 0.01569961246224755\n",
      "Episode 3558 finished after 265 timesteps, total rewards 4.0, mean loss 0.018183056401299698\n",
      "Episode 3559 finished after 96 timesteps, total rewards 3.0, mean loss 0.01686265683141149\n",
      "Episode 3560 finished after 92 timesteps, total rewards 1.0, mean loss 0.01730845412669663\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 3561 finished after 282 timesteps, total rewards 6.0, mean loss 0.01915448208951312\n",
      "Episode 3562 finished after 128 timesteps, total rewards 1.0, mean loss 0.021352870546252234\n",
      "Episode 3563 finished after 259 timesteps, total rewards 7.0, mean loss 0.018888061746602524\n",
      "Episode 3564 finished after 263 timesteps, total rewards 6.0, mean loss 0.019380135668845343\n",
      "Episode 3565 finished after 211 timesteps, total rewards 5.0, mean loss 0.020158138029306015\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 3566 finished after 118 timesteps, total rewards 4.0, mean loss 0.018208009469070297\n",
      "Episode 3567 finished after 262 timesteps, total rewards 6.0, mean loss 0.017359590864800743\n",
      "Episode 3568 finished after 280 timesteps, total rewards 6.0, mean loss 0.017762671717043432\n",
      "Episode 3569 finished after 368 timesteps, total rewards 6.0, mean loss 0.01870147725212914\n",
      "Episode 3570 finished after 280 timesteps, total rewards 4.0, mean loss 0.017416524988740485\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 3571 finished after 233 timesteps, total rewards 4.0, mean loss 0.016998783037737972\n",
      "Episode 3572 finished after 239 timesteps, total rewards 8.0, mean loss 0.018227677407994025\n",
      "Episode 3573 finished after 162 timesteps, total rewards 6.0, mean loss 0.01588924280885193\n",
      "Episode 3574 finished after 141 timesteps, total rewards 2.0, mean loss 0.0194645494475287\n",
      "Episode 3575 finished after 110 timesteps, total rewards 3.0, mean loss 0.016869929182576016\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3576 finished after 115 timesteps, total rewards 2.0, mean loss 0.016185695308503573\n",
      "Episode 3577 finished after 86 timesteps, total rewards 1.0, mean loss 0.019064874843109484\n",
      "Episode 3578 finished after 139 timesteps, total rewards 4.0, mean loss 0.019237733939361068\n",
      "Episode 3579 finished after 178 timesteps, total rewards 3.0, mean loss 0.02019498572917655\n",
      "Episode 3580 finished after 93 timesteps, total rewards 1.0, mean loss 0.019980259961949322\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 100.000000\n",
      "---------------------------------------\n",
      "Episode 3581 finished after 233 timesteps, total rewards 5.0, mean loss 0.01871680499747987\n",
      "Episode 3582 finished after 183 timesteps, total rewards 8.0, mean loss 0.018590133120000484\n",
      "Episode 3583 finished after 90 timesteps, total rewards 1.0, mean loss 0.017926531640761016\n",
      "Episode 3584 finished after 175 timesteps, total rewards 4.0, mean loss 0.016808093303947575\n",
      "Episode 3585 finished after 159 timesteps, total rewards 4.0, mean loss 0.01836249640770842\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 3586 finished after 196 timesteps, total rewards 3.0, mean loss 0.019025862948702916\n",
      "Episode 3587 finished after 86 timesteps, total rewards 1.0, mean loss 0.02104045087363311\n",
      "Episode 3588 finished after 122 timesteps, total rewards 2.0, mean loss 0.017234588248968185\n",
      "Episode 3589 finished after 163 timesteps, total rewards 3.0, mean loss 0.01822692947105099\n",
      "Episode 3590 finished after 270 timesteps, total rewards 4.0, mean loss 0.019100778436081278\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 3591 finished after 151 timesteps, total rewards 4.0, mean loss 0.02027519216949108\n",
      "Episode 3592 finished after 140 timesteps, total rewards 2.0, mean loss 0.015589390471411337\n",
      "Episode 3593 finished after 135 timesteps, total rewards 2.0, mean loss 0.013134225103486743\n",
      "Episode 3594 finished after 91 timesteps, total rewards 2.0, mean loss 0.0168098158108398\n",
      "Episode 3595 finished after 190 timesteps, total rewards 3.0, mean loss 0.021090200013407556\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3596 finished after 163 timesteps, total rewards 6.0, mean loss 0.017707493124769692\n",
      "Episode 3597 finished after 154 timesteps, total rewards 4.0, mean loss 0.012049187381461553\n",
      "Episode 3598 finished after 241 timesteps, total rewards 4.0, mean loss 0.017700971630247333\n",
      "Episode 3599 finished after 294 timesteps, total rewards 6.0, mean loss 0.018353335270903638\n",
      "Episode 3600 finished after 136 timesteps, total rewards 5.0, mean loss 0.017810668771477033\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 3601 finished after 95 timesteps, total rewards 2.0, mean loss 0.02081513066138876\n",
      "Episode 3602 finished after 301 timesteps, total rewards 3.0, mean loss 0.01728577043360681\n",
      "Episode 3603 finished after 128 timesteps, total rewards 0.0, mean loss 0.019400295530431322\n",
      "Episode 3604 finished after 163 timesteps, total rewards 4.0, mean loss 0.017443936499567027\n",
      "Episode 3605 finished after 158 timesteps, total rewards 3.0, mean loss 0.016285978238130744\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 3606 finished after 272 timesteps, total rewards 1.0, mean loss 0.015982390666442158\n",
      "Episode 3607 finished after 108 timesteps, total rewards 1.0, mean loss 0.01926856950632538\n",
      "Episode 3608 finished after 106 timesteps, total rewards 2.0, mean loss 0.021552857231208176\n",
      "Episode 3609 finished after 209 timesteps, total rewards 7.0, mean loss 0.018856435747367093\n",
      "Episode 3610 finished after 111 timesteps, total rewards 1.0, mean loss 0.017682057691566007\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 3611 finished after 156 timesteps, total rewards 1.0, mean loss 0.01656748024143804\n",
      "Episode 3612 finished after 388 timesteps, total rewards 11.0, mean loss 0.018220154836038175\n",
      "Episode 3613 finished after 286 timesteps, total rewards 6.0, mean loss 0.018799558695457824\n",
      "Episode 3614 finished after 228 timesteps, total rewards 3.0, mean loss 0.01950554325083973\n",
      "Episode 3615 finished after 166 timesteps, total rewards 5.0, mean loss 0.017952631807995466\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 123.333333\n",
      "---------------------------------------\n",
      "Episode 3616 finished after 110 timesteps, total rewards 0.0, mean loss 0.018931675937280738\n",
      "Episode 3617 finished after 96 timesteps, total rewards 1.0, mean loss 0.016228660652510978\n",
      "Episode 3618 finished after 323 timesteps, total rewards 6.0, mean loss 0.017670704544184516\n",
      "Episode 3619 finished after 238 timesteps, total rewards 6.0, mean loss 0.019968630601710665\n",
      "Episode 3620 finished after 166 timesteps, total rewards 1.0, mean loss 0.020914698346428215\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 3621 finished after 173 timesteps, total rewards 2.0, mean loss 0.017758069103691362\n",
      "Episode 3622 finished after 236 timesteps, total rewards 5.0, mean loss 0.018480110333658265\n",
      "Episode 3623 finished after 271 timesteps, total rewards 1.0, mean loss 0.017964382215387556\n",
      "Episode 3624 finished after 284 timesteps, total rewards 5.0, mean loss 0.017764273176664933\n",
      "Episode 3625 finished after 177 timesteps, total rewards 4.0, mean loss 0.016920337683355467\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3626 finished after 145 timesteps, total rewards 1.0, mean loss 0.018298259227343933\n",
      "Episode 3627 finished after 182 timesteps, total rewards 6.0, mean loss 0.01550059588395755\n",
      "Episode 3628 finished after 233 timesteps, total rewards 6.0, mean loss 0.016737188764671944\n",
      "Episode 3629 finished after 241 timesteps, total rewards 2.0, mean loss 0.01656903743529422\n",
      "Episode 3630 finished after 169 timesteps, total rewards 5.0, mean loss 0.016921160539774743\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 3631 finished after 256 timesteps, total rewards 4.0, mean loss 0.017127639924183313\n",
      "Episode 3632 finished after 171 timesteps, total rewards 4.0, mean loss 0.016252033063210547\n",
      "Episode 3633 finished after 228 timesteps, total rewards 2.0, mean loss 0.015627345724313176\n",
      "Episode 3634 finished after 278 timesteps, total rewards 7.0, mean loss 0.01870055932203214\n",
      "Episode 3635 finished after 154 timesteps, total rewards 3.0, mean loss 0.019233219183425045\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 81.666667\n",
      "---------------------------------------\n",
      "Episode 3636 finished after 196 timesteps, total rewards 6.0, mean loss 0.021150502950019603\n",
      "Episode 3637 finished after 208 timesteps, total rewards 4.0, mean loss 0.01714823039219482\n",
      "Episode 3638 finished after 248 timesteps, total rewards 4.0, mean loss 0.017568405696018148\n",
      "Episode 3639 finished after 165 timesteps, total rewards 3.0, mean loss 0.01893337215861362\n",
      "Episode 3640 finished after 152 timesteps, total rewards 3.0, mean loss 0.020587105056620203\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 91.666667\n",
      "---------------------------------------\n",
      "Episode 3641 finished after 269 timesteps, total rewards 6.0, mean loss 0.018875116744636104\n",
      "Episode 3642 finished after 199 timesteps, total rewards 4.0, mean loss 0.016958552087427834\n",
      "Episode 3643 finished after 158 timesteps, total rewards 6.0, mean loss 0.016459252560051464\n",
      "Episode 3644 finished after 272 timesteps, total rewards 5.0, mean loss 0.017433291948691476\n",
      "Episode 3645 finished after 217 timesteps, total rewards 4.0, mean loss 0.017914337523963972\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3646 finished after 139 timesteps, total rewards 3.0, mean loss 0.020035812132087327\n",
      "Episode 3647 finished after 322 timesteps, total rewards 5.0, mean loss 0.017700390034479593\n",
      "Episode 3648 finished after 211 timesteps, total rewards 0.0, mean loss 0.020688664059428347\n",
      "Episode 3649 finished after 148 timesteps, total rewards 2.0, mean loss 0.019901470646948076\n",
      "Episode 3650 finished after 187 timesteps, total rewards 1.0, mean loss 0.016400092910479676\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 3651 finished after 187 timesteps, total rewards 6.0, mean loss 0.01723577051465823\n",
      "Episode 3652 finished after 226 timesteps, total rewards 8.0, mean loss 0.0161268386162562\n",
      "Episode 3653 finished after 143 timesteps, total rewards 5.0, mean loss 0.017210690176010235\n",
      "Episode 3654 finished after 154 timesteps, total rewards 4.0, mean loss 0.017475292432067503\n",
      "Episode 3655 finished after 201 timesteps, total rewards 1.0, mean loss 0.01766702624162965\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 3656 finished after 320 timesteps, total rewards 5.0, mean loss 0.017540255259882544\n",
      "Episode 3657 finished after 269 timesteps, total rewards 5.0, mean loss 0.020030423889990868\n",
      "Episode 3658 finished after 111 timesteps, total rewards 0.0, mean loss 0.017766512181559526\n",
      "Episode 3659 finished after 218 timesteps, total rewards 4.0, mean loss 0.01690617577603232\n",
      "Episode 3660 finished after 264 timesteps, total rewards 3.0, mean loss 0.018388708660936434\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 3661 finished after 237 timesteps, total rewards 4.0, mean loss 0.018886263233723458\n",
      "Episode 3662 finished after 117 timesteps, total rewards 1.0, mean loss 0.014858043426249782\n",
      "Episode 3663 finished after 164 timesteps, total rewards 2.0, mean loss 0.01757164086036316\n",
      "Episode 3664 finished after 104 timesteps, total rewards 3.0, mean loss 0.01858429742480019\n",
      "Episode 3665 finished after 122 timesteps, total rewards 2.0, mean loss 0.01426489214264009\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 115.000000\n",
      "---------------------------------------\n",
      "Episode 3666 finished after 192 timesteps, total rewards 3.0, mean loss 0.015479043129441076\n",
      "Episode 3667 finished after 344 timesteps, total rewards 9.0, mean loss 0.01727730517720988\n",
      "Episode 3668 finished after 155 timesteps, total rewards 3.0, mean loss 0.01891434769176187\n",
      "Episode 3669 finished after 159 timesteps, total rewards 2.0, mean loss 0.020115940354957756\n",
      "Episode 3670 finished after 141 timesteps, total rewards 4.0, mean loss 0.019396082600512614\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 125.000000\n",
      "---------------------------------------\n",
      "Episode 3671 finished after 95 timesteps, total rewards 0.0, mean loss 0.017861384621478224\n",
      "Episode 3672 finished after 220 timesteps, total rewards 1.0, mean loss 0.017267000035975467\n",
      "Episode 3673 finished after 144 timesteps, total rewards 1.0, mean loss 0.01667767362475085\n",
      "Episode 3674 finished after 107 timesteps, total rewards 2.0, mean loss 0.01643206768181767\n",
      "Episode 3675 finished after 276 timesteps, total rewards 4.0, mean loss 0.01746674526847251\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 3676 finished after 92 timesteps, total rewards 0.0, mean loss 0.014500390786333415\n",
      "Episode 3677 finished after 221 timesteps, total rewards 4.0, mean loss 0.016828424335200324\n",
      "Episode 3678 finished after 161 timesteps, total rewards 3.0, mean loss 0.018858209124372936\n",
      "Episode 3679 finished after 161 timesteps, total rewards 7.0, mean loss 0.01972851186292246\n",
      "Episode 3680 finished after 162 timesteps, total rewards 4.0, mean loss 0.01649074417533192\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 3681 finished after 163 timesteps, total rewards 4.0, mean loss 0.01751677653726556\n",
      "Episode 3682 finished after 128 timesteps, total rewards 4.0, mean loss 0.020235274374499568\n",
      "Episode 3683 finished after 257 timesteps, total rewards 3.0, mean loss 0.018311506970731202\n",
      "Episode 3684 finished after 216 timesteps, total rewards 8.0, mean loss 0.01729957117748671\n",
      "Episode 3685 finished after 169 timesteps, total rewards 7.0, mean loss 0.017280814449713404\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 3686 finished after 91 timesteps, total rewards 1.0, mean loss 0.017108577005513995\n",
      "Episode 3687 finished after 98 timesteps, total rewards 1.0, mean loss 0.01684650738025084\n",
      "Episode 3688 finished after 167 timesteps, total rewards 2.0, mean loss 0.01818435277558395\n",
      "Episode 3689 finished after 214 timesteps, total rewards 4.0, mean loss 0.01858321868620865\n",
      "Episode 3690 finished after 93 timesteps, total rewards 4.0, mean loss 0.015771652679509855\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 3691 finished after 299 timesteps, total rewards 4.0, mean loss 0.01723324949580578\n",
      "Episode 3692 finished after 287 timesteps, total rewards 9.0, mean loss 0.0179447659523752\n",
      "Episode 3693 finished after 129 timesteps, total rewards 3.0, mean loss 0.016641012156046495\n",
      "Episode 3694 finished after 158 timesteps, total rewards 1.0, mean loss 0.017517183025741855\n",
      "Episode 3695 finished after 143 timesteps, total rewards 0.0, mean loss 0.018808322961090484\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3696 finished after 314 timesteps, total rewards 4.0, mean loss 0.017173768535025037\n",
      "Episode 3697 finished after 127 timesteps, total rewards 7.0, mean loss 0.016734010443700053\n",
      "Episode 3698 finished after 139 timesteps, total rewards 3.0, mean loss 0.016645809123795076\n",
      "Episode 3699 finished after 181 timesteps, total rewards 3.0, mean loss 0.017538764779983368\n",
      "Episode 3700 finished after 205 timesteps, total rewards 3.0, mean loss 0.016384770326236853\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 3701 finished after 206 timesteps, total rewards 1.0, mean loss 0.0162261247751303\n",
      "Episode 3702 finished after 156 timesteps, total rewards 4.0, mean loss 0.022210266032160666\n",
      "Episode 3703 finished after 236 timesteps, total rewards 8.0, mean loss 0.0171254694405588\n",
      "Episode 3704 finished after 107 timesteps, total rewards 1.0, mean loss 0.018921079476381365\n",
      "Episode 3705 finished after 127 timesteps, total rewards 0.0, mean loss 0.017976631584584596\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 3706 finished after 151 timesteps, total rewards 4.0, mean loss 0.018821441728023455\n",
      "Episode 3707 finished after 161 timesteps, total rewards 3.0, mean loss 0.01703755813433864\n",
      "Episode 3708 finished after 163 timesteps, total rewards 1.0, mean loss 0.01928712446756798\n",
      "Episode 3709 finished after 211 timesteps, total rewards 2.0, mean loss 0.01558440697480929\n",
      "Episode 3710 finished after 303 timesteps, total rewards 5.0, mean loss 0.01726749924275864\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 3711 finished after 169 timesteps, total rewards 2.0, mean loss 0.019513080220327838\n",
      "Episode 3712 finished after 167 timesteps, total rewards 4.0, mean loss 0.017959626204010745\n",
      "Episode 3713 finished after 248 timesteps, total rewards 4.0, mean loss 0.018443189062846584\n",
      "Episode 3714 finished after 87 timesteps, total rewards 0.0, mean loss 0.01783913991617403\n",
      "Episode 3715 finished after 231 timesteps, total rewards 5.0, mean loss 0.01784118998834049\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3716 finished after 80 timesteps, total rewards 3.0, mean loss 0.013822970711044037\n",
      "Episode 3717 finished after 244 timesteps, total rewards 5.0, mean loss 0.017507518780490262\n",
      "Episode 3718 finished after 264 timesteps, total rewards 5.0, mean loss 0.017471492172979088\n",
      "Episode 3719 finished after 161 timesteps, total rewards 5.0, mean loss 0.015076126956080104\n",
      "Episode 3720 finished after 226 timesteps, total rewards 5.0, mean loss 0.017981064095845923\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 3721 finished after 89 timesteps, total rewards 3.0, mean loss 0.01652287285162785\n",
      "Episode 3722 finished after 155 timesteps, total rewards 5.0, mean loss 0.018838078446776395\n",
      "Episode 3723 finished after 128 timesteps, total rewards 1.0, mean loss 0.01833672524026042\n",
      "Episode 3724 finished after 114 timesteps, total rewards 0.0, mean loss 0.018243024189008826\n",
      "Episode 3725 finished after 180 timesteps, total rewards 2.0, mean loss 0.016562065224732376\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 3726 finished after 167 timesteps, total rewards 2.0, mean loss 0.01617437020755613\n",
      "Episode 3727 finished after 92 timesteps, total rewards 1.0, mean loss 0.017240701173685247\n",
      "Episode 3728 finished after 91 timesteps, total rewards 0.0, mean loss 0.01702036397214365\n",
      "Episode 3729 finished after 206 timesteps, total rewards 8.0, mean loss 0.015892723552629495\n",
      "Episode 3730 finished after 224 timesteps, total rewards 5.0, mean loss 0.01663350575082794\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 3731 finished after 154 timesteps, total rewards 5.0, mean loss 0.014968659390858677\n",
      "Episode 3732 finished after 93 timesteps, total rewards 3.0, mean loss 0.018448613155933637\n",
      "Episode 3733 finished after 109 timesteps, total rewards 2.0, mean loss 0.014301677815479422\n",
      "Episode 3734 finished after 105 timesteps, total rewards 1.0, mean loss 0.013459230811401669\n",
      "Episode 3735 finished after 260 timesteps, total rewards 3.0, mean loss 0.0180230477587499\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 3736 finished after 167 timesteps, total rewards 3.0, mean loss 0.016037164625787026\n",
      "Episode 3737 finished after 143 timesteps, total rewards 3.0, mean loss 0.02055793356596512\n",
      "Episode 3738 finished after 87 timesteps, total rewards 0.0, mean loss 0.01775507591450694\n",
      "Episode 3739 finished after 95 timesteps, total rewards 2.0, mean loss 0.014705289419936506\n",
      "Episode 3740 finished after 202 timesteps, total rewards 2.0, mean loss 0.018925996738748792\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 3741 finished after 130 timesteps, total rewards 2.0, mean loss 0.016728953133301378\n",
      "Episode 3742 finished after 205 timesteps, total rewards 3.0, mean loss 0.019464209011369724\n",
      "Episode 3743 finished after 141 timesteps, total rewards 3.0, mean loss 0.017631513922779115\n",
      "Episode 3744 finished after 163 timesteps, total rewards 1.0, mean loss 0.017438892630054586\n",
      "Episode 3745 finished after 170 timesteps, total rewards 3.0, mean loss 0.01854393026271068\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 121.666667\n",
      "---------------------------------------\n",
      "Episode 3746 finished after 146 timesteps, total rewards 0.0, mean loss 0.018325647041010223\n",
      "Episode 3747 finished after 112 timesteps, total rewards 5.0, mean loss 0.01801272528973641\n",
      "Episode 3748 finished after 127 timesteps, total rewards 4.0, mean loss 0.018229330327053827\n",
      "Episode 3749 finished after 131 timesteps, total rewards 2.0, mean loss 0.016925771228386132\n",
      "Episode 3750 finished after 272 timesteps, total rewards 5.0, mean loss 0.01725119637729436\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 3751 finished after 154 timesteps, total rewards 0.0, mean loss 0.017674476420809212\n",
      "Episode 3752 finished after 145 timesteps, total rewards 8.0, mean loss 0.018466597674253942\n",
      "Episode 3753 finished after 214 timesteps, total rewards 3.0, mean loss 0.01538144129439405\n",
      "Episode 3754 finished after 232 timesteps, total rewards 6.0, mean loss 0.0182715633930237\n",
      "Episode 3755 finished after 250 timesteps, total rewards 13.0, mean loss 0.016393663248978556\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 3756 finished after 221 timesteps, total rewards 6.0, mean loss 0.016949144201521757\n",
      "Episode 3757 finished after 125 timesteps, total rewards 2.0, mean loss 0.017112145650200545\n",
      "Episode 3758 finished after 156 timesteps, total rewards 4.0, mean loss 0.017728022296721928\n",
      "Episode 3759 finished after 194 timesteps, total rewards 1.0, mean loss 0.015587774098169097\n",
      "Episode 3760 finished after 143 timesteps, total rewards 3.0, mean loss 0.017735419865336166\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3761 finished after 116 timesteps, total rewards 1.0, mean loss 0.01640661692062136\n",
      "Episode 3762 finished after 178 timesteps, total rewards 3.0, mean loss 0.020048435490311572\n",
      "Episode 3763 finished after 137 timesteps, total rewards 1.0, mean loss 0.016755477509148638\n",
      "Episode 3764 finished after 223 timesteps, total rewards 2.0, mean loss 0.016359978399318832\n",
      "Episode 3765 finished after 89 timesteps, total rewards 1.0, mean loss 0.013982897675386892\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 3766 finished after 242 timesteps, total rewards 4.0, mean loss 0.017031935580971474\n",
      "Episode 3767 finished after 117 timesteps, total rewards 1.0, mean loss 0.014465002175898124\n",
      "Episode 3768 finished after 157 timesteps, total rewards 3.0, mean loss 0.017078849370591342\n",
      "Episode 3769 finished after 141 timesteps, total rewards 1.0, mean loss 0.019673681893907752\n",
      "Episode 3770 finished after 233 timesteps, total rewards 4.0, mean loss 0.01943609265754471\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 3771 finished after 99 timesteps, total rewards 0.0, mean loss 0.018980089205815787\n",
      "Episode 3772 finished after 128 timesteps, total rewards 3.0, mean loss 0.01841963666083757\n",
      "Episode 3773 finished after 206 timesteps, total rewards 3.0, mean loss 0.01614642358052734\n",
      "Episode 3774 finished after 131 timesteps, total rewards 2.0, mean loss 0.018031501507151958\n",
      "Episode 3775 finished after 125 timesteps, total rewards 1.0, mean loss 0.01747956049628556\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 3776 finished after 138 timesteps, total rewards 2.0, mean loss 0.018011322251750506\n",
      "Episode 3777 finished after 207 timesteps, total rewards 4.0, mean loss 0.016800319928347452\n",
      "Episode 3778 finished after 180 timesteps, total rewards 5.0, mean loss 0.01686106105464407\n",
      "Episode 3779 finished after 140 timesteps, total rewards 2.0, mean loss 0.016270206107791247\n",
      "Episode 3780 finished after 334 timesteps, total rewards 8.0, mean loss 0.015786661971590486\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 3781 finished after 88 timesteps, total rewards 2.0, mean loss 0.015975536352594976\n",
      "Episode 3782 finished after 151 timesteps, total rewards 1.0, mean loss 0.015263253056586084\n",
      "Episode 3783 finished after 89 timesteps, total rewards 0.0, mean loss 0.015079532628303414\n",
      "Episode 3784 finished after 90 timesteps, total rewards 3.0, mean loss 0.015522421871881104\n",
      "Episode 3785 finished after 116 timesteps, total rewards 0.0, mean loss 0.015353385770516792\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3786 finished after 135 timesteps, total rewards 1.0, mean loss 0.018683401567654478\n",
      "Episode 3787 finished after 225 timesteps, total rewards 5.0, mean loss 0.018483497622526355\n",
      "Episode 3788 finished after 157 timesteps, total rewards 4.0, mean loss 0.016112359444192568\n",
      "Episode 3789 finished after 228 timesteps, total rewards 2.0, mean loss 0.015978131653626628\n",
      "Episode 3790 finished after 123 timesteps, total rewards 3.0, mean loss 0.020611857408581954\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 3791 finished after 166 timesteps, total rewards 3.0, mean loss 0.018272647797550827\n",
      "Episode 3792 finished after 172 timesteps, total rewards 5.0, mean loss 0.018495135871574368\n",
      "Episode 3793 finished after 305 timesteps, total rewards 13.0, mean loss 0.017537406667089854\n",
      "Episode 3794 finished after 179 timesteps, total rewards 5.0, mean loss 0.0195760686541049\n",
      "Episode 3795 finished after 154 timesteps, total rewards 3.0, mean loss 0.015215825368105979\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 3796 finished after 161 timesteps, total rewards 5.0, mean loss 0.016364438002292783\n",
      "Episode 3797 finished after 201 timesteps, total rewards 6.0, mean loss 0.017526860358147184\n",
      "Episode 3798 finished after 170 timesteps, total rewards 2.0, mean loss 0.015501884658656576\n",
      "Episode 3799 finished after 161 timesteps, total rewards 2.0, mean loss 0.020359827475969372\n",
      "Episode 3800 finished after 211 timesteps, total rewards 5.0, mean loss 0.01831887170487035\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 3801 finished after 268 timesteps, total rewards 6.0, mean loss 0.014705020199363022\n",
      "Episode 3802 finished after 128 timesteps, total rewards 0.0, mean loss 0.015457352969860949\n",
      "Episode 3803 finished after 279 timesteps, total rewards 7.0, mean loss 0.016020178360076735\n",
      "Episode 3804 finished after 164 timesteps, total rewards 2.0, mean loss 0.016414295884116168\n",
      "Episode 3805 finished after 207 timesteps, total rewards 4.0, mean loss 0.016159867759847988\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 3806 finished after 160 timesteps, total rewards 2.0, mean loss 0.020675193121860502\n",
      "Episode 3807 finished after 145 timesteps, total rewards 1.0, mean loss 0.01664078908506781\n",
      "Episode 3808 finished after 90 timesteps, total rewards 1.0, mean loss 0.020727147203352718\n",
      "Episode 3809 finished after 88 timesteps, total rewards 3.0, mean loss 0.02047246497452513\n",
      "Episode 3810 finished after 192 timesteps, total rewards 1.0, mean loss 0.019483380022090085\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 3811 finished after 171 timesteps, total rewards 3.0, mean loss 0.018700577233569445\n",
      "Episode 3812 finished after 161 timesteps, total rewards 1.0, mean loss 0.017441401329695575\n",
      "Episode 3813 finished after 168 timesteps, total rewards 2.0, mean loss 0.016911506323335095\n",
      "Episode 3814 finished after 212 timesteps, total rewards 5.0, mean loss 0.015956232015083033\n",
      "Episode 3815 finished after 183 timesteps, total rewards 4.0, mean loss 0.01860955431790198\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 3816 finished after 157 timesteps, total rewards 3.0, mean loss 0.017934244134237953\n",
      "Episode 3817 finished after 170 timesteps, total rewards 5.0, mean loss 0.019206107563256997\n",
      "Episode 3818 finished after 168 timesteps, total rewards 1.0, mean loss 0.01976146489531467\n",
      "Episode 3819 finished after 90 timesteps, total rewards 0.0, mean loss 0.019847965456493612\n",
      "Episode 3820 finished after 189 timesteps, total rewards 4.0, mean loss 0.019503812949917224\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 3821 finished after 199 timesteps, total rewards 4.0, mean loss 0.017311587774115216\n",
      "Episode 3822 finished after 129 timesteps, total rewards 0.0, mean loss 0.016217656468326413\n",
      "Episode 3823 finished after 222 timesteps, total rewards 6.0, mean loss 0.016901214796890347\n",
      "Episode 3824 finished after 234 timesteps, total rewards 4.0, mean loss 0.01507676523238516\n",
      "Episode 3825 finished after 175 timesteps, total rewards 0.0, mean loss 0.015641575874428132\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 3826 finished after 256 timesteps, total rewards 5.0, mean loss 0.01629484628301725\n",
      "Episode 3827 finished after 247 timesteps, total rewards 6.0, mean loss 0.0168483944200658\n",
      "Episode 3828 finished after 129 timesteps, total rewards 1.0, mean loss 0.017875095909685185\n",
      "Episode 3829 finished after 212 timesteps, total rewards 8.0, mean loss 0.017414842490084854\n",
      "Episode 3830 finished after 263 timesteps, total rewards 7.0, mean loss 0.016315668471203585\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 3831 finished after 141 timesteps, total rewards 2.0, mean loss 0.01915012572063067\n",
      "Episode 3832 finished after 339 timesteps, total rewards 1.0, mean loss 0.016939801487118378\n",
      "Episode 3833 finished after 175 timesteps, total rewards 3.0, mean loss 0.014166456339215594\n",
      "Episode 3834 finished after 287 timesteps, total rewards 5.0, mean loss 0.016722018727042976\n",
      "Episode 3835 finished after 94 timesteps, total rewards 1.0, mean loss 0.01780919010250611\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 85.000000\n",
      "---------------------------------------\n",
      "Episode 3836 finished after 249 timesteps, total rewards 5.0, mean loss 0.017256435419219625\n",
      "Episode 3837 finished after 142 timesteps, total rewards 1.0, mean loss 0.018545178508772974\n",
      "Episode 3838 finished after 295 timesteps, total rewards 4.0, mean loss 0.01772218544778051\n",
      "Episode 3839 finished after 91 timesteps, total rewards 2.0, mean loss 0.02013661374163988\n",
      "Episode 3840 finished after 128 timesteps, total rewards 2.0, mean loss 0.020049039293553506\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 3841 finished after 124 timesteps, total rewards 4.0, mean loss 0.015051122202778296\n",
      "Episode 3842 finished after 154 timesteps, total rewards 2.0, mean loss 0.017807487228298682\n",
      "Episode 3843 finished after 186 timesteps, total rewards 8.0, mean loss 0.017034804087660466\n",
      "Episode 3844 finished after 174 timesteps, total rewards 3.0, mean loss 0.01792235555924775\n",
      "Episode 3845 finished after 162 timesteps, total rewards 4.0, mean loss 0.019897873747668424\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 3846 finished after 175 timesteps, total rewards 3.0, mean loss 0.017585076933859713\n",
      "Episode 3847 finished after 137 timesteps, total rewards 1.0, mean loss 0.01872733723623746\n",
      "Episode 3848 finished after 318 timesteps, total rewards 8.0, mean loss 0.016458767366671713\n",
      "Episode 3849 finished after 213 timesteps, total rewards 5.0, mean loss 0.01576489033155633\n",
      "Episode 3850 finished after 87 timesteps, total rewards 1.0, mean loss 0.01959903044331733\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 3851 finished after 126 timesteps, total rewards 1.0, mean loss 0.017314721551162027\n",
      "Episode 3852 finished after 184 timesteps, total rewards 1.0, mean loss 0.01862532283469235\n",
      "Episode 3853 finished after 272 timesteps, total rewards 6.0, mean loss 0.01621972755588906\n",
      "Episode 3854 finished after 176 timesteps, total rewards 4.0, mean loss 0.016689811970123133\n",
      "Episode 3855 finished after 266 timesteps, total rewards 5.0, mean loss 0.016673923770308886\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3856 finished after 124 timesteps, total rewards 1.0, mean loss 0.019056686474340816\n",
      "Episode 3857 finished after 102 timesteps, total rewards 0.0, mean loss 0.017598978726549402\n",
      "Episode 3858 finished after 182 timesteps, total rewards 4.0, mean loss 0.017952859473343078\n",
      "Episode 3859 finished after 119 timesteps, total rewards 4.0, mean loss 0.017267574090510607\n",
      "Episode 3860 finished after 232 timesteps, total rewards 3.0, mean loss 0.016476557353222423\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 3861 finished after 127 timesteps, total rewards 2.0, mean loss 0.015362862086906208\n",
      "Episode 3862 finished after 92 timesteps, total rewards 2.0, mean loss 0.016432035681487912\n",
      "Episode 3863 finished after 129 timesteps, total rewards 5.0, mean loss 0.016277534813194196\n",
      "Episode 3864 finished after 163 timesteps, total rewards 2.0, mean loss 0.01607393766853364\n",
      "Episode 3865 finished after 95 timesteps, total rewards 2.0, mean loss 0.015378942855290676\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 3866 finished after 148 timesteps, total rewards 4.0, mean loss 0.0174348901192708\n",
      "Episode 3867 finished after 109 timesteps, total rewards 2.0, mean loss 0.02045256453843941\n",
      "Episode 3868 finished after 86 timesteps, total rewards 2.0, mean loss 0.02241250923915961\n",
      "Episode 3869 finished after 262 timesteps, total rewards 5.0, mean loss 0.01660148885886661\n",
      "Episode 3870 finished after 217 timesteps, total rewards 3.0, mean loss 0.016444768767894035\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3871 finished after 198 timesteps, total rewards 5.0, mean loss 0.018545199514865274\n",
      "Episode 3872 finished after 162 timesteps, total rewards 4.0, mean loss 0.014005710261968759\n",
      "Episode 3873 finished after 296 timesteps, total rewards 4.0, mean loss 0.017131412473375077\n",
      "Episode 3874 finished after 194 timesteps, total rewards 3.0, mean loss 0.01594417287965699\n",
      "Episode 3875 finished after 256 timesteps, total rewards 7.0, mean loss 0.01600180150808228\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3876 finished after 200 timesteps, total rewards 1.0, mean loss 0.01655394709086977\n",
      "Episode 3877 finished after 291 timesteps, total rewards 6.0, mean loss 0.017871909093950263\n",
      "Episode 3878 finished after 128 timesteps, total rewards 6.0, mean loss 0.017824981119701988\n",
      "Episode 3879 finished after 167 timesteps, total rewards 3.0, mean loss 0.017338182287803757\n",
      "Episode 3880 finished after 141 timesteps, total rewards 2.0, mean loss 0.017428739874868107\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 3881 finished after 168 timesteps, total rewards 2.0, mean loss 0.01687167708171598\n",
      "Episode 3882 finished after 169 timesteps, total rewards 0.0, mean loss 0.01717921858033463\n",
      "Episode 3883 finished after 155 timesteps, total rewards 1.0, mean loss 0.016907501050962077\n",
      "Episode 3884 finished after 214 timesteps, total rewards 5.0, mean loss 0.017346492430486794\n",
      "Episode 3885 finished after 173 timesteps, total rewards 7.0, mean loss 0.01752675559796996\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 3886 finished after 161 timesteps, total rewards 2.0, mean loss 0.016922648282438193\n",
      "Episode 3887 finished after 194 timesteps, total rewards 4.0, mean loss 0.019816293445362988\n",
      "Episode 3888 finished after 134 timesteps, total rewards 1.0, mean loss 0.01671160792379376\n",
      "Episode 3889 finished after 234 timesteps, total rewards 3.0, mean loss 0.017493610673297483\n",
      "Episode 3890 finished after 140 timesteps, total rewards 4.0, mean loss 0.01781768249680421\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 3891 finished after 218 timesteps, total rewards 3.0, mean loss 0.01854948030566848\n",
      "Episode 3892 finished after 95 timesteps, total rewards 2.0, mean loss 0.01596922436851616\n",
      "Episode 3893 finished after 101 timesteps, total rewards 0.0, mean loss 0.022834499927910763\n",
      "Episode 3894 finished after 199 timesteps, total rewards 5.0, mean loss 0.016823234603431395\n",
      "Episode 3895 finished after 85 timesteps, total rewards 3.0, mean loss 0.020105499718064333\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 3896 finished after 239 timesteps, total rewards 7.0, mean loss 0.01865550068160324\n",
      "Episode 3897 finished after 128 timesteps, total rewards 2.0, mean loss 0.01852516609324084\n",
      "Episode 3898 finished after 193 timesteps, total rewards 4.0, mean loss 0.018491527714665227\n",
      "Episode 3899 finished after 95 timesteps, total rewards 2.0, mean loss 0.016521845802076555\n",
      "Episode 3900 finished after 112 timesteps, total rewards 3.0, mean loss 0.018311340160185603\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 3901 finished after 173 timesteps, total rewards 3.0, mean loss 0.017163779891386757\n",
      "Episode 3902 finished after 287 timesteps, total rewards 4.0, mean loss 0.01903297324406911\n",
      "Episode 3903 finished after 194 timesteps, total rewards 6.0, mean loss 0.020979266217669722\n",
      "Episode 3904 finished after 203 timesteps, total rewards 1.0, mean loss 0.015233919730414568\n",
      "Episode 3905 finished after 218 timesteps, total rewards 4.0, mean loss 0.016534922985197684\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 3906 finished after 238 timesteps, total rewards 3.0, mean loss 0.018078644968326667\n",
      "Episode 3907 finished after 270 timesteps, total rewards 4.0, mean loss 0.016754958804921005\n",
      "Episode 3908 finished after 255 timesteps, total rewards 1.0, mean loss 0.01735988194396829\n",
      "Episode 3909 finished after 128 timesteps, total rewards 3.0, mean loss 0.017442753100112895\n",
      "Episode 3910 finished after 93 timesteps, total rewards 1.0, mean loss 0.01821254378044477\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 3911 finished after 124 timesteps, total rewards 2.0, mean loss 0.017856539254764006\n",
      "Episode 3912 finished after 165 timesteps, total rewards 1.0, mean loss 0.016877753581061507\n",
      "Episode 3913 finished after 121 timesteps, total rewards 0.0, mean loss 0.016880821298099747\n",
      "Episode 3914 finished after 128 timesteps, total rewards 3.0, mean loss 0.01979642992682784\n",
      "Episode 3915 finished after 136 timesteps, total rewards 3.0, mean loss 0.016105648246593773\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 3916 finished after 109 timesteps, total rewards 2.0, mean loss 0.018082981427208683\n",
      "Episode 3917 finished after 118 timesteps, total rewards 1.0, mean loss 0.017663015140480978\n",
      "Episode 3918 finished after 169 timesteps, total rewards 2.0, mean loss 0.01981105490245983\n",
      "Episode 3919 finished after 309 timesteps, total rewards 4.0, mean loss 0.016688067990077906\n",
      "Episode 3920 finished after 231 timesteps, total rewards 3.0, mean loss 0.018260094361859516\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 3921 finished after 194 timesteps, total rewards 6.0, mean loss 0.018051446924951003\n",
      "Episode 3922 finished after 117 timesteps, total rewards 0.0, mean loss 0.016764359585503038\n",
      "Episode 3923 finished after 239 timesteps, total rewards 6.0, mean loss 0.017547552791427994\n",
      "Episode 3924 finished after 167 timesteps, total rewards 1.0, mean loss 0.017792613421215447\n",
      "Episode 3925 finished after 233 timesteps, total rewards 6.0, mean loss 0.016007600824574465\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3926 finished after 190 timesteps, total rewards 3.0, mean loss 0.016834121704175087\n",
      "Episode 3927 finished after 98 timesteps, total rewards 1.0, mean loss 0.015302432513301621\n",
      "Episode 3928 finished after 211 timesteps, total rewards 5.0, mean loss 0.016132136212079234\n",
      "Episode 3929 finished after 324 timesteps, total rewards 5.0, mean loss 0.016308638196435677\n",
      "Episode 3930 finished after 249 timesteps, total rewards 4.0, mean loss 0.018402829566637587\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 3931 finished after 163 timesteps, total rewards 5.0, mean loss 0.01489371376432067\n",
      "Episode 3932 finished after 108 timesteps, total rewards 3.0, mean loss 0.01612476446057877\n",
      "Episode 3933 finished after 233 timesteps, total rewards 1.0, mean loss 0.01628135932437564\n",
      "Episode 3934 finished after 171 timesteps, total rewards 3.0, mean loss 0.016810569132689104\n",
      "Episode 3935 finished after 150 timesteps, total rewards 2.0, mean loss 0.015667146273578205\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 3936 finished after 174 timesteps, total rewards 0.0, mean loss 0.01514929496340625\n",
      "Episode 3937 finished after 138 timesteps, total rewards 2.0, mean loss 0.015940698703912938\n",
      "Episode 3938 finished after 160 timesteps, total rewards 4.0, mean loss 0.01784181428301963\n",
      "Episode 3939 finished after 136 timesteps, total rewards 5.0, mean loss 0.01587078253043514\n",
      "Episode 3940 finished after 224 timesteps, total rewards 5.0, mean loss 0.016966408559320762\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 3941 finished after 101 timesteps, total rewards 2.0, mean loss 0.01925784239250392\n",
      "Episode 3942 finished after 80 timesteps, total rewards 0.0, mean loss 0.01807207142410334\n",
      "Episode 3943 finished after 206 timesteps, total rewards 6.0, mean loss 0.016660502099644536\n",
      "Episode 3944 finished after 118 timesteps, total rewards 1.0, mean loss 0.01533609293342047\n",
      "Episode 3945 finished after 152 timesteps, total rewards 4.0, mean loss 0.01676012126661494\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 3946 finished after 184 timesteps, total rewards 2.0, mean loss 0.01701802163101409\n",
      "Episode 3947 finished after 117 timesteps, total rewards 4.0, mean loss 0.015636784685218435\n",
      "Episode 3948 finished after 302 timesteps, total rewards 3.0, mean loss 0.01675944102254221\n",
      "Episode 3949 finished after 113 timesteps, total rewards 0.0, mean loss 0.0150650024931235\n",
      "Episode 3950 finished after 350 timesteps, total rewards 6.0, mean loss 0.017648780588060617\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 3951 finished after 251 timesteps, total rewards 3.0, mean loss 0.017481527042638258\n",
      "Episode 3952 finished after 155 timesteps, total rewards 6.0, mean loss 0.014287464693188668\n",
      "Episode 3953 finished after 216 timesteps, total rewards 3.0, mean loss 0.020056319635909016\n",
      "Episode 3954 finished after 305 timesteps, total rewards 9.0, mean loss 0.017106507241069414\n",
      "Episode 3955 finished after 183 timesteps, total rewards 4.0, mean loss 0.015762210788607314\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 3956 finished after 232 timesteps, total rewards 2.0, mean loss 0.016813521061176916\n",
      "Episode 3957 finished after 117 timesteps, total rewards 2.0, mean loss 0.016895622850304198\n",
      "Episode 3958 finished after 291 timesteps, total rewards 4.0, mean loss 0.016000084056522174\n",
      "Episode 3959 finished after 267 timesteps, total rewards 3.0, mean loss 0.01686300359507639\n",
      "Episode 3960 finished after 235 timesteps, total rewards 1.0, mean loss 0.015414941205365702\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 118.333333\n",
      "---------------------------------------\n",
      "Episode 3961 finished after 225 timesteps, total rewards 4.0, mean loss 0.016563867994377184\n",
      "Episode 3962 finished after 95 timesteps, total rewards 2.0, mean loss 0.015958089575073435\n",
      "Episode 3963 finished after 133 timesteps, total rewards 4.0, mean loss 0.01754757009544655\n",
      "Episode 3964 finished after 377 timesteps, total rewards 10.0, mean loss 0.014785685643506798\n",
      "Episode 3965 finished after 235 timesteps, total rewards 7.0, mean loss 0.017466807534343542\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 3966 finished after 226 timesteps, total rewards 6.0, mean loss 0.018341914249946717\n",
      "Episode 3967 finished after 350 timesteps, total rewards 9.0, mean loss 0.016747865001338402\n",
      "Episode 3968 finished after 159 timesteps, total rewards 3.0, mean loss 0.016953666883362632\n",
      "Episode 3969 finished after 235 timesteps, total rewards 2.0, mean loss 0.01796047033425024\n",
      "Episode 3970 finished after 182 timesteps, total rewards 7.0, mean loss 0.016782038279036366\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 3971 finished after 110 timesteps, total rewards 3.0, mean loss 0.017677250736266036\n",
      "Episode 3972 finished after 174 timesteps, total rewards 3.0, mean loss 0.015764457270658266\n",
      "Episode 3973 finished after 233 timesteps, total rewards 5.0, mean loss 0.017804565706066467\n",
      "Episode 3974 finished after 227 timesteps, total rewards 6.0, mean loss 0.017851183793532132\n",
      "Episode 3975 finished after 204 timesteps, total rewards 4.0, mean loss 0.018138050494884487\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 81.666667\n",
      "---------------------------------------\n",
      "Episode 3976 finished after 153 timesteps, total rewards 3.0, mean loss 0.01704312560872899\n",
      "Episode 3977 finished after 160 timesteps, total rewards 3.0, mean loss 0.017285730440926274\n",
      "Episode 3978 finished after 176 timesteps, total rewards 1.0, mean loss 0.01862281551108357\n",
      "Episode 3979 finished after 114 timesteps, total rewards 0.0, mean loss 0.015646721563187607\n",
      "Episode 3980 finished after 194 timesteps, total rewards 7.0, mean loss 0.017901050241918487\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 3981 finished after 180 timesteps, total rewards 5.0, mean loss 0.019418618959995606\n",
      "Episode 3982 finished after 156 timesteps, total rewards 6.0, mean loss 0.016988996037234288\n",
      "Episode 3983 finished after 235 timesteps, total rewards 4.0, mean loss 0.016854186608465983\n",
      "Episode 3984 finished after 179 timesteps, total rewards 1.0, mean loss 0.01958991096461681\n",
      "Episode 3985 finished after 187 timesteps, total rewards 5.0, mean loss 0.015899362166432097\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 3986 finished after 163 timesteps, total rewards 2.0, mean loss 0.016505037212995757\n",
      "Episode 3987 finished after 264 timesteps, total rewards 4.0, mean loss 0.01638347321766725\n",
      "Episode 3988 finished after 88 timesteps, total rewards 0.0, mean loss 0.015435944992614995\n",
      "Episode 3989 finished after 234 timesteps, total rewards 4.0, mean loss 0.017704671131134525\n",
      "Episode 3990 finished after 90 timesteps, total rewards 1.0, mean loss 0.01671103611588478\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 3991 finished after 123 timesteps, total rewards 2.0, mean loss 0.016466656498179386\n",
      "Episode 3992 finished after 157 timesteps, total rewards 0.0, mean loss 0.01604883763998462\n",
      "Episode 3993 finished after 267 timesteps, total rewards 6.0, mean loss 0.01759479448507969\n",
      "Episode 3994 finished after 186 timesteps, total rewards 5.0, mean loss 0.015457092903889916\n",
      "Episode 3995 finished after 239 timesteps, total rewards 5.0, mean loss 0.01590612053502912\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3996 finished after 113 timesteps, total rewards 3.0, mean loss 0.017496784974870363\n",
      "Episode 3997 finished after 145 timesteps, total rewards 1.0, mean loss 0.01997401741622337\n",
      "Episode 3998 finished after 272 timesteps, total rewards 4.0, mean loss 0.017274857363886172\n",
      "Episode 3999 finished after 326 timesteps, total rewards 7.0, mean loss 0.018095942946190893\n",
      "Episode 4000 finished after 247 timesteps, total rewards 8.0, mean loss 0.017007864486644805\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 4001 finished after 312 timesteps, total rewards 7.0, mean loss 0.017258668735918876\n",
      "Episode 4002 finished after 161 timesteps, total rewards 1.0, mean loss 0.016210880303344694\n",
      "Episode 4003 finished after 116 timesteps, total rewards 2.0, mean loss 0.016388658570639533\n",
      "Episode 4004 finished after 284 timesteps, total rewards 4.0, mean loss 0.01665999580168126\n",
      "Episode 4005 finished after 97 timesteps, total rewards 0.0, mean loss 0.01671487814866813\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 4006 finished after 96 timesteps, total rewards 0.0, mean loss 0.016296643874132617\n",
      "Episode 4007 finished after 183 timesteps, total rewards 6.0, mean loss 0.018530661453330086\n",
      "Episode 4008 finished after 153 timesteps, total rewards 5.0, mean loss 0.014294434498593795\n",
      "Episode 4009 finished after 99 timesteps, total rewards 1.0, mean loss 0.016572581679381505\n",
      "Episode 4010 finished after 126 timesteps, total rewards 3.0, mean loss 0.017700913237474327\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 4011 finished after 197 timesteps, total rewards 3.0, mean loss 0.015018936650246041\n",
      "Episode 4012 finished after 129 timesteps, total rewards 3.0, mean loss 0.016208061114560034\n",
      "Episode 4013 finished after 246 timesteps, total rewards 5.0, mean loss 0.017996282953715966\n",
      "Episode 4014 finished after 125 timesteps, total rewards 2.0, mean loss 0.016623039783909915\n",
      "Episode 4015 finished after 169 timesteps, total rewards 2.0, mean loss 0.01788494118403908\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 103.333333\n",
      "---------------------------------------\n",
      "Episode 4016 finished after 226 timesteps, total rewards 11.0, mean loss 0.014942703995260253\n",
      "Episode 4017 finished after 309 timesteps, total rewards 6.0, mean loss 0.016015817136970926\n",
      "Episode 4018 finished after 111 timesteps, total rewards 0.0, mean loss 0.016668220171243423\n",
      "Episode 4019 finished after 192 timesteps, total rewards 4.0, mean loss 0.016906332885506952\n",
      "Episode 4020 finished after 256 timesteps, total rewards 8.0, mean loss 0.014330747464100568\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 4021 finished after 286 timesteps, total rewards 9.0, mean loss 0.017341785688910608\n",
      "Episode 4022 finished after 258 timesteps, total rewards 10.0, mean loss 0.01656170368064628\n",
      "Episode 4023 finished after 126 timesteps, total rewards 1.0, mean loss 0.01648765043189217\n",
      "Episode 4024 finished after 132 timesteps, total rewards 4.0, mean loss 0.01598984527553319\n",
      "Episode 4025 finished after 195 timesteps, total rewards 2.0, mean loss 0.017009482082600395\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 4026 finished after 236 timesteps, total rewards 8.0, mean loss 0.01542443573431475\n",
      "Episode 4027 finished after 274 timesteps, total rewards 2.0, mean loss 0.017367556549686886\n",
      "Episode 4028 finished after 87 timesteps, total rewards 1.0, mean loss 0.01658839390254526\n",
      "Episode 4029 finished after 131 timesteps, total rewards 0.0, mean loss 0.018794406439135037\n",
      "Episode 4030 finished after 141 timesteps, total rewards 1.0, mean loss 0.01904987074362101\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 4031 finished after 249 timesteps, total rewards 9.0, mean loss 0.01575672371924507\n",
      "Episode 4032 finished after 206 timesteps, total rewards 3.0, mean loss 0.01791315803838921\n",
      "Episode 4033 finished after 164 timesteps, total rewards 2.0, mean loss 0.01779287160450888\n",
      "Episode 4034 finished after 95 timesteps, total rewards 4.0, mean loss 0.017031974933649364\n",
      "Episode 4035 finished after 166 timesteps, total rewards 2.0, mean loss 0.016770075105484127\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 4036 finished after 132 timesteps, total rewards 1.0, mean loss 0.014514082908686814\n",
      "Episode 4037 finished after 194 timesteps, total rewards 6.0, mean loss 0.017916930094121274\n",
      "Episode 4038 finished after 179 timesteps, total rewards 3.0, mean loss 0.015716121933298112\n",
      "Episode 4039 finished after 203 timesteps, total rewards 4.0, mean loss 0.018944111833316755\n",
      "Episode 4040 finished after 176 timesteps, total rewards 6.0, mean loss 0.018767428787462202\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 4041 finished after 103 timesteps, total rewards 3.0, mean loss 0.016336333167115316\n",
      "Episode 4042 finished after 130 timesteps, total rewards 1.0, mean loss 0.018073867979602747\n",
      "Episode 4043 finished after 95 timesteps, total rewards 2.0, mean loss 0.014299973490108786\n",
      "Episode 4044 finished after 95 timesteps, total rewards 2.0, mean loss 0.016831104655897145\n",
      "Episode 4045 finished after 111 timesteps, total rewards 4.0, mean loss 0.014954914451106914\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 86.666667\n",
      "---------------------------------------\n",
      "Episode 4046 finished after 145 timesteps, total rewards 4.0, mean loss 0.017246948002741253\n",
      "Episode 4047 finished after 158 timesteps, total rewards 6.0, mean loss 0.017756952838589073\n",
      "Episode 4048 finished after 187 timesteps, total rewards 6.0, mean loss 0.016654063211654357\n",
      "Episode 4049 finished after 163 timesteps, total rewards 2.0, mean loss 0.017116766270147236\n",
      "Episode 4050 finished after 157 timesteps, total rewards 2.0, mean loss 0.015927195147118847\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4051 finished after 224 timesteps, total rewards 4.0, mean loss 0.016399583053498645\n",
      "Episode 4052 finished after 113 timesteps, total rewards 1.0, mean loss 0.01610971916883162\n",
      "Episode 4053 finished after 130 timesteps, total rewards 2.0, mean loss 0.014878173039939541\n",
      "Episode 4054 finished after 242 timesteps, total rewards 3.0, mean loss 0.01669628783908465\n",
      "Episode 4055 finished after 114 timesteps, total rewards 1.0, mean loss 0.018940249531480827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 4056 finished after 161 timesteps, total rewards 2.0, mean loss 0.016245051453422223\n",
      "Episode 4057 finished after 220 timesteps, total rewards 3.0, mean loss 0.014070371140471914\n",
      "Episode 4058 finished after 211 timesteps, total rewards 5.0, mean loss 0.014221763203000006\n",
      "Episode 4059 finished after 124 timesteps, total rewards 2.0, mean loss 0.018925114169589155\n",
      "Episode 4060 finished after 203 timesteps, total rewards 6.0, mean loss 0.015165101916854972\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4061 finished after 91 timesteps, total rewards 2.0, mean loss 0.01770825461090471\n",
      "Episode 4062 finished after 157 timesteps, total rewards 3.0, mean loss 0.014741952552543182\n",
      "Episode 4063 finished after 99 timesteps, total rewards 4.0, mean loss 0.017038509571887177\n",
      "Episode 4064 finished after 252 timesteps, total rewards 7.0, mean loss 0.015991677240457476\n",
      "Episode 4065 finished after 170 timesteps, total rewards 1.0, mean loss 0.017325636517831727\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4066 finished after 208 timesteps, total rewards 6.0, mean loss 0.01569468772280603\n",
      "Episode 4067 finished after 137 timesteps, total rewards 2.0, mean loss 0.01638777994883865\n",
      "Episode 4068 finished after 113 timesteps, total rewards 0.0, mean loss 0.014451278651244443\n",
      "Episode 4069 finished after 156 timesteps, total rewards 4.0, mean loss 0.016349840108514167\n",
      "Episode 4070 finished after 185 timesteps, total rewards 2.0, mean loss 0.01544378823160219\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 4071 finished after 128 timesteps, total rewards 2.0, mean loss 0.017692614459519973\n",
      "Episode 4072 finished after 168 timesteps, total rewards 4.0, mean loss 0.013503990687673823\n",
      "Episode 4073 finished after 119 timesteps, total rewards 3.0, mean loss 0.018741622519241097\n",
      "Episode 4074 finished after 193 timesteps, total rewards 2.0, mean loss 0.016252694964539162\n",
      "Episode 4075 finished after 187 timesteps, total rewards 2.0, mean loss 0.01791492682373659\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4076 finished after 235 timesteps, total rewards 6.0, mean loss 0.013824033861028704\n",
      "Episode 4077 finished after 192 timesteps, total rewards 5.0, mean loss 0.01602156245947602\n",
      "Episode 4078 finished after 231 timesteps, total rewards 1.0, mean loss 0.01633765763911576\n",
      "Episode 4079 finished after 120 timesteps, total rewards 3.0, mean loss 0.012315687187462268\n",
      "Episode 4080 finished after 147 timesteps, total rewards 5.0, mean loss 0.01642174052461019\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 4081 finished after 207 timesteps, total rewards 5.0, mean loss 0.01586792929545675\n",
      "Episode 4082 finished after 175 timesteps, total rewards 4.0, mean loss 0.015246312273666263\n",
      "Episode 4083 finished after 98 timesteps, total rewards 1.0, mean loss 0.017063738203284388\n",
      "Episode 4084 finished after 195 timesteps, total rewards 1.0, mean loss 0.014240244372437398\n",
      "Episode 4085 finished after 125 timesteps, total rewards 1.0, mean loss 0.013765095644630491\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 4086 finished after 246 timesteps, total rewards 6.0, mean loss 0.016601761129620387\n",
      "Episode 4087 finished after 240 timesteps, total rewards 6.0, mean loss 0.0159856541538223\n",
      "Episode 4088 finished after 144 timesteps, total rewards 1.0, mean loss 0.016884992671192676\n",
      "Episode 4089 finished after 207 timesteps, total rewards 7.0, mean loss 0.016557747526836676\n",
      "Episode 4090 finished after 250 timesteps, total rewards 4.0, mean loss 0.015328799461014569\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4091 finished after 129 timesteps, total rewards 2.0, mean loss 0.016318150115553147\n",
      "Episode 4092 finished after 291 timesteps, total rewards 6.0, mean loss 0.016511598255091645\n",
      "Episode 4093 finished after 208 timesteps, total rewards 6.0, mean loss 0.01797884313866514\n",
      "Episode 4094 finished after 159 timesteps, total rewards 2.0, mean loss 0.016075889541146655\n",
      "Episode 4095 finished after 204 timesteps, total rewards 2.0, mean loss 0.016802282728866545\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 4096 finished after 148 timesteps, total rewards 4.0, mean loss 0.017291765096062492\n",
      "Episode 4097 finished after 174 timesteps, total rewards 4.0, mean loss 0.016572510198042352\n",
      "Episode 4098 finished after 159 timesteps, total rewards 4.0, mean loss 0.01696998682004874\n",
      "Episode 4099 finished after 169 timesteps, total rewards 3.0, mean loss 0.016674733735185625\n",
      "Episode 4100 finished after 241 timesteps, total rewards 3.0, mean loss 0.015320070730367698\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 4101 finished after 159 timesteps, total rewards 3.0, mean loss 0.015206275477056514\n",
      "Episode 4102 finished after 207 timesteps, total rewards 4.0, mean loss 0.01792717184185316\n",
      "Episode 4103 finished after 333 timesteps, total rewards 11.0, mean loss 0.01629620427356658\n",
      "Episode 4104 finished after 277 timesteps, total rewards 7.0, mean loss 0.016995516389655452\n",
      "Episode 4105 finished after 108 timesteps, total rewards 1.0, mean loss 0.01760935632261896\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 146.666667\n",
      "---------------------------------------\n",
      "Episode 4106 finished after 309 timesteps, total rewards 7.0, mean loss 0.017543471382906763\n",
      "Episode 4107 finished after 190 timesteps, total rewards 5.0, mean loss 0.01570208152805112\n",
      "Episode 4108 finished after 134 timesteps, total rewards 2.0, mean loss 0.015062702683307953\n",
      "Episode 4109 finished after 237 timesteps, total rewards 3.0, mean loss 0.01629175807699121\n",
      "Episode 4110 finished after 199 timesteps, total rewards 6.0, mean loss 0.017315920132876902\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 4111 finished after 185 timesteps, total rewards 3.0, mean loss 0.018038737439751828\n",
      "Episode 4112 finished after 213 timesteps, total rewards 5.0, mean loss 0.014003806743123721\n",
      "Episode 4113 finished after 130 timesteps, total rewards 4.0, mean loss 0.01925379369685498\n",
      "Episode 4114 finished after 135 timesteps, total rewards 2.0, mean loss 0.01659284084604156\n",
      "Episode 4115 finished after 157 timesteps, total rewards 1.0, mean loss 0.01685979020719529\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4116 finished after 233 timesteps, total rewards 5.0, mean loss 0.01643129210777245\n",
      "Episode 4117 finished after 100 timesteps, total rewards 2.0, mean loss 0.017282601055921987\n",
      "Episode 4118 finished after 281 timesteps, total rewards 7.0, mean loss 0.017210718334214992\n",
      "Episode 4119 finished after 174 timesteps, total rewards 4.0, mean loss 0.01885045824997962\n",
      "Episode 4120 finished after 251 timesteps, total rewards 2.0, mean loss 0.016098082017346443\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 100.000000\n",
      "---------------------------------------\n",
      "Episode 4121 finished after 128 timesteps, total rewards 0.0, mean loss 0.015695134927227627\n",
      "Episode 4122 finished after 212 timesteps, total rewards 3.0, mean loss 0.016736151531387015\n",
      "Episode 4123 finished after 112 timesteps, total rewards 1.0, mean loss 0.01831404647756634\n",
      "Episode 4124 finished after 225 timesteps, total rewards 6.0, mean loss 0.016073989336792792\n",
      "Episode 4125 finished after 89 timesteps, total rewards 1.0, mean loss 0.014399373028793613\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 4126 finished after 240 timesteps, total rewards 5.0, mean loss 0.016696747781922264\n",
      "Episode 4127 finished after 102 timesteps, total rewards 0.0, mean loss 0.018842732310112492\n",
      "Episode 4128 finished after 141 timesteps, total rewards 3.0, mean loss 0.016335486938040836\n",
      "Episode 4129 finished after 134 timesteps, total rewards 4.0, mean loss 0.01774587156164196\n",
      "Episode 4130 finished after 295 timesteps, total rewards 5.0, mean loss 0.016681017149318703\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 4131 finished after 94 timesteps, total rewards 3.0, mean loss 0.016542433865090952\n",
      "Episode 4132 finished after 155 timesteps, total rewards 4.0, mean loss 0.0160539013190916\n",
      "Episode 4133 finished after 134 timesteps, total rewards 6.0, mean loss 0.016838092498529925\n",
      "Episode 4134 finished after 134 timesteps, total rewards 4.0, mean loss 0.014934052963421415\n",
      "Episode 4135 finished after 174 timesteps, total rewards 3.0, mean loss 0.01758325399413448\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4136 finished after 188 timesteps, total rewards 3.0, mean loss 0.016320285913063847\n",
      "Episode 4137 finished after 229 timesteps, total rewards 5.0, mean loss 0.01703657492726846\n",
      "Episode 4138 finished after 242 timesteps, total rewards 12.0, mean loss 0.016020082276937943\n",
      "Episode 4139 finished after 94 timesteps, total rewards 2.0, mean loss 0.0166509075442012\n",
      "Episode 4140 finished after 104 timesteps, total rewards 1.0, mean loss 0.020204335295532543\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4141 finished after 159 timesteps, total rewards 3.0, mean loss 0.015427775151658593\n",
      "Episode 4142 finished after 154 timesteps, total rewards 3.0, mean loss 0.015681582474294905\n",
      "Episode 4143 finished after 164 timesteps, total rewards 6.0, mean loss 0.01629065567976795\n",
      "Episode 4144 finished after 160 timesteps, total rewards 4.0, mean loss 0.016185307224805\n",
      "Episode 4145 finished after 151 timesteps, total rewards 1.0, mean loss 0.015854154703429352\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4146 finished after 100 timesteps, total rewards 2.0, mean loss 0.015877575115300715\n",
      "Episode 4147 finished after 95 timesteps, total rewards 2.0, mean loss 0.01487046190120868\n",
      "Episode 4148 finished after 182 timesteps, total rewards 5.0, mean loss 0.01813102403596801\n",
      "Episode 4149 finished after 303 timesteps, total rewards 11.0, mean loss 0.016301784427719738\n",
      "Episode 4150 finished after 130 timesteps, total rewards 1.0, mean loss 0.016211631906648667\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 4151 finished after 199 timesteps, total rewards 4.0, mean loss 0.017454897057634765\n",
      "Episode 4152 finished after 102 timesteps, total rewards 1.0, mean loss 0.018497083299592428\n",
      "Episode 4153 finished after 226 timesteps, total rewards 5.0, mean loss 0.0159195624940938\n",
      "Episode 4154 finished after 195 timesteps, total rewards 2.0, mean loss 0.01574091085543235\n",
      "Episode 4155 finished after 286 timesteps, total rewards 6.0, mean loss 0.016741297896495543\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4156 finished after 208 timesteps, total rewards 1.0, mean loss 0.015776855158597194\n",
      "Episode 4157 finished after 135 timesteps, total rewards 0.0, mean loss 0.015661570432388947\n",
      "Episode 4158 finished after 178 timesteps, total rewards 2.0, mean loss 0.018006530057984204\n",
      "Episode 4159 finished after 351 timesteps, total rewards 7.0, mean loss 0.01703257808571899\n",
      "Episode 4160 finished after 114 timesteps, total rewards 3.0, mean loss 0.017192208365406515\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 4161 finished after 93 timesteps, total rewards 2.0, mean loss 0.014950670535996637\n",
      "Episode 4162 finished after 166 timesteps, total rewards 4.0, mean loss 0.018204607330374867\n",
      "Episode 4163 finished after 111 timesteps, total rewards 2.0, mean loss 0.01915145617582508\n",
      "Episode 4164 finished after 181 timesteps, total rewards 6.0, mean loss 0.01731632943766819\n",
      "Episode 4165 finished after 283 timesteps, total rewards 4.0, mean loss 0.01677777204340054\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 4166 finished after 254 timesteps, total rewards 6.0, mean loss 0.017457675947771062\n",
      "Episode 4167 finished after 137 timesteps, total rewards 3.0, mean loss 0.016581688682976974\n",
      "Episode 4168 finished after 173 timesteps, total rewards 4.0, mean loss 0.01478541074482175\n",
      "Episode 4169 finished after 172 timesteps, total rewards 4.0, mean loss 0.015065788110680229\n",
      "Episode 4170 finished after 132 timesteps, total rewards 2.0, mean loss 0.01732393530299746\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4171 finished after 165 timesteps, total rewards 4.0, mean loss 0.01725474988307917\n",
      "Episode 4172 finished after 206 timesteps, total rewards 3.0, mean loss 0.01840660446583411\n",
      "Episode 4173 finished after 131 timesteps, total rewards 3.0, mean loss 0.018784757835065363\n",
      "Episode 4174 finished after 174 timesteps, total rewards 1.0, mean loss 0.01618213476425829\n",
      "Episode 4175 finished after 95 timesteps, total rewards 2.0, mean loss 0.016351042669511547\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 4176 finished after 268 timesteps, total rewards 8.0, mean loss 0.017979859583837382\n",
      "Episode 4177 finished after 107 timesteps, total rewards 3.0, mean loss 0.016196511847959293\n",
      "Episode 4178 finished after 203 timesteps, total rewards 3.0, mean loss 0.013790668267642005\n",
      "Episode 4179 finished after 132 timesteps, total rewards 1.0, mean loss 0.01690709767034136\n",
      "Episode 4180 finished after 173 timesteps, total rewards 3.0, mean loss 0.01587558051401674\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 4181 finished after 281 timesteps, total rewards 3.0, mean loss 0.01607313560772674\n",
      "Episode 4182 finished after 96 timesteps, total rewards 3.0, mean loss 0.017877550326375058\n",
      "Episode 4183 finished after 267 timesteps, total rewards 7.0, mean loss 0.01753824927812714\n",
      "Episode 4184 finished after 101 timesteps, total rewards 3.0, mean loss 0.016757561749142436\n",
      "Episode 4185 finished after 278 timesteps, total rewards 7.0, mean loss 0.01662423895971796\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 4186 finished after 216 timesteps, total rewards 3.0, mean loss 0.017273875787160015\n",
      "Episode 4187 finished after 236 timesteps, total rewards 7.0, mean loss 0.015509601279410486\n",
      "Episode 4188 finished after 103 timesteps, total rewards 1.0, mean loss 0.019789969939648093\n",
      "Episode 4189 finished after 172 timesteps, total rewards 4.0, mean loss 0.017441641628677243\n",
      "Episode 4190 finished after 85 timesteps, total rewards 1.0, mean loss 0.017390263657195166\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 4191 finished after 206 timesteps, total rewards 5.0, mean loss 0.014053180776491255\n",
      "Episode 4192 finished after 219 timesteps, total rewards 6.0, mean loss 0.017414363568380782\n",
      "Episode 4193 finished after 216 timesteps, total rewards 3.0, mean loss 0.017392241687587188\n",
      "Episode 4194 finished after 214 timesteps, total rewards 3.0, mean loss 0.013852589315447573\n",
      "Episode 4195 finished after 125 timesteps, total rewards 3.0, mean loss 0.01934153901319951\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4196 finished after 134 timesteps, total rewards 3.0, mean loss 0.015904365149118117\n",
      "Episode 4197 finished after 159 timesteps, total rewards 0.0, mean loss 0.017684329474643577\n",
      "Episode 4198 finished after 119 timesteps, total rewards 1.0, mean loss 0.017700096504261888\n",
      "Episode 4199 finished after 94 timesteps, total rewards 2.0, mean loss 0.015399408370672546\n",
      "Episode 4200 finished after 191 timesteps, total rewards 4.0, mean loss 0.017728439049492957\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4201 finished after 134 timesteps, total rewards 3.0, mean loss 0.01733120820752637\n",
      "Episode 4202 finished after 164 timesteps, total rewards 2.0, mean loss 0.016123169852795487\n",
      "Episode 4203 finished after 166 timesteps, total rewards 5.0, mean loss 0.01751331147928554\n",
      "Episode 4204 finished after 165 timesteps, total rewards 2.0, mean loss 0.016136436182017804\n",
      "Episode 4205 finished after 221 timesteps, total rewards 3.0, mean loss 0.01474411060561624\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 116.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4206 finished after 189 timesteps, total rewards 3.0, mean loss 0.01701886680965169\n",
      "Episode 4207 finished after 180 timesteps, total rewards 6.0, mean loss 0.015063584928349074\n",
      "Episode 4208 finished after 317 timesteps, total rewards 7.0, mean loss 0.015788719742940827\n",
      "Episode 4209 finished after 273 timesteps, total rewards 3.0, mean loss 0.017040156445469395\n",
      "Episode 4210 finished after 141 timesteps, total rewards 5.0, mean loss 0.017931749294545\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 4211 finished after 131 timesteps, total rewards 1.0, mean loss 0.014598019582837934\n",
      "Episode 4212 finished after 93 timesteps, total rewards 0.0, mean loss 0.013122072604845368\n",
      "Episode 4213 finished after 464 timesteps, total rewards 9.0, mean loss 0.015876046678197874\n",
      "Episode 4214 finished after 83 timesteps, total rewards 1.0, mean loss 0.01733459058464561\n",
      "Episode 4215 finished after 187 timesteps, total rewards 5.0, mean loss 0.018459028858855805\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 108.333333\n",
      "---------------------------------------\n",
      "Episode 4216 finished after 179 timesteps, total rewards 5.0, mean loss 0.017037700329693917\n",
      "Episode 4217 finished after 87 timesteps, total rewards 1.0, mean loss 0.01586462557449637\n",
      "Episode 4218 finished after 328 timesteps, total rewards 2.0, mean loss 0.01732158489601982\n",
      "Episode 4219 finished after 232 timesteps, total rewards 5.0, mean loss 0.017343897942517852\n",
      "Episode 4220 finished after 148 timesteps, total rewards 1.0, mean loss 0.01945429452304804\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4221 finished after 214 timesteps, total rewards 5.0, mean loss 0.015078996169428715\n",
      "Episode 4222 finished after 168 timesteps, total rewards 5.0, mean loss 0.017106080544181168\n",
      "Episode 4223 finished after 197 timesteps, total rewards 2.0, mean loss 0.018816787165088514\n",
      "Episode 4224 finished after 107 timesteps, total rewards 2.0, mean loss 0.017662747000413728\n",
      "Episode 4225 finished after 91 timesteps, total rewards 2.0, mean loss 0.017890488616303428\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 90.000000\n",
      "---------------------------------------\n",
      "Episode 4226 finished after 117 timesteps, total rewards 1.0, mean loss 0.016529709407184113\n",
      "Episode 4227 finished after 258 timesteps, total rewards 6.0, mean loss 0.0159745968313339\n",
      "Episode 4228 finished after 235 timesteps, total rewards 9.0, mean loss 0.01641429006716514\n",
      "Episode 4229 finished after 161 timesteps, total rewards 5.0, mean loss 0.014689891209380457\n",
      "Episode 4230 finished after 167 timesteps, total rewards 2.0, mean loss 0.016734697184703887\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 4231 finished after 237 timesteps, total rewards 8.0, mean loss 0.018213942254017176\n",
      "Episode 4232 finished after 142 timesteps, total rewards 0.0, mean loss 0.015153000164042476\n",
      "Episode 4233 finished after 323 timesteps, total rewards 7.0, mean loss 0.01584548939916164\n",
      "Episode 4234 finished after 178 timesteps, total rewards 2.0, mean loss 0.017931556555421585\n",
      "Episode 4235 finished after 99 timesteps, total rewards 0.0, mean loss 0.020407070813822852\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4236 finished after 296 timesteps, total rewards 8.0, mean loss 0.016209327248459984\n",
      "Episode 4237 finished after 194 timesteps, total rewards 5.0, mean loss 0.017129867040605005\n",
      "Episode 4238 finished after 117 timesteps, total rewards 3.0, mean loss 0.01662382086675264\n",
      "Episode 4239 finished after 193 timesteps, total rewards 4.0, mean loss 0.014677147463041267\n",
      "Episode 4240 finished after 167 timesteps, total rewards 2.0, mean loss 0.017255104339993867\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 4241 finished after 139 timesteps, total rewards 2.0, mean loss 0.018632845445569576\n",
      "Episode 4242 finished after 166 timesteps, total rewards 3.0, mean loss 0.01678988529041028\n",
      "Episode 4243 finished after 198 timesteps, total rewards 4.0, mean loss 0.01689182715184723\n",
      "Episode 4244 finished after 272 timesteps, total rewards 7.0, mean loss 0.017367787178527043\n",
      "Episode 4245 finished after 93 timesteps, total rewards 3.0, mean loss 0.018027448159221922\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4246 finished after 122 timesteps, total rewards 0.0, mean loss 0.018095692605368\n",
      "Episode 4247 finished after 84 timesteps, total rewards 1.0, mean loss 0.016479631877570813\n",
      "Episode 4248 finished after 188 timesteps, total rewards 2.0, mean loss 0.016910322844001287\n",
      "Episode 4249 finished after 96 timesteps, total rewards 0.0, mean loss 0.014504166386662595\n",
      "Episode 4250 finished after 173 timesteps, total rewards 2.0, mean loss 0.015145517677752398\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 4251 finished after 203 timesteps, total rewards 4.0, mean loss 0.016478004779327166\n",
      "Episode 4252 finished after 129 timesteps, total rewards 3.0, mean loss 0.017403412245704915\n",
      "Episode 4253 finished after 129 timesteps, total rewards 2.0, mean loss 0.015538059345576479\n",
      "Episode 4254 finished after 228 timesteps, total rewards 4.0, mean loss 0.018253645534410647\n",
      "Episode 4255 finished after 121 timesteps, total rewards 5.0, mean loss 0.015431277211447639\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 4256 finished after 117 timesteps, total rewards 2.0, mean loss 0.016202342466642268\n",
      "Episode 4257 finished after 139 timesteps, total rewards 2.0, mean loss 0.017925914733757754\n",
      "Episode 4258 finished after 117 timesteps, total rewards 4.0, mean loss 0.015532422359061673\n",
      "Episode 4259 finished after 183 timesteps, total rewards 3.0, mean loss 0.017594764804304705\n",
      "Episode 4260 finished after 140 timesteps, total rewards 3.0, mean loss 0.017974728852277622\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 4261 finished after 89 timesteps, total rewards 0.0, mean loss 0.016685900689113173\n",
      "Episode 4262 finished after 124 timesteps, total rewards 1.0, mean loss 0.019432011859363787\n",
      "Episode 4263 finished after 81 timesteps, total rewards 1.0, mean loss 0.018984903630306138\n",
      "Episode 4264 finished after 90 timesteps, total rewards 2.0, mean loss 0.014320537582453755\n",
      "Episode 4265 finished after 306 timesteps, total rewards 8.0, mean loss 0.017153745771650516\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 4266 finished after 89 timesteps, total rewards 3.0, mean loss 0.016409368285637223\n",
      "Episode 4267 finished after 257 timesteps, total rewards 7.0, mean loss 0.016210209702391627\n",
      "Episode 4268 finished after 219 timesteps, total rewards 4.0, mean loss 0.01680550661807684\n",
      "Episode 4269 finished after 300 timesteps, total rewards 9.0, mean loss 0.01696802552556619\n",
      "Episode 4270 finished after 312 timesteps, total rewards 5.0, mean loss 0.0159693647527512\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 4271 finished after 171 timesteps, total rewards 2.0, mean loss 0.016957624564873198\n",
      "Episode 4272 finished after 248 timesteps, total rewards 7.0, mean loss 0.01672837805310865\n",
      "Episode 4273 finished after 135 timesteps, total rewards 3.0, mean loss 0.01710057165387466\n",
      "Episode 4274 finished after 85 timesteps, total rewards 2.0, mean loss 0.020784922256408368\n",
      "Episode 4275 finished after 248 timesteps, total rewards 7.0, mean loss 0.01699725207647369\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4276 finished after 182 timesteps, total rewards 3.0, mean loss 0.017130694472983193\n",
      "Episode 4277 finished after 129 timesteps, total rewards 2.0, mean loss 0.015238258771980975\n",
      "Episode 4278 finished after 264 timesteps, total rewards 4.0, mean loss 0.015703584361207588\n",
      "Episode 4279 finished after 185 timesteps, total rewards 3.0, mean loss 0.014412215592440318\n",
      "Episode 4280 finished after 119 timesteps, total rewards 0.0, mean loss 0.019062697493173807\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 4281 finished after 259 timesteps, total rewards 10.0, mean loss 0.0177801660222494\n",
      "Episode 4282 finished after 225 timesteps, total rewards 3.0, mean loss 0.017368905811260143\n",
      "Episode 4283 finished after 166 timesteps, total rewards 3.0, mean loss 0.01586251585542348\n",
      "Episode 4284 finished after 238 timesteps, total rewards 4.0, mean loss 0.015619519552090714\n",
      "Episode 4285 finished after 104 timesteps, total rewards 4.0, mean loss 0.015618415474399136\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 85.000000\n",
      "---------------------------------------\n",
      "Episode 4286 finished after 197 timesteps, total rewards 2.0, mean loss 0.018129579353543368\n",
      "Episode 4287 finished after 92 timesteps, total rewards 1.0, mean loss 0.01550799237599637\n",
      "Episode 4288 finished after 96 timesteps, total rewards 0.0, mean loss 0.020104372655623592\n",
      "Episode 4289 finished after 181 timesteps, total rewards 7.0, mean loss 0.01626540299695376\n",
      "Episode 4290 finished after 195 timesteps, total rewards 8.0, mean loss 0.017482644885491866\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 4291 finished after 338 timesteps, total rewards 3.0, mean loss 0.015916420247394744\n",
      "Episode 4292 finished after 103 timesteps, total rewards 2.0, mean loss 0.01634250721754054\n",
      "Episode 4293 finished after 211 timesteps, total rewards 9.0, mean loss 0.01374730952988904\n",
      "Episode 4294 finished after 104 timesteps, total rewards 1.0, mean loss 0.018057516711101364\n",
      "Episode 4295 finished after 216 timesteps, total rewards 4.0, mean loss 0.018452768852175387\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 4296 finished after 99 timesteps, total rewards 2.0, mean loss 0.015958942689302594\n",
      "Episode 4297 finished after 140 timesteps, total rewards 2.0, mean loss 0.014827571353609008\n",
      "Episode 4298 finished after 96 timesteps, total rewards 1.0, mean loss 0.014248790087246258\n",
      "Episode 4299 finished after 163 timesteps, total rewards 2.0, mean loss 0.018191157017428815\n",
      "Episode 4300 finished after 200 timesteps, total rewards 3.0, mean loss 0.017773705201107076\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 4301 finished after 174 timesteps, total rewards 1.0, mean loss 0.0174763498964302\n",
      "Episode 4302 finished after 203 timesteps, total rewards 6.0, mean loss 0.018152588356763418\n",
      "Episode 4303 finished after 138 timesteps, total rewards 4.0, mean loss 0.016749750589108284\n",
      "Episode 4304 finished after 130 timesteps, total rewards 3.0, mean loss 0.01689249063627078\n",
      "Episode 4305 finished after 222 timesteps, total rewards 3.0, mean loss 0.01557940059563833\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 4306 finished after 176 timesteps, total rewards 2.0, mean loss 0.01676943219155708\n",
      "Episode 4307 finished after 190 timesteps, total rewards 3.0, mean loss 0.017245779918065588\n",
      "Episode 4308 finished after 219 timesteps, total rewards 4.0, mean loss 0.01679164566765178\n",
      "Episode 4309 finished after 127 timesteps, total rewards 3.0, mean loss 0.017315992142264473\n",
      "Episode 4310 finished after 168 timesteps, total rewards 2.0, mean loss 0.016517231720388822\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 4311 finished after 188 timesteps, total rewards 3.0, mean loss 0.01769735084653416\n",
      "Episode 4312 finished after 138 timesteps, total rewards 4.0, mean loss 0.015779553256267547\n",
      "Episode 4313 finished after 218 timesteps, total rewards 4.0, mean loss 0.017746097262448912\n",
      "Episode 4314 finished after 145 timesteps, total rewards 4.0, mean loss 0.01588417504641131\n",
      "Episode 4315 finished after 177 timesteps, total rewards 3.0, mean loss 0.017952859300414972\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 4316 finished after 92 timesteps, total rewards 2.0, mean loss 0.017747599906136242\n",
      "Episode 4317 finished after 153 timesteps, total rewards 1.0, mean loss 0.016140554143645455\n",
      "Episode 4318 finished after 101 timesteps, total rewards 3.0, mean loss 0.017431559026158314\n",
      "Episode 4319 finished after 293 timesteps, total rewards 5.0, mean loss 0.01553211270381757\n",
      "Episode 4320 finished after 129 timesteps, total rewards 1.0, mean loss 0.016674617101445563\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4321 finished after 138 timesteps, total rewards 3.0, mean loss 0.01601134024668431\n",
      "Episode 4322 finished after 252 timesteps, total rewards 8.0, mean loss 0.016266663312091537\n",
      "Episode 4323 finished after 115 timesteps, total rewards 0.0, mean loss 0.01586511056748745\n",
      "Episode 4324 finished after 194 timesteps, total rewards 3.0, mean loss 0.017527061105902624\n",
      "Episode 4325 finished after 167 timesteps, total rewards 3.0, mean loss 0.014912271150666663\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 4326 finished after 120 timesteps, total rewards 3.0, mean loss 0.015281555315596052\n",
      "Episode 4327 finished after 256 timesteps, total rewards 6.0, mean loss 0.016275367356229253\n",
      "Episode 4328 finished after 223 timesteps, total rewards 1.0, mean loss 0.018480374071133142\n",
      "Episode 4329 finished after 140 timesteps, total rewards 3.0, mean loss 0.01771834904710496\n",
      "Episode 4330 finished after 126 timesteps, total rewards 6.0, mean loss 0.01643794073006286\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4331 finished after 129 timesteps, total rewards 1.0, mean loss 0.01749035106906884\n",
      "Episode 4332 finished after 193 timesteps, total rewards 5.0, mean loss 0.017304560367423328\n",
      "Episode 4333 finished after 90 timesteps, total rewards 1.0, mean loss 0.01616293121025794\n",
      "Episode 4334 finished after 200 timesteps, total rewards 2.0, mean loss 0.017184594207210465\n",
      "Episode 4335 finished after 205 timesteps, total rewards 6.0, mean loss 0.016801549202092446\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 4336 finished after 199 timesteps, total rewards 4.0, mean loss 0.015780868956142915\n",
      "Episode 4337 finished after 166 timesteps, total rewards 1.0, mean loss 0.01826300701134872\n",
      "Episode 4338 finished after 127 timesteps, total rewards 2.0, mean loss 0.016798096032461195\n",
      "Episode 4339 finished after 257 timesteps, total rewards 4.0, mean loss 0.016448719204577404\n",
      "Episode 4340 finished after 148 timesteps, total rewards 4.0, mean loss 0.01772510036165398\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 4341 finished after 153 timesteps, total rewards 3.0, mean loss 0.016664213720812567\n",
      "Episode 4342 finished after 204 timesteps, total rewards 5.0, mean loss 0.017650964026745663\n",
      "Episode 4343 finished after 135 timesteps, total rewards 2.0, mean loss 0.01736579148305787\n",
      "Episode 4344 finished after 98 timesteps, total rewards 2.0, mean loss 0.018108314948574622\n",
      "Episode 4345 finished after 253 timesteps, total rewards 11.0, mean loss 0.01580167318033533\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4346 finished after 130 timesteps, total rewards 4.0, mean loss 0.014753632607439962\n",
      "Episode 4347 finished after 89 timesteps, total rewards 2.0, mean loss 0.016227133118212642\n",
      "Episode 4348 finished after 143 timesteps, total rewards 4.0, mean loss 0.01783795972921319\n",
      "Episode 4349 finished after 267 timesteps, total rewards 3.0, mean loss 0.01608000944319708\n",
      "Episode 4350 finished after 227 timesteps, total rewards 3.0, mean loss 0.01601285594597557\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4351 finished after 232 timesteps, total rewards 2.0, mean loss 0.014453391459718315\n",
      "Episode 4352 finished after 164 timesteps, total rewards 0.0, mean loss 0.015201989889979681\n",
      "Episode 4353 finished after 151 timesteps, total rewards 3.0, mean loss 0.01538754509074231\n",
      "Episode 4354 finished after 135 timesteps, total rewards 4.0, mean loss 0.01608673943577472\n",
      "Episode 4355 finished after 232 timesteps, total rewards 5.0, mean loss 0.015017320612216657\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 4356 finished after 219 timesteps, total rewards 5.0, mean loss 0.016890786783812253\n",
      "Episode 4357 finished after 150 timesteps, total rewards 5.0, mean loss 0.015289433527893076\n",
      "Episode 4358 finished after 138 timesteps, total rewards 1.0, mean loss 0.01694618220579392\n",
      "Episode 4359 finished after 191 timesteps, total rewards 7.0, mean loss 0.015349059252931459\n",
      "Episode 4360 finished after 104 timesteps, total rewards 1.0, mean loss 0.014946710272432448\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4361 finished after 301 timesteps, total rewards 4.0, mean loss 0.016826683984902314\n",
      "Episode 4362 finished after 101 timesteps, total rewards 3.0, mean loss 0.015804956467720763\n",
      "Episode 4363 finished after 219 timesteps, total rewards 3.0, mean loss 0.015763862702259837\n",
      "Episode 4364 finished after 165 timesteps, total rewards 3.0, mean loss 0.014471474193939658\n",
      "Episode 4365 finished after 154 timesteps, total rewards 5.0, mean loss 0.016501880728627568\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4366 finished after 126 timesteps, total rewards 3.0, mean loss 0.017658363703438745\n",
      "Episode 4367 finished after 168 timesteps, total rewards 3.0, mean loss 0.01721049187056321\n",
      "Episode 4368 finished after 245 timesteps, total rewards 7.0, mean loss 0.016396645438021086\n",
      "Episode 4369 finished after 219 timesteps, total rewards 6.0, mean loss 0.018185531219242133\n",
      "Episode 4370 finished after 186 timesteps, total rewards 4.0, mean loss 0.015592084929687522\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 4371 finished after 136 timesteps, total rewards 2.0, mean loss 0.016046620055396275\n",
      "Episode 4372 finished after 269 timesteps, total rewards 6.0, mean loss 0.015631928801931247\n",
      "Episode 4373 finished after 134 timesteps, total rewards 3.0, mean loss 0.01621686950423268\n",
      "Episode 4374 finished after 264 timesteps, total rewards 6.0, mean loss 0.01720473372596175\n",
      "Episode 4375 finished after 241 timesteps, total rewards 5.0, mean loss 0.016652877714496524\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 4376 finished after 181 timesteps, total rewards 5.0, mean loss 0.01803248244767381\n",
      "Episode 4377 finished after 147 timesteps, total rewards 4.0, mean loss 0.018586119925951723\n",
      "Episode 4378 finished after 121 timesteps, total rewards 1.0, mean loss 0.017355931894892197\n",
      "Episode 4379 finished after 197 timesteps, total rewards 4.0, mean loss 0.018876819301108228\n",
      "Episode 4380 finished after 87 timesteps, total rewards 2.0, mean loss 0.0185417374462189\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 4381 finished after 255 timesteps, total rewards 4.0, mean loss 0.016075336395799382\n",
      "Episode 4382 finished after 242 timesteps, total rewards 4.0, mean loss 0.01676385759853196\n",
      "Episode 4383 finished after 181 timesteps, total rewards 2.0, mean loss 0.01619484808903126\n",
      "Episode 4384 finished after 125 timesteps, total rewards 3.0, mean loss 0.01690576203353703\n",
      "Episode 4385 finished after 160 timesteps, total rewards 5.0, mean loss 0.018914882654644315\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4386 finished after 220 timesteps, total rewards 6.0, mean loss 0.016856190536319364\n",
      "Episode 4387 finished after 134 timesteps, total rewards 1.0, mean loss 0.017446011289217467\n",
      "Episode 4388 finished after 117 timesteps, total rewards 2.0, mean loss 0.014716348356097681\n",
      "Episode 4389 finished after 87 timesteps, total rewards 1.0, mean loss 0.016686880491384917\n",
      "Episode 4390 finished after 226 timesteps, total rewards 2.0, mean loss 0.016721430592613905\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 4391 finished after 91 timesteps, total rewards 1.0, mean loss 0.015030758890354044\n",
      "Episode 4392 finished after 396 timesteps, total rewards 11.0, mean loss 0.018650229352072466\n",
      "Episode 4393 finished after 166 timesteps, total rewards 3.0, mean loss 0.01585562587070492\n",
      "Episode 4394 finished after 112 timesteps, total rewards 2.0, mean loss 0.019444716180649784\n",
      "Episode 4395 finished after 146 timesteps, total rewards 3.0, mean loss 0.015040546642897064\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 4396 finished after 163 timesteps, total rewards 2.0, mean loss 0.018035725788984654\n",
      "Episode 4397 finished after 159 timesteps, total rewards 4.0, mean loss 0.01681277728507752\n",
      "Episode 4398 finished after 320 timesteps, total rewards 8.0, mean loss 0.015349435183452442\n",
      "Episode 4399 finished after 92 timesteps, total rewards 0.0, mean loss 0.01434311745865473\n",
      "Episode 4400 finished after 124 timesteps, total rewards 0.0, mean loss 0.01867349454926537\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 4401 finished after 98 timesteps, total rewards 3.0, mean loss 0.014295541397438442\n",
      "Episode 4402 finished after 218 timesteps, total rewards 4.0, mean loss 0.01554266143421463\n",
      "Episode 4403 finished after 200 timesteps, total rewards 6.0, mean loss 0.017043436815729365\n",
      "Episode 4404 finished after 294 timesteps, total rewards 8.0, mean loss 0.016918250658948506\n",
      "Episode 4405 finished after 250 timesteps, total rewards 10.0, mean loss 0.017958221812732517\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 4406 finished after 256 timesteps, total rewards 8.0, mean loss 0.018242513218865497\n",
      "Episode 4407 finished after 181 timesteps, total rewards 3.0, mean loss 0.019164908288216518\n",
      "Episode 4408 finished after 161 timesteps, total rewards 3.0, mean loss 0.018984429099342514\n",
      "Episode 4409 finished after 179 timesteps, total rewards 8.0, mean loss 0.018764309393887162\n",
      "Episode 4410 finished after 122 timesteps, total rewards 1.0, mean loss 0.016884084736554287\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 4411 finished after 151 timesteps, total rewards 3.0, mean loss 0.015329496519461226\n",
      "Episode 4412 finished after 149 timesteps, total rewards 1.0, mean loss 0.01677072300081885\n",
      "Episode 4413 finished after 136 timesteps, total rewards 4.0, mean loss 0.01874439119488028\n",
      "Episode 4414 finished after 328 timesteps, total rewards 5.0, mean loss 0.016466591012076384\n",
      "Episode 4415 finished after 170 timesteps, total rewards 3.0, mean loss 0.01523038091014742\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4416 finished after 135 timesteps, total rewards 0.0, mean loss 0.01557652721595433\n",
      "Episode 4417 finished after 164 timesteps, total rewards 4.0, mean loss 0.01686875711737496\n",
      "Episode 4418 finished after 133 timesteps, total rewards 2.0, mean loss 0.017678050614921448\n",
      "Episode 4419 finished after 155 timesteps, total rewards 2.0, mean loss 0.01485087510438696\n",
      "Episode 4420 finished after 92 timesteps, total rewards 2.0, mean loss 0.016816355847328414\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 4421 finished after 247 timesteps, total rewards 6.0, mean loss 0.01778190992188146\n",
      "Episode 4422 finished after 111 timesteps, total rewards 1.0, mean loss 0.017063063409951357\n",
      "Episode 4423 finished after 127 timesteps, total rewards 2.0, mean loss 0.016309445558893163\n",
      "Episode 4424 finished after 100 timesteps, total rewards 0.0, mean loss 0.017164294561371207\n",
      "Episode 4425 finished after 138 timesteps, total rewards 3.0, mean loss 0.016626558848656714\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 4426 finished after 177 timesteps, total rewards 3.0, mean loss 0.015293987630340298\n",
      "Episode 4427 finished after 162 timesteps, total rewards 5.0, mean loss 0.015105807061456604\n",
      "Episode 4428 finished after 170 timesteps, total rewards 3.0, mean loss 0.01838822051855352\n",
      "Episode 4429 finished after 233 timesteps, total rewards 4.0, mean loss 0.017774066955246862\n",
      "Episode 4430 finished after 93 timesteps, total rewards 2.0, mean loss 0.016822786311999523\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 4431 finished after 248 timesteps, total rewards 7.0, mean loss 0.015380953841339496\n",
      "Episode 4432 finished after 238 timesteps, total rewards 2.0, mean loss 0.018245224869309164\n",
      "Episode 4433 finished after 154 timesteps, total rewards 3.0, mean loss 0.015449085878359468\n",
      "Episode 4434 finished after 204 timesteps, total rewards 4.0, mean loss 0.01733435661127955\n",
      "Episode 4435 finished after 101 timesteps, total rewards 3.0, mean loss 0.015594624673906198\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 105.000000\n",
      "---------------------------------------\n",
      "Episode 4436 finished after 97 timesteps, total rewards 2.0, mean loss 0.01701182897980373\n",
      "Episode 4437 finished after 167 timesteps, total rewards 3.0, mean loss 0.01758009359285146\n",
      "Episode 4438 finished after 109 timesteps, total rewards 1.0, mean loss 0.015599755230658782\n",
      "Episode 4439 finished after 314 timesteps, total rewards 6.0, mean loss 0.016537192289103535\n",
      "Episode 4440 finished after 163 timesteps, total rewards 3.0, mean loss 0.018501190050145142\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 4441 finished after 128 timesteps, total rewards 3.0, mean loss 0.015801192225808336\n",
      "Episode 4442 finished after 198 timesteps, total rewards 5.0, mean loss 0.016569261824843858\n",
      "Episode 4443 finished after 237 timesteps, total rewards 4.0, mean loss 0.015431282690132846\n",
      "Episode 4444 finished after 95 timesteps, total rewards 1.0, mean loss 0.014913469404717418\n",
      "Episode 4445 finished after 85 timesteps, total rewards 1.0, mean loss 0.01693662663729971\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4446 finished after 161 timesteps, total rewards 3.0, mean loss 0.01785952233389795\n",
      "Episode 4447 finished after 184 timesteps, total rewards 3.0, mean loss 0.016034121360027475\n",
      "Episode 4448 finished after 263 timesteps, total rewards 5.0, mean loss 0.016503127155464297\n",
      "Episode 4449 finished after 316 timesteps, total rewards 4.0, mean loss 0.014420170445640607\n",
      "Episode 4450 finished after 124 timesteps, total rewards 2.0, mean loss 0.015157594941669114\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 4451 finished after 129 timesteps, total rewards 4.0, mean loss 0.01631389707940551\n",
      "Episode 4452 finished after 178 timesteps, total rewards 2.0, mean loss 0.014861744793205197\n",
      "Episode 4453 finished after 129 timesteps, total rewards 1.0, mean loss 0.014731075615593225\n",
      "Episode 4454 finished after 134 timesteps, total rewards 2.0, mean loss 0.015453601918016462\n",
      "Episode 4455 finished after 224 timesteps, total rewards 6.0, mean loss 0.017583432464432138\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 4456 finished after 90 timesteps, total rewards 2.0, mean loss 0.020512969634081755\n",
      "Episode 4457 finished after 163 timesteps, total rewards 7.0, mean loss 0.019650962076612686\n",
      "Episode 4458 finished after 229 timesteps, total rewards 5.0, mean loss 0.018467832613415103\n",
      "Episode 4459 finished after 222 timesteps, total rewards 2.0, mean loss 0.017298418691335665\n",
      "Episode 4460 finished after 221 timesteps, total rewards 8.0, mean loss 0.01939708631532056\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 4461 finished after 254 timesteps, total rewards 2.0, mean loss 0.01852684707965658\n",
      "Episode 4462 finished after 240 timesteps, total rewards 3.0, mean loss 0.017569366230842813\n",
      "Episode 4463 finished after 191 timesteps, total rewards 5.0, mean loss 0.018001195899069466\n",
      "Episode 4464 finished after 297 timesteps, total rewards 7.0, mean loss 0.01824238889392854\n",
      "Episode 4465 finished after 128 timesteps, total rewards 2.0, mean loss 0.020174585548375035\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 4466 finished after 153 timesteps, total rewards 0.0, mean loss 0.015928153746024847\n",
      "Episode 4467 finished after 134 timesteps, total rewards 1.0, mean loss 0.01991569139619372\n",
      "Episode 4468 finished after 303 timesteps, total rewards 3.0, mean loss 0.02118890469266784\n",
      "Episode 4469 finished after 198 timesteps, total rewards 3.0, mean loss 0.018753388258652064\n",
      "Episode 4470 finished after 247 timesteps, total rewards 6.0, mean loss 0.018196491338878205\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 4471 finished after 316 timesteps, total rewards 10.0, mean loss 0.019061210670263826\n",
      "Episode 4472 finished after 223 timesteps, total rewards 6.0, mean loss 0.01790721392396821\n",
      "Episode 4473 finished after 138 timesteps, total rewards 8.0, mean loss 0.019681378251677244\n",
      "Episode 4474 finished after 123 timesteps, total rewards 2.0, mean loss 0.016245243334536995\n",
      "Episode 4475 finished after 104 timesteps, total rewards 2.0, mean loss 0.016824504618013564\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 155.000000\n",
      "---------------------------------------\n",
      "Episode 4476 finished after 139 timesteps, total rewards 1.0, mean loss 0.017875359505046453\n",
      "Episode 4477 finished after 197 timesteps, total rewards 3.0, mean loss 0.019403721030971724\n",
      "Episode 4478 finished after 146 timesteps, total rewards 3.0, mean loss 0.018227259850142244\n",
      "Episode 4479 finished after 261 timesteps, total rewards 5.0, mean loss 0.019165672365419023\n",
      "Episode 4480 finished after 203 timesteps, total rewards 3.0, mean loss 0.017476969219060622\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 4481 finished after 197 timesteps, total rewards 4.0, mean loss 0.016328795247493776\n",
      "Episode 4482 finished after 96 timesteps, total rewards 0.0, mean loss 0.01642932984395884\n",
      "Episode 4483 finished after 196 timesteps, total rewards 5.0, mean loss 0.017318289968118603\n",
      "Episode 4484 finished after 131 timesteps, total rewards 1.0, mean loss 0.015339605467578838\n",
      "Episode 4485 finished after 99 timesteps, total rewards 2.0, mean loss 0.01880857613727902\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4486 finished after 155 timesteps, total rewards 4.0, mean loss 0.017774892110197293\n",
      "Episode 4487 finished after 185 timesteps, total rewards 3.0, mean loss 0.016977185702479972\n",
      "Episode 4488 finished after 127 timesteps, total rewards 5.0, mean loss 0.01842935962096502\n",
      "Episode 4489 finished after 270 timesteps, total rewards 4.0, mean loss 0.0167935210995859\n",
      "Episode 4490 finished after 223 timesteps, total rewards 3.0, mean loss 0.0184469329075253\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4491 finished after 119 timesteps, total rewards 5.0, mean loss 0.015457824649320556\n",
      "Episode 4492 finished after 261 timesteps, total rewards 6.0, mean loss 0.018081989896241078\n",
      "Episode 4493 finished after 178 timesteps, total rewards 6.0, mean loss 0.01773765900308329\n",
      "Episode 4494 finished after 260 timesteps, total rewards 4.0, mean loss 0.021241419981323326\n",
      "Episode 4495 finished after 330 timesteps, total rewards 8.0, mean loss 0.016556670408075055\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 4496 finished after 156 timesteps, total rewards 1.0, mean loss 0.018907581148549724\n",
      "Episode 4497 finished after 142 timesteps, total rewards 4.0, mean loss 0.0180630551939699\n",
      "Episode 4498 finished after 183 timesteps, total rewards 5.0, mean loss 0.01899247319256897\n",
      "Episode 4499 finished after 118 timesteps, total rewards 1.0, mean loss 0.01858230942457725\n",
      "Episode 4500 finished after 213 timesteps, total rewards 3.0, mean loss 0.01645003318592049\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4501 finished after 169 timesteps, total rewards 1.0, mean loss 0.016786399012233114\n",
      "Episode 4502 finished after 193 timesteps, total rewards 7.0, mean loss 0.016911924407706436\n",
      "Episode 4503 finished after 135 timesteps, total rewards 7.0, mean loss 0.018305121703694263\n",
      "Episode 4504 finished after 131 timesteps, total rewards 4.0, mean loss 0.019350769383057895\n",
      "Episode 4505 finished after 163 timesteps, total rewards 2.0, mean loss 0.017320170007257555\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4506 finished after 214 timesteps, total rewards 7.0, mean loss 0.016551373912691722\n",
      "Episode 4507 finished after 141 timesteps, total rewards 3.0, mean loss 0.01820182289716834\n",
      "Episode 4508 finished after 227 timesteps, total rewards 2.0, mean loss 0.01764585381262989\n",
      "Episode 4509 finished after 194 timesteps, total rewards 4.0, mean loss 0.018911728029350566\n",
      "Episode 4510 finished after 205 timesteps, total rewards 6.0, mean loss 0.0175452067437241\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 91.666667\n",
      "---------------------------------------\n",
      "Episode 4511 finished after 110 timesteps, total rewards 2.0, mean loss 0.01997663348976692\n",
      "Episode 4512 finished after 212 timesteps, total rewards 3.0, mean loss 0.018203393541900266\n",
      "Episode 4513 finished after 184 timesteps, total rewards 2.0, mean loss 0.018830135306495282\n",
      "Episode 4514 finished after 209 timesteps, total rewards 1.0, mean loss 0.01695162367686166\n",
      "Episode 4515 finished after 166 timesteps, total rewards 1.0, mean loss 0.015327502612209687\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4516 finished after 254 timesteps, total rewards 6.0, mean loss 0.01694162414953714\n",
      "Episode 4517 finished after 173 timesteps, total rewards 4.0, mean loss 0.017106503932575314\n",
      "Episode 4518 finished after 100 timesteps, total rewards 4.0, mean loss 0.016586074639344587\n",
      "Episode 4519 finished after 95 timesteps, total rewards 3.0, mean loss 0.017829636768683006\n",
      "Episode 4520 finished after 158 timesteps, total rewards 4.0, mean loss 0.01541046988988248\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 4521 finished after 107 timesteps, total rewards 2.0, mean loss 0.020568891643833753\n",
      "Episode 4522 finished after 223 timesteps, total rewards 3.0, mean loss 0.01715601806063205\n",
      "Episode 4523 finished after 203 timesteps, total rewards 4.0, mean loss 0.017543586784758113\n",
      "Episode 4524 finished after 178 timesteps, total rewards 5.0, mean loss 0.018639384797026058\n",
      "Episode 4525 finished after 105 timesteps, total rewards 4.0, mean loss 0.01850194525239723\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 4526 finished after 233 timesteps, total rewards 5.0, mean loss 0.017527382511069976\n",
      "Episode 4527 finished after 84 timesteps, total rewards 0.0, mean loss 0.019126792028102847\n",
      "Episode 4528 finished after 229 timesteps, total rewards 6.0, mean loss 0.01719952340839699\n",
      "Episode 4529 finished after 162 timesteps, total rewards 2.0, mean loss 0.01826756471770896\n",
      "Episode 4530 finished after 216 timesteps, total rewards 6.0, mean loss 0.0186625191973117\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.666667\n",
      "---------------------------------------\n",
      "Episode 4531 finished after 123 timesteps, total rewards 2.0, mean loss 0.017623631097187054\n",
      "Episode 4532 finished after 177 timesteps, total rewards 2.0, mean loss 0.016792382576665973\n",
      "Episode 4533 finished after 162 timesteps, total rewards 5.0, mean loss 0.016972019099256736\n",
      "Episode 4534 finished after 245 timesteps, total rewards 9.0, mean loss 0.01731339086094225\n",
      "Episode 4535 finished after 215 timesteps, total rewards 3.0, mean loss 0.01848983114834355\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 4536 finished after 158 timesteps, total rewards 1.0, mean loss 0.018660839132027394\n",
      "Episode 4537 finished after 185 timesteps, total rewards 6.0, mean loss 0.0177179588155972\n",
      "Episode 4538 finished after 80 timesteps, total rewards 0.0, mean loss 0.01686192584456876\n",
      "Episode 4539 finished after 357 timesteps, total rewards 5.0, mean loss 0.0162034091263806\n",
      "Episode 4540 finished after 98 timesteps, total rewards 1.0, mean loss 0.019473909302994762\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4541 finished after 374 timesteps, total rewards 5.0, mean loss 0.017147718590714116\n",
      "Episode 4542 finished after 158 timesteps, total rewards 4.0, mean loss 0.018685598820860534\n",
      "Episode 4543 finished after 233 timesteps, total rewards 4.0, mean loss 0.015713732617671398\n",
      "Episode 4544 finished after 226 timesteps, total rewards 2.0, mean loss 0.018214620831649157\n",
      "Episode 4545 finished after 124 timesteps, total rewards 2.0, mean loss 0.017778376872170595\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 4546 finished after 254 timesteps, total rewards 4.0, mean loss 0.019254778025077846\n",
      "Episode 4547 finished after 134 timesteps, total rewards 1.0, mean loss 0.016723918580839325\n",
      "Episode 4548 finished after 131 timesteps, total rewards 1.0, mean loss 0.018655786136136825\n",
      "Episode 4549 finished after 296 timesteps, total rewards 3.0, mean loss 0.017358031572626175\n",
      "Episode 4550 finished after 226 timesteps, total rewards 7.0, mean loss 0.016963331945718522\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 4551 finished after 106 timesteps, total rewards 0.0, mean loss 0.016384537204362032\n",
      "Episode 4552 finished after 95 timesteps, total rewards 0.0, mean loss 0.01699623382454248\n",
      "Episode 4553 finished after 131 timesteps, total rewards 5.0, mean loss 0.020278160490507957\n",
      "Episode 4554 finished after 103 timesteps, total rewards 0.0, mean loss 0.014739460598912488\n",
      "Episode 4555 finished after 209 timesteps, total rewards 7.0, mean loss 0.017505200603156926\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4556 finished after 240 timesteps, total rewards 3.0, mean loss 0.01813238929656412\n",
      "Episode 4557 finished after 93 timesteps, total rewards 2.0, mean loss 0.016920350051374846\n",
      "Episode 4558 finished after 235 timesteps, total rewards 2.0, mean loss 0.016757633437977194\n",
      "Episode 4559 finished after 163 timesteps, total rewards 2.0, mean loss 0.017028684340748775\n",
      "Episode 4560 finished after 224 timesteps, total rewards 5.0, mean loss 0.016909166569218672\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 13.333333\n",
      "---------------------------------------\n",
      "Episode 4561 finished after 103 timesteps, total rewards 1.0, mean loss 0.018942037604080097\n",
      "Episode 4562 finished after 161 timesteps, total rewards 5.0, mean loss 0.017870700699768913\n",
      "Episode 4563 finished after 184 timesteps, total rewards 7.0, mean loss 0.017991338909031703\n",
      "Episode 4564 finished after 317 timesteps, total rewards 6.0, mean loss 0.016371287936153778\n",
      "Episode 4565 finished after 156 timesteps, total rewards 3.0, mean loss 0.01592620689263687\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 4566 finished after 136 timesteps, total rewards 0.0, mean loss 0.01629808815778233\n",
      "Episode 4567 finished after 94 timesteps, total rewards 2.0, mean loss 0.019439227492647603\n",
      "Episode 4568 finished after 123 timesteps, total rewards 1.0, mean loss 0.01674956850124508\n",
      "Episode 4569 finished after 278 timesteps, total rewards 12.0, mean loss 0.015510683368368866\n",
      "Episode 4570 finished after 156 timesteps, total rewards 2.0, mean loss 0.019263070766240932\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 4571 finished after 423 timesteps, total rewards 6.0, mean loss 0.01635844684585744\n",
      "Episode 4572 finished after 156 timesteps, total rewards 1.0, mean loss 0.015924645075126767\n",
      "Episode 4573 finished after 187 timesteps, total rewards 3.0, mean loss 0.017273994008288347\n",
      "Episode 4574 finished after 324 timesteps, total rewards 7.0, mean loss 0.017366836341184185\n",
      "Episode 4575 finished after 271 timesteps, total rewards 4.0, mean loss 0.016554983380865786\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 4576 finished after 163 timesteps, total rewards 2.0, mean loss 0.016896228975205013\n",
      "Episode 4577 finished after 113 timesteps, total rewards 1.0, mean loss 0.015457457813343643\n",
      "Episode 4578 finished after 95 timesteps, total rewards 3.0, mean loss 0.016195309507709584\n",
      "Episode 4579 finished after 158 timesteps, total rewards 5.0, mean loss 0.018484256018094624\n",
      "Episode 4580 finished after 132 timesteps, total rewards 3.0, mean loss 0.017867014796303756\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 4581 finished after 245 timesteps, total rewards 8.0, mean loss 0.016382077001320314\n",
      "Episode 4582 finished after 163 timesteps, total rewards 5.0, mean loss 0.015816625848303956\n",
      "Episode 4583 finished after 130 timesteps, total rewards 3.0, mean loss 0.018614567624619948\n",
      "Episode 4584 finished after 186 timesteps, total rewards 5.0, mean loss 0.01657408527323916\n",
      "Episode 4585 finished after 90 timesteps, total rewards 2.0, mean loss 0.01909802425911443\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 91.666667\n",
      "---------------------------------------\n",
      "Episode 4586 finished after 152 timesteps, total rewards 1.0, mean loss 0.01909545561341618\n",
      "Episode 4587 finished after 84 timesteps, total rewards 0.0, mean loss 0.017558020791814972\n",
      "Episode 4588 finished after 287 timesteps, total rewards 7.0, mean loss 0.017049503988888152\n",
      "Episode 4589 finished after 183 timesteps, total rewards 4.0, mean loss 0.016221777363578518\n",
      "Episode 4590 finished after 287 timesteps, total rewards 4.0, mean loss 0.0169769824999271\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 4591 finished after 228 timesteps, total rewards 6.0, mean loss 0.016971411131906502\n",
      "Episode 4592 finished after 139 timesteps, total rewards 2.0, mean loss 0.02111322969606937\n",
      "Episode 4593 finished after 225 timesteps, total rewards 4.0, mean loss 0.017526680300943554\n",
      "Episode 4594 finished after 197 timesteps, total rewards 3.0, mean loss 0.01787543622658678\n",
      "Episode 4595 finished after 288 timesteps, total rewards 4.0, mean loss 0.0160633622722849\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 4596 finished after 108 timesteps, total rewards 2.0, mean loss 0.018313463557408087\n",
      "Episode 4597 finished after 273 timesteps, total rewards 7.0, mean loss 0.016509196547886882\n",
      "Episode 4598 finished after 325 timesteps, total rewards 4.0, mean loss 0.016918325122589103\n",
      "Episode 4599 finished after 193 timesteps, total rewards 3.0, mean loss 0.01744429660005582\n",
      "Episode 4600 finished after 127 timesteps, total rewards 3.0, mean loss 0.01719583714688857\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 4601 finished after 204 timesteps, total rewards 3.0, mean loss 0.017252361597082412\n",
      "Episode 4602 finished after 341 timesteps, total rewards 6.0, mean loss 0.016824222860456366\n",
      "Episode 4603 finished after 133 timesteps, total rewards 3.0, mean loss 0.01808477831213154\n",
      "Episode 4604 finished after 206 timesteps, total rewards 5.0, mean loss 0.01567428340475319\n",
      "Episode 4605 finished after 158 timesteps, total rewards 2.0, mean loss 0.015148544219864791\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4606 finished after 231 timesteps, total rewards 4.0, mean loss 0.01736490158626237\n",
      "Episode 4607 finished after 97 timesteps, total rewards 3.0, mean loss 0.01522481778502157\n",
      "Episode 4608 finished after 167 timesteps, total rewards 3.0, mean loss 0.01807593629823741\n",
      "Episode 4609 finished after 101 timesteps, total rewards 1.0, mean loss 0.014919728268915326\n",
      "Episode 4610 finished after 267 timesteps, total rewards 4.0, mean loss 0.01588886491657838\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 4611 finished after 144 timesteps, total rewards 3.0, mean loss 0.01660485732847721\n",
      "Episode 4612 finished after 125 timesteps, total rewards 1.0, mean loss 0.017946767042391003\n",
      "Episode 4613 finished after 159 timesteps, total rewards 3.0, mean loss 0.01735511363656758\n",
      "Episode 4614 finished after 242 timesteps, total rewards 7.0, mean loss 0.01631523322839336\n",
      "Episode 4615 finished after 158 timesteps, total rewards 3.0, mean loss 0.01663766472741867\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 4616 finished after 236 timesteps, total rewards 8.0, mean loss 0.018288480875603225\n",
      "Episode 4617 finished after 137 timesteps, total rewards 4.0, mean loss 0.016301178219592212\n",
      "Episode 4618 finished after 306 timesteps, total rewards 9.0, mean loss 0.016668531672003394\n",
      "Episode 4619 finished after 207 timesteps, total rewards 2.0, mean loss 0.017785984003165025\n",
      "Episode 4620 finished after 138 timesteps, total rewards 1.0, mean loss 0.017251917999575213\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4621 finished after 166 timesteps, total rewards 3.0, mean loss 0.016190533234502178\n",
      "Episode 4622 finished after 244 timesteps, total rewards 2.0, mean loss 0.016956809929144553\n",
      "Episode 4623 finished after 142 timesteps, total rewards 5.0, mean loss 0.01880923028327\n",
      "Episode 4624 finished after 275 timesteps, total rewards 10.0, mean loss 0.01722919960590926\n",
      "Episode 4625 finished after 155 timesteps, total rewards 3.0, mean loss 0.015564716973852727\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4626 finished after 160 timesteps, total rewards 3.0, mean loss 0.01606420836978941\n",
      "Episode 4627 finished after 128 timesteps, total rewards 3.0, mean loss 0.018121857658115914\n",
      "Episode 4628 finished after 277 timesteps, total rewards 4.0, mean loss 0.016416176469384657\n",
      "Episode 4629 finished after 216 timesteps, total rewards 5.0, mean loss 0.015562258255495518\n",
      "Episode 4630 finished after 155 timesteps, total rewards 4.0, mean loss 0.018560494631228427\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 4631 finished after 110 timesteps, total rewards 0.0, mean loss 0.017301698092540556\n",
      "Episode 4632 finished after 197 timesteps, total rewards 6.0, mean loss 0.015613570193013147\n",
      "Episode 4633 finished after 171 timesteps, total rewards 1.0, mean loss 0.01705502763142733\n",
      "Episode 4634 finished after 217 timesteps, total rewards 5.0, mean loss 0.015586248188052104\n",
      "Episode 4635 finished after 358 timesteps, total rewards 7.0, mean loss 0.014999500244619149\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4636 finished after 103 timesteps, total rewards 3.0, mean loss 0.017149153927304935\n",
      "Episode 4637 finished after 200 timesteps, total rewards 4.0, mean loss 0.01666233204829041\n",
      "Episode 4638 finished after 125 timesteps, total rewards 3.0, mean loss 0.01561725649330765\n",
      "Episode 4639 finished after 350 timesteps, total rewards 7.0, mean loss 0.016664537474779147\n",
      "Episode 4640 finished after 170 timesteps, total rewards 3.0, mean loss 0.018296516698319464\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 115.000000\n",
      "---------------------------------------\n",
      "Episode 4641 finished after 129 timesteps, total rewards 1.0, mean loss 0.018851765987369443\n",
      "Episode 4642 finished after 158 timesteps, total rewards 2.0, mean loss 0.017803034431277478\n",
      "Episode 4643 finished after 275 timesteps, total rewards 4.0, mean loss 0.015838092931715602\n",
      "Episode 4644 finished after 171 timesteps, total rewards 4.0, mean loss 0.016535912538274078\n",
      "Episode 4645 finished after 270 timesteps, total rewards 7.0, mean loss 0.017173515982856904\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 4646 finished after 206 timesteps, total rewards 9.0, mean loss 0.01728666959967898\n",
      "Episode 4647 finished after 173 timesteps, total rewards 4.0, mean loss 0.0169797776713752\n",
      "Episode 4648 finished after 209 timesteps, total rewards 4.0, mean loss 0.01614585244119096\n",
      "Episode 4649 finished after 115 timesteps, total rewards 1.0, mean loss 0.017886344934611217\n",
      "Episode 4650 finished after 162 timesteps, total rewards 3.0, mean loss 0.01868962800204984\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 4651 finished after 163 timesteps, total rewards 4.0, mean loss 0.016077109598954625\n",
      "Episode 4652 finished after 176 timesteps, total rewards 2.0, mean loss 0.016298112188285977\n",
      "Episode 4653 finished after 194 timesteps, total rewards 4.0, mean loss 0.014844876687647295\n",
      "Episode 4654 finished after 196 timesteps, total rewards 4.0, mean loss 0.0173781446284586\n",
      "Episode 4655 finished after 210 timesteps, total rewards 2.0, mean loss 0.01726096680864603\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 4656 finished after 91 timesteps, total rewards 2.0, mean loss 0.016315862682269335\n",
      "Episode 4657 finished after 114 timesteps, total rewards 2.0, mean loss 0.014875759061205349\n",
      "Episode 4658 finished after 124 timesteps, total rewards 5.0, mean loss 0.016990862815873697\n",
      "Episode 4659 finished after 96 timesteps, total rewards 1.0, mean loss 0.013673051050621629\n",
      "Episode 4660 finished after 133 timesteps, total rewards 1.0, mean loss 0.01543890123423609\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 4661 finished after 205 timesteps, total rewards 4.0, mean loss 0.015915911427199295\n",
      "Episode 4662 finished after 254 timesteps, total rewards 6.0, mean loss 0.015327078639987973\n",
      "Episode 4663 finished after 250 timesteps, total rewards 6.0, mean loss 0.014512973699718714\n",
      "Episode 4664 finished after 215 timesteps, total rewards 6.0, mean loss 0.015466610136506862\n",
      "Episode 4665 finished after 110 timesteps, total rewards 5.0, mean loss 0.01837483763440766\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 4666 finished after 181 timesteps, total rewards 2.0, mean loss 0.015129998692913363\n",
      "Episode 4667 finished after 133 timesteps, total rewards 5.0, mean loss 0.015664281404080025\n",
      "Episode 4668 finished after 270 timesteps, total rewards 8.0, mean loss 0.017255617662643392\n",
      "Episode 4669 finished after 162 timesteps, total rewards 5.0, mean loss 0.0162158710846137\n",
      "Episode 4670 finished after 111 timesteps, total rewards 1.0, mean loss 0.0147913876534985\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 4671 finished after 136 timesteps, total rewards 3.0, mean loss 0.015830504364738075\n",
      "Episode 4672 finished after 88 timesteps, total rewards 2.0, mean loss 0.016452931445779872\n",
      "Episode 4673 finished after 167 timesteps, total rewards 2.0, mean loss 0.01833778131379562\n",
      "Episode 4674 finished after 291 timesteps, total rewards 4.0, mean loss 0.01666346259448754\n",
      "Episode 4675 finished after 226 timesteps, total rewards 5.0, mean loss 0.01679554392080919\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 4676 finished after 380 timesteps, total rewards 6.0, mean loss 0.016792071562591254\n",
      "Episode 4677 finished after 178 timesteps, total rewards 2.0, mean loss 0.017174809269889603\n",
      "Episode 4678 finished after 110 timesteps, total rewards 0.0, mean loss 0.01586162405596538\n",
      "Episode 4679 finished after 212 timesteps, total rewards 1.0, mean loss 0.01790649265617589\n",
      "Episode 4680 finished after 93 timesteps, total rewards 3.0, mean loss 0.014595304559465618\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 4681 finished after 278 timesteps, total rewards 6.0, mean loss 0.016483139002146267\n",
      "Episode 4682 finished after 153 timesteps, total rewards 6.0, mean loss 0.015338090624282855\n",
      "Episode 4683 finished after 242 timesteps, total rewards 5.0, mean loss 0.01828646461945027\n",
      "Episode 4684 finished after 118 timesteps, total rewards 3.0, mean loss 0.014543361329449877\n",
      "Episode 4685 finished after 215 timesteps, total rewards 2.0, mean loss 0.018072757980418068\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 4686 finished after 96 timesteps, total rewards 2.0, mean loss 0.014150400598737178\n",
      "Episode 4687 finished after 213 timesteps, total rewards 1.0, mean loss 0.016507053129624208\n",
      "Episode 4688 finished after 158 timesteps, total rewards 3.0, mean loss 0.01636197679245821\n",
      "Episode 4689 finished after 94 timesteps, total rewards 1.0, mean loss 0.01907061829053341\n",
      "Episode 4690 finished after 156 timesteps, total rewards 3.0, mean loss 0.015934056595702153\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 4691 finished after 125 timesteps, total rewards 3.0, mean loss 0.018882340375334025\n",
      "Episode 4692 finished after 193 timesteps, total rewards 6.0, mean loss 0.01688812962380947\n",
      "Episode 4693 finished after 103 timesteps, total rewards 0.0, mean loss 0.01779980005388989\n",
      "Episode 4694 finished after 200 timesteps, total rewards 3.0, mean loss 0.016473177791340277\n",
      "Episode 4695 finished after 92 timesteps, total rewards 0.0, mean loss 0.013535980604659848\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 105.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4696 finished after 123 timesteps, total rewards 1.0, mean loss 0.017665669162061096\n",
      "Episode 4697 finished after 211 timesteps, total rewards 6.0, mean loss 0.018964621086132612\n",
      "Episode 4698 finished after 124 timesteps, total rewards 0.0, mean loss 0.016734087999324285\n",
      "Episode 4699 finished after 127 timesteps, total rewards 2.0, mean loss 0.015789871998717935\n",
      "Episode 4700 finished after 169 timesteps, total rewards 7.0, mean loss 0.015701443567243817\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 4701 finished after 97 timesteps, total rewards 2.0, mean loss 0.017071731042923387\n",
      "Episode 4702 finished after 144 timesteps, total rewards 2.0, mean loss 0.018234054986452166\n",
      "Episode 4703 finished after 123 timesteps, total rewards 1.0, mean loss 0.02001535024766515\n",
      "Episode 4704 finished after 272 timesteps, total rewards 5.0, mean loss 0.01822015103928553\n",
      "Episode 4705 finished after 141 timesteps, total rewards 1.0, mean loss 0.01724712211879776\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 4706 finished after 104 timesteps, total rewards 3.0, mean loss 0.01879547886290731\n",
      "Episode 4707 finished after 99 timesteps, total rewards 2.0, mean loss 0.01881614043800668\n",
      "Episode 4708 finished after 137 timesteps, total rewards 3.0, mean loss 0.017740499438976286\n",
      "Episode 4709 finished after 239 timesteps, total rewards 5.0, mean loss 0.015950443623996648\n",
      "Episode 4710 finished after 204 timesteps, total rewards 6.0, mean loss 0.014946157366092153\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4711 finished after 136 timesteps, total rewards 2.0, mean loss 0.015379501540070017\n",
      "Episode 4712 finished after 165 timesteps, total rewards 6.0, mean loss 0.016910313474804615\n",
      "Episode 4713 finished after 130 timesteps, total rewards 2.0, mean loss 0.016034590479774544\n",
      "Episode 4714 finished after 278 timesteps, total rewards 6.0, mean loss 0.017316435619505296\n",
      "Episode 4715 finished after 84 timesteps, total rewards 1.0, mean loss 0.015797218646011538\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 4716 finished after 126 timesteps, total rewards 4.0, mean loss 0.014636555124842932\n",
      "Episode 4717 finished after 159 timesteps, total rewards 3.0, mean loss 0.015989618274857693\n",
      "Episode 4718 finished after 170 timesteps, total rewards 4.0, mean loss 0.01703650850446566\n",
      "Episode 4719 finished after 183 timesteps, total rewards 6.0, mean loss 0.01545170458323765\n",
      "Episode 4720 finished after 194 timesteps, total rewards 2.0, mean loss 0.017687237288049162\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 4721 finished after 214 timesteps, total rewards 3.0, mean loss 0.016714485736441947\n",
      "Episode 4722 finished after 201 timesteps, total rewards 5.0, mean loss 0.016506702380268766\n",
      "Episode 4723 finished after 175 timesteps, total rewards 4.0, mean loss 0.01577987293712795\n",
      "Episode 4724 finished after 126 timesteps, total rewards 0.0, mean loss 0.017024085515119608\n",
      "Episode 4725 finished after 324 timesteps, total rewards 6.0, mean loss 0.017067417457849246\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 4726 finished after 255 timesteps, total rewards 6.0, mean loss 0.01829470224660255\n",
      "Episode 4727 finished after 189 timesteps, total rewards 5.0, mean loss 0.016776073584825826\n",
      "Episode 4728 finished after 91 timesteps, total rewards 1.0, mean loss 0.01955026458145457\n",
      "Episode 4729 finished after 266 timesteps, total rewards 7.0, mean loss 0.01681434514773029\n",
      "Episode 4730 finished after 157 timesteps, total rewards 1.0, mean loss 0.014441435027327745\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 10.000000\n",
      "---------------------------------------\n",
      "Episode 4731 finished after 255 timesteps, total rewards 3.0, mean loss 0.017001954866025376\n",
      "Episode 4732 finished after 87 timesteps, total rewards 2.0, mean loss 0.01735884832598312\n",
      "Episode 4733 finished after 161 timesteps, total rewards 4.0, mean loss 0.014617608844227395\n",
      "Episode 4734 finished after 168 timesteps, total rewards 4.0, mean loss 0.016584397568034807\n",
      "Episode 4735 finished after 191 timesteps, total rewards 3.0, mean loss 0.01660754576115747\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 4736 finished after 254 timesteps, total rewards 6.0, mean loss 0.01685985099702091\n",
      "Episode 4737 finished after 144 timesteps, total rewards 1.0, mean loss 0.01620385037757741\n",
      "Episode 4738 finished after 206 timesteps, total rewards 3.0, mean loss 0.014876270295997846\n",
      "Episode 4739 finished after 204 timesteps, total rewards 3.0, mean loss 0.01571192116220938\n",
      "Episode 4740 finished after 185 timesteps, total rewards 1.0, mean loss 0.016146456319964617\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 4741 finished after 257 timesteps, total rewards 5.0, mean loss 0.016821421923365894\n",
      "Episode 4742 finished after 156 timesteps, total rewards 0.0, mean loss 0.015931975714360867\n",
      "Episode 4743 finished after 130 timesteps, total rewards 2.0, mean loss 0.01419453290470231\n",
      "Episode 4744 finished after 93 timesteps, total rewards 0.0, mean loss 0.017143886328564697\n",
      "Episode 4745 finished after 212 timesteps, total rewards 7.0, mean loss 0.01651157740116963\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 63.333333\n",
      "---------------------------------------\n",
      "Episode 4746 finished after 209 timesteps, total rewards 2.0, mean loss 0.016656003395567814\n",
      "Episode 4747 finished after 145 timesteps, total rewards 1.0, mean loss 0.019332132774308838\n",
      "Episode 4748 finished after 143 timesteps, total rewards 2.0, mean loss 0.016080322042042454\n",
      "Episode 4749 finished after 220 timesteps, total rewards 4.0, mean loss 0.017911181748125025\n",
      "Episode 4750 finished after 189 timesteps, total rewards 3.0, mean loss 0.015529719649483917\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 4751 finished after 93 timesteps, total rewards 2.0, mean loss 0.017677088050291903\n",
      "Episode 4752 finished after 127 timesteps, total rewards 1.0, mean loss 0.017463954086745465\n",
      "Episode 4753 finished after 157 timesteps, total rewards 3.0, mean loss 0.015728560690827384\n",
      "Episode 4754 finished after 156 timesteps, total rewards 4.0, mean loss 0.016212693978256237\n",
      "Episode 4755 finished after 118 timesteps, total rewards 2.0, mean loss 0.01930553117723567\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 4756 finished after 227 timesteps, total rewards 5.0, mean loss 0.01567988421579224\n",
      "Episode 4757 finished after 170 timesteps, total rewards 7.0, mean loss 0.016641452627749565\n",
      "Episode 4758 finished after 155 timesteps, total rewards 4.0, mean loss 0.018179146090762748\n",
      "Episode 4759 finished after 82 timesteps, total rewards 2.0, mean loss 0.017374308576818737\n",
      "Episode 4760 finished after 232 timesteps, total rewards 2.0, mean loss 0.01750011480682337\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4761 finished after 156 timesteps, total rewards 2.0, mean loss 0.016556166815392386\n",
      "Episode 4762 finished after 114 timesteps, total rewards 2.0, mean loss 0.016422694766477338\n",
      "Episode 4763 finished after 128 timesteps, total rewards 3.0, mean loss 0.015483228621633316\n",
      "Episode 4764 finished after 124 timesteps, total rewards 4.0, mean loss 0.01576957483719584\n",
      "Episode 4765 finished after 161 timesteps, total rewards 3.0, mean loss 0.015999421507687024\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4766 finished after 130 timesteps, total rewards 1.0, mean loss 0.01654665532271163\n",
      "Episode 4767 finished after 107 timesteps, total rewards 0.0, mean loss 0.014613749655224731\n",
      "Episode 4768 finished after 153 timesteps, total rewards 0.0, mean loss 0.014960511050375751\n",
      "Episode 4769 finished after 200 timesteps, total rewards 4.0, mean loss 0.014806636531720868\n",
      "Episode 4770 finished after 200 timesteps, total rewards 7.0, mean loss 0.016531069412594662\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 4771 finished after 250 timesteps, total rewards 4.0, mean loss 0.01558166541159153\n",
      "Episode 4772 finished after 143 timesteps, total rewards 3.0, mean loss 0.016083973828683627\n",
      "Episode 4773 finished after 110 timesteps, total rewards 3.0, mean loss 0.01979851419825784\n",
      "Episode 4774 finished after 236 timesteps, total rewards 4.0, mean loss 0.01775997525901865\n",
      "Episode 4775 finished after 196 timesteps, total rewards 4.0, mean loss 0.01756613225467047\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4776 finished after 230 timesteps, total rewards 8.0, mean loss 0.014776816302606756\n",
      "Episode 4777 finished after 139 timesteps, total rewards 5.0, mean loss 0.01586871202482353\n",
      "Episode 4778 finished after 152 timesteps, total rewards 2.0, mean loss 0.018523513566163417\n",
      "Episode 4779 finished after 89 timesteps, total rewards 2.0, mean loss 0.017061346096740093\n",
      "Episode 4780 finished after 147 timesteps, total rewards 2.0, mean loss 0.01596290732695892\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 4781 finished after 218 timesteps, total rewards 3.0, mean loss 0.016461580692696463\n",
      "Episode 4782 finished after 149 timesteps, total rewards 3.0, mean loss 0.017124580020153733\n",
      "Episode 4783 finished after 180 timesteps, total rewards 6.0, mean loss 0.016053906257729976\n",
      "Episode 4784 finished after 84 timesteps, total rewards 1.0, mean loss 0.015987513650630024\n",
      "Episode 4785 finished after 102 timesteps, total rewards 0.0, mean loss 0.017518399863083865\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 4786 finished after 117 timesteps, total rewards 1.0, mean loss 0.016145900595320277\n",
      "Episode 4787 finished after 265 timesteps, total rewards 4.0, mean loss 0.015747207388246677\n",
      "Episode 4788 finished after 165 timesteps, total rewards 3.0, mean loss 0.017414370031716923\n",
      "Episode 4789 finished after 192 timesteps, total rewards 2.0, mean loss 0.016302727774018422\n",
      "Episode 4790 finished after 131 timesteps, total rewards 2.0, mean loss 0.014239249006269201\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 4791 finished after 112 timesteps, total rewards 4.0, mean loss 0.01511587259717219\n",
      "Episode 4792 finished after 281 timesteps, total rewards 9.0, mean loss 0.015766178969098286\n",
      "Episode 4793 finished after 177 timesteps, total rewards 2.0, mean loss 0.015090262149013368\n",
      "Episode 4794 finished after 160 timesteps, total rewards 3.0, mean loss 0.0173043952556327\n",
      "Episode 4795 finished after 243 timesteps, total rewards 5.0, mean loss 0.015242797925727204\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 56.666667\n",
      "---------------------------------------\n",
      "Episode 4796 finished after 177 timesteps, total rewards 5.0, mean loss 0.013629949124829874\n",
      "Episode 4797 finished after 214 timesteps, total rewards 5.0, mean loss 0.014458441068897042\n",
      "Episode 4798 finished after 171 timesteps, total rewards 0.0, mean loss 0.016181579634427415\n",
      "Episode 4799 finished after 94 timesteps, total rewards 1.0, mean loss 0.011488428330692918\n",
      "Episode 4800 finished after 143 timesteps, total rewards 2.0, mean loss 0.01770201943801729\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 4801 finished after 151 timesteps, total rewards 5.0, mean loss 0.015401926539246215\n",
      "Episode 4802 finished after 236 timesteps, total rewards 3.0, mean loss 0.014563467134563727\n",
      "Episode 4803 finished after 237 timesteps, total rewards 5.0, mean loss 0.015580279806153337\n",
      "Episode 4804 finished after 321 timesteps, total rewards 5.0, mean loss 0.01610769497374264\n",
      "Episode 4805 finished after 165 timesteps, total rewards 2.0, mean loss 0.017653482867348375\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 150.000000\n",
      "---------------------------------------\n",
      "Episode 4806 finished after 239 timesteps, total rewards 8.0, mean loss 0.015228168447560409\n",
      "Episode 4807 finished after 190 timesteps, total rewards 2.0, mean loss 0.01605637376809395\n",
      "Episode 4808 finished after 173 timesteps, total rewards 1.0, mean loss 0.017334835609889486\n",
      "Episode 4809 finished after 129 timesteps, total rewards 1.0, mean loss 0.0166520321111794\n",
      "Episode 4810 finished after 109 timesteps, total rewards 1.0, mean loss 0.01565767716992339\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 4811 finished after 289 timesteps, total rewards 6.0, mean loss 0.015686016554773422\n",
      "Episode 4812 finished after 93 timesteps, total rewards 1.0, mean loss 0.0156813503621066\n",
      "Episode 4813 finished after 128 timesteps, total rewards 2.0, mean loss 0.01698784345353488\n",
      "Episode 4814 finished after 191 timesteps, total rewards 1.0, mean loss 0.015420002742129237\n",
      "Episode 4815 finished after 142 timesteps, total rewards 0.0, mean loss 0.016289109319061513\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 4816 finished after 244 timesteps, total rewards 1.0, mean loss 0.01575162780488322\n",
      "Episode 4817 finished after 166 timesteps, total rewards 2.0, mean loss 0.015346477349591723\n",
      "Episode 4818 finished after 160 timesteps, total rewards 5.0, mean loss 0.015936445316765458\n",
      "Episode 4819 finished after 168 timesteps, total rewards 2.0, mean loss 0.017866607431122766\n",
      "Episode 4820 finished after 171 timesteps, total rewards 0.0, mean loss 0.015771335389121975\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 4821 finished after 352 timesteps, total rewards 5.0, mean loss 0.016476014783124396\n",
      "Episode 4822 finished after 138 timesteps, total rewards 1.0, mean loss 0.016440846915930455\n",
      "Episode 4823 finished after 90 timesteps, total rewards 1.0, mean loss 0.01373363453636153\n",
      "Episode 4824 finished after 306 timesteps, total rewards 10.0, mean loss 0.01555627428514128\n",
      "Episode 4825 finished after 309 timesteps, total rewards 7.0, mean loss 0.01631105378869933\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 4826 finished after 131 timesteps, total rewards 1.0, mean loss 0.014594602238614368\n",
      "Episode 4827 finished after 92 timesteps, total rewards 4.0, mean loss 0.013002921323514665\n",
      "Episode 4828 finished after 126 timesteps, total rewards 2.0, mean loss 0.014899064256395731\n",
      "Episode 4829 finished after 172 timesteps, total rewards 5.0, mean loss 0.014873253533530027\n",
      "Episode 4830 finished after 255 timesteps, total rewards 9.0, mean loss 0.0168409875735604\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 5.000000\n",
      "---------------------------------------\n",
      "Episode 4831 finished after 160 timesteps, total rewards 6.0, mean loss 0.01756091152492445\n",
      "Episode 4832 finished after 94 timesteps, total rewards 3.0, mean loss 0.01598157430875135\n",
      "Episode 4833 finished after 162 timesteps, total rewards 1.0, mean loss 0.013625289263192243\n",
      "Episode 4834 finished after 163 timesteps, total rewards 1.0, mean loss 0.01529044623449559\n",
      "Episode 4835 finished after 262 timesteps, total rewards 4.0, mean loss 0.015751933487476995\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4836 finished after 194 timesteps, total rewards 4.0, mean loss 0.01551535431794899\n",
      "Episode 4837 finished after 219 timesteps, total rewards 9.0, mean loss 0.017304969985531344\n",
      "Episode 4838 finished after 215 timesteps, total rewards 1.0, mean loss 0.014094279737341717\n",
      "Episode 4839 finished after 128 timesteps, total rewards 0.0, mean loss 0.014143906970275566\n",
      "Episode 4840 finished after 114 timesteps, total rewards 4.0, mean loss 0.01660175197716933\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 4841 finished after 157 timesteps, total rewards 0.0, mean loss 0.014768756179370365\n",
      "Episode 4842 finished after 113 timesteps, total rewards 0.0, mean loss 0.017442253269914505\n",
      "Episode 4843 finished after 186 timesteps, total rewards 4.0, mean loss 0.01705900665187347\n",
      "Episode 4844 finished after 116 timesteps, total rewards 2.0, mean loss 0.017543318974464362\n",
      "Episode 4845 finished after 103 timesteps, total rewards 2.0, mean loss 0.017964879049720623\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 4846 finished after 179 timesteps, total rewards 1.0, mean loss 0.016438262411601342\n",
      "Episode 4847 finished after 157 timesteps, total rewards 3.0, mean loss 0.015329927053290662\n",
      "Episode 4848 finished after 169 timesteps, total rewards 1.0, mean loss 0.0134446675608144\n",
      "Episode 4849 finished after 97 timesteps, total rewards 0.0, mean loss 0.01674115988809961\n",
      "Episode 4850 finished after 156 timesteps, total rewards 3.0, mean loss 0.015921952669323686\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4851 finished after 225 timesteps, total rewards 4.0, mean loss 0.019000127244119844\n",
      "Episode 4852 finished after 221 timesteps, total rewards 5.0, mean loss 0.015900820049107478\n",
      "Episode 4853 finished after 118 timesteps, total rewards 1.0, mean loss 0.014956010625918665\n",
      "Episode 4854 finished after 125 timesteps, total rewards 1.0, mean loss 0.016228321136906743\n",
      "Episode 4855 finished after 237 timesteps, total rewards 4.0, mean loss 0.015502879194687376\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 4856 finished after 99 timesteps, total rewards 4.0, mean loss 0.016367224099189795\n",
      "Episode 4857 finished after 199 timesteps, total rewards 1.0, mean loss 0.016358187553781697\n",
      "Episode 4858 finished after 243 timesteps, total rewards 2.0, mean loss 0.01576975994281578\n",
      "Episode 4859 finished after 138 timesteps, total rewards 2.0, mean loss 0.01450127825829322\n",
      "Episode 4860 finished after 167 timesteps, total rewards 4.0, mean loss 0.014568971871526655\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 4861 finished after 187 timesteps, total rewards 5.0, mean loss 0.01512113799849017\n",
      "Episode 4862 finished after 179 timesteps, total rewards 3.0, mean loss 0.014058317301024855\n",
      "Episode 4863 finished after 249 timesteps, total rewards 8.0, mean loss 0.017016904663302034\n",
      "Episode 4864 finished after 159 timesteps, total rewards 2.0, mean loss 0.018872780800041445\n",
      "Episode 4865 finished after 140 timesteps, total rewards 3.0, mean loss 0.01690869524359836\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 4866 finished after 176 timesteps, total rewards 7.0, mean loss 0.01669682979197453\n",
      "Episode 4867 finished after 189 timesteps, total rewards 3.0, mean loss 0.015039092229762997\n",
      "Episode 4868 finished after 186 timesteps, total rewards 5.0, mean loss 0.014977315110304663\n",
      "Episode 4869 finished after 180 timesteps, total rewards 4.0, mean loss 0.01747347225124637\n",
      "Episode 4870 finished after 157 timesteps, total rewards 5.0, mean loss 0.01674331960736946\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 4871 finished after 193 timesteps, total rewards 2.0, mean loss 0.01619761742511523\n",
      "Episode 4872 finished after 115 timesteps, total rewards 1.0, mean loss 0.01614357872420679\n",
      "Episode 4873 finished after 80 timesteps, total rewards 0.0, mean loss 0.014392279481398873\n",
      "Episode 4874 finished after 183 timesteps, total rewards 3.0, mean loss 0.0124738054258413\n",
      "Episode 4875 finished after 164 timesteps, total rewards 3.0, mean loss 0.014653130769502462\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 4876 finished after 179 timesteps, total rewards 4.0, mean loss 0.0145253103275851\n",
      "Episode 4877 finished after 283 timesteps, total rewards 2.0, mean loss 0.017457406593127958\n",
      "Episode 4878 finished after 142 timesteps, total rewards 2.0, mean loss 0.01612328401353525\n",
      "Episode 4879 finished after 131 timesteps, total rewards 3.0, mean loss 0.01661964682400056\n",
      "Episode 4880 finished after 250 timesteps, total rewards 5.0, mean loss 0.014699877954088152\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 4881 finished after 180 timesteps, total rewards 5.0, mean loss 0.016158802934094437\n",
      "Episode 4882 finished after 214 timesteps, total rewards 3.0, mean loss 0.016060109857347942\n",
      "Episode 4883 finished after 133 timesteps, total rewards 2.0, mean loss 0.015139970830396601\n",
      "Episode 4884 finished after 284 timesteps, total rewards 5.0, mean loss 0.015794465358493666\n",
      "Episode 4885 finished after 111 timesteps, total rewards 2.0, mean loss 0.015537208860723285\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 4886 finished after 89 timesteps, total rewards 2.0, mean loss 0.018160079681743563\n",
      "Episode 4887 finished after 168 timesteps, total rewards 2.0, mean loss 0.016337235075687722\n",
      "Episode 4888 finished after 242 timesteps, total rewards 4.0, mean loss 0.016398223914583177\n",
      "Episode 4889 finished after 283 timesteps, total rewards 4.0, mean loss 0.015657708034422842\n",
      "Episode 4890 finished after 235 timesteps, total rewards 2.0, mean loss 0.014989063509323813\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 4891 finished after 247 timesteps, total rewards 2.0, mean loss 0.016012747743051982\n",
      "Episode 4892 finished after 148 timesteps, total rewards 3.0, mean loss 0.013872736168245005\n",
      "Episode 4893 finished after 228 timesteps, total rewards 2.0, mean loss 0.015405948843659931\n",
      "Episode 4894 finished after 116 timesteps, total rewards 0.0, mean loss 0.016535293132243358\n",
      "Episode 4895 finished after 124 timesteps, total rewards 1.0, mean loss 0.014956166541519305\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 4896 finished after 175 timesteps, total rewards 4.0, mean loss 0.01899576206159379\n",
      "Episode 4897 finished after 147 timesteps, total rewards 3.0, mean loss 0.014835107898130556\n",
      "Episode 4898 finished after 262 timesteps, total rewards 7.0, mean loss 0.016451698566168432\n",
      "Episode 4899 finished after 130 timesteps, total rewards 0.0, mean loss 0.015239260485395789\n",
      "Episode 4900 finished after 159 timesteps, total rewards 1.0, mean loss 0.016497800071026733\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 4901 finished after 192 timesteps, total rewards 7.0, mean loss 0.015244122861380069\n",
      "Episode 4902 finished after 152 timesteps, total rewards 2.0, mean loss 0.015387824419456976\n",
      "Episode 4903 finished after 111 timesteps, total rewards 2.0, mean loss 0.017125550649120466\n",
      "Episode 4904 finished after 241 timesteps, total rewards 7.0, mean loss 0.015422282304278579\n",
      "Episode 4905 finished after 229 timesteps, total rewards 2.0, mean loss 0.01553102416356448\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4906 finished after 167 timesteps, total rewards 2.0, mean loss 0.016010410008773564\n",
      "Episode 4907 finished after 115 timesteps, total rewards 2.0, mean loss 0.01694081133008813\n",
      "Episode 4908 finished after 278 timesteps, total rewards 4.0, mean loss 0.015539683461886087\n",
      "Episode 4909 finished after 93 timesteps, total rewards 1.0, mean loss 0.012725820251670416\n",
      "Episode 4910 finished after 141 timesteps, total rewards 3.0, mean loss 0.01635601162309395\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 98.333333\n",
      "---------------------------------------\n",
      "Episode 4911 finished after 252 timesteps, total rewards 7.0, mean loss 0.014993817700694004\n",
      "Episode 4912 finished after 157 timesteps, total rewards 1.0, mean loss 0.016318011205127332\n",
      "Episode 4913 finished after 161 timesteps, total rewards 3.0, mean loss 0.01584044337035475\n",
      "Episode 4914 finished after 185 timesteps, total rewards 4.0, mean loss 0.015227037481334362\n",
      "Episode 4915 finished after 158 timesteps, total rewards 1.0, mean loss 0.015060760740992388\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 4916 finished after 133 timesteps, total rewards 2.0, mean loss 0.01610929489527878\n",
      "Episode 4917 finished after 175 timesteps, total rewards 6.0, mean loss 0.014079751489417893\n",
      "Episode 4918 finished after 279 timesteps, total rewards 2.0, mean loss 0.017724723854739766\n",
      "Episode 4919 finished after 194 timesteps, total rewards 4.0, mean loss 0.015653138098153333\n",
      "Episode 4920 finished after 122 timesteps, total rewards 3.0, mean loss 0.013957083269144546\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 78.333333\n",
      "---------------------------------------\n",
      "Episode 4921 finished after 89 timesteps, total rewards 3.0, mean loss 0.016037930916272772\n",
      "Episode 4922 finished after 223 timesteps, total rewards 2.0, mean loss 0.015185973324057031\n",
      "Episode 4923 finished after 225 timesteps, total rewards 3.0, mean loss 0.015269440294553837\n",
      "Episode 4924 finished after 95 timesteps, total rewards 1.0, mean loss 0.016361182626630916\n",
      "Episode 4925 finished after 95 timesteps, total rewards 0.0, mean loss 0.0154894706670587\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 16.666667\n",
      "---------------------------------------\n",
      "Episode 4926 finished after 164 timesteps, total rewards 4.0, mean loss 0.01404645823427794\n",
      "Episode 4927 finished after 160 timesteps, total rewards 3.0, mean loss 0.014696430279582274\n",
      "Episode 4928 finished after 173 timesteps, total rewards 5.0, mean loss 0.016821975080226412\n",
      "Episode 4929 finished after 127 timesteps, total rewards 3.0, mean loss 0.013819516058937888\n",
      "Episode 4930 finished after 98 timesteps, total rewards 3.0, mean loss 0.017165451063489427\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 4931 finished after 168 timesteps, total rewards 2.0, mean loss 0.015755523475014514\n",
      "Episode 4932 finished after 232 timesteps, total rewards 1.0, mean loss 0.015650974287539077\n",
      "Episode 4933 finished after 320 timesteps, total rewards 6.0, mean loss 0.015354186628974276\n",
      "Episode 4934 finished after 332 timesteps, total rewards 8.0, mean loss 0.015222811226730224\n",
      "Episode 4935 finished after 150 timesteps, total rewards 2.0, mean loss 0.015462908369178574\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 4936 finished after 229 timesteps, total rewards 2.0, mean loss 0.013985840236162157\n",
      "Episode 4937 finished after 222 timesteps, total rewards 6.0, mean loss 0.015197691759346304\n",
      "Episode 4938 finished after 101 timesteps, total rewards 2.0, mean loss 0.014198214991368575\n",
      "Episode 4939 finished after 96 timesteps, total rewards 2.0, mean loss 0.012908189460479965\n",
      "Episode 4940 finished after 130 timesteps, total rewards 2.0, mean loss 0.01665169945380722\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 71.666667\n",
      "---------------------------------------\n",
      "Episode 4941 finished after 130 timesteps, total rewards 1.0, mean loss 0.017251926073088095\n",
      "Episode 4942 finished after 281 timesteps, total rewards 5.0, mean loss 0.015031837202853844\n",
      "Episode 4943 finished after 282 timesteps, total rewards 8.0, mean loss 0.014087082678617262\n",
      "Episode 4944 finished after 200 timesteps, total rewards 4.0, mean loss 0.013196118935593405\n",
      "Episode 4945 finished after 259 timesteps, total rewards 3.0, mean loss 0.01424719435504447\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 4946 finished after 125 timesteps, total rewards 0.0, mean loss 0.014407885605469345\n",
      "Episode 4947 finished after 161 timesteps, total rewards 4.0, mean loss 0.014383874992660669\n",
      "Episode 4948 finished after 245 timesteps, total rewards 2.0, mean loss 0.015295256136878564\n",
      "Episode 4949 finished after 175 timesteps, total rewards 4.0, mean loss 0.013924297224730254\n",
      "Episode 4950 finished after 87 timesteps, total rewards 2.0, mean loss 0.018352673142805868\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 3.333333\n",
      "---------------------------------------\n",
      "Episode 4951 finished after 124 timesteps, total rewards 3.0, mean loss 0.014705109870016215\n",
      "Episode 4952 finished after 203 timesteps, total rewards 2.0, mean loss 0.016895888795332957\n",
      "Episode 4953 finished after 114 timesteps, total rewards 1.0, mean loss 0.01442788924874836\n",
      "Episode 4954 finished after 304 timesteps, total rewards 6.0, mean loss 0.015617251133478834\n",
      "Episode 4955 finished after 193 timesteps, total rewards 3.0, mean loss 0.015338441396826984\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 4956 finished after 281 timesteps, total rewards 3.0, mean loss 0.015100899923036001\n",
      "Episode 4957 finished after 133 timesteps, total rewards 2.0, mean loss 0.01408177015592197\n",
      "Episode 4958 finished after 90 timesteps, total rewards 1.0, mean loss 0.01607065357836998\n",
      "Episode 4959 finished after 95 timesteps, total rewards 0.0, mean loss 0.014966169832960556\n",
      "Episode 4960 finished after 158 timesteps, total rewards 2.0, mean loss 0.013814443071030929\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 4961 finished after 187 timesteps, total rewards 6.0, mean loss 0.013914731800566144\n",
      "Episode 4962 finished after 87 timesteps, total rewards 2.0, mean loss 0.014555315311794737\n",
      "Episode 4963 finished after 175 timesteps, total rewards 6.0, mean loss 0.015217158457131258\n",
      "Episode 4964 finished after 167 timesteps, total rewards 3.0, mean loss 0.015322795687773137\n",
      "Episode 4965 finished after 228 timesteps, total rewards 2.0, mean loss 0.015617650429097315\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 4966 finished after 206 timesteps, total rewards 7.0, mean loss 0.016101917033807093\n",
      "Episode 4967 finished after 185 timesteps, total rewards 4.0, mean loss 0.014627942879608757\n",
      "Episode 4968 finished after 272 timesteps, total rewards 3.0, mean loss 0.01485508825734381\n",
      "Episode 4969 finished after 106 timesteps, total rewards 2.0, mean loss 0.016264867736026645\n",
      "Episode 4970 finished after 88 timesteps, total rewards 1.0, mean loss 0.012941290855947458\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 4971 finished after 173 timesteps, total rewards 5.0, mean loss 0.015853623759992032\n",
      "Episode 4972 finished after 161 timesteps, total rewards 6.0, mean loss 0.01624307303432005\n",
      "Episode 4973 finished after 98 timesteps, total rewards 1.0, mean loss 0.017283443147696708\n",
      "Episode 4974 finished after 106 timesteps, total rewards 1.0, mean loss 0.015708319528713682\n",
      "Episode 4975 finished after 243 timesteps, total rewards 5.0, mean loss 0.015827852307048477\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4976 finished after 94 timesteps, total rewards 2.0, mean loss 0.01391903156701951\n",
      "Episode 4977 finished after 161 timesteps, total rewards 3.0, mean loss 0.015068678480358968\n",
      "Episode 4978 finished after 204 timesteps, total rewards 1.0, mean loss 0.016228047438655233\n",
      "Episode 4979 finished after 95 timesteps, total rewards 0.0, mean loss 0.01454430004875911\n",
      "Episode 4980 finished after 226 timesteps, total rewards 1.0, mean loss 0.01495647451900094\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 4981 finished after 254 timesteps, total rewards 3.0, mean loss 0.015537986550186797\n",
      "Episode 4982 finished after 85 timesteps, total rewards 2.0, mean loss 0.012981233967687278\n",
      "Episode 4983 finished after 199 timesteps, total rewards 3.0, mean loss 0.015323976967390159\n",
      "Episode 4984 finished after 119 timesteps, total rewards 3.0, mean loss 0.01419648468032667\n",
      "Episode 4985 finished after 168 timesteps, total rewards 1.0, mean loss 0.015665749957579897\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 4986 finished after 189 timesteps, total rewards 1.0, mean loss 0.015521558474074241\n",
      "Episode 4987 finished after 134 timesteps, total rewards 4.0, mean loss 0.016542033228064097\n",
      "Episode 4988 finished after 304 timesteps, total rewards 7.0, mean loss 0.013765492243309333\n",
      "Episode 4989 finished after 146 timesteps, total rewards 3.0, mean loss 0.015093227778242468\n",
      "Episode 4990 finished after 186 timesteps, total rewards 3.0, mean loss 0.013103009281700016\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 4991 finished after 132 timesteps, total rewards 1.0, mean loss 0.013288964205209843\n",
      "Episode 4992 finished after 95 timesteps, total rewards 1.0, mean loss 0.013935742897324657\n",
      "Episode 4993 finished after 111 timesteps, total rewards 1.0, mean loss 0.015591750465761367\n",
      "Episode 4994 finished after 264 timesteps, total rewards 10.0, mean loss 0.01588361123029961\n",
      "Episode 4995 finished after 239 timesteps, total rewards 6.0, mean loss 0.016128303252246276\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 4996 finished after 140 timesteps, total rewards 3.0, mean loss 0.013422198618562626\n",
      "Episode 4997 finished after 157 timesteps, total rewards 2.0, mean loss 0.014072399792575817\n",
      "Episode 4998 finished after 94 timesteps, total rewards 1.0, mean loss 0.015264487936795551\n",
      "Episode 4999 finished after 255 timesteps, total rewards 5.0, mean loss 0.01590032241873297\n",
      "Episode 5000 finished after 119 timesteps, total rewards 1.0, mean loss 0.016070716897957027\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 5001 finished after 100 timesteps, total rewards 0.0, mean loss 0.013064882688922808\n",
      "Episode 5002 finished after 270 timesteps, total rewards 5.0, mean loss 0.01531274464343571\n",
      "Episode 5003 finished after 146 timesteps, total rewards 3.0, mean loss 0.018240294937516423\n",
      "Episode 5004 finished after 166 timesteps, total rewards 0.0, mean loss 0.015165657164803874\n",
      "Episode 5005 finished after 229 timesteps, total rewards 4.0, mean loss 0.016547646024264395\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.000000\n",
      "---------------------------------------\n",
      "Episode 5006 finished after 124 timesteps, total rewards 2.0, mean loss 0.013976838874382778\n",
      "Episode 5007 finished after 209 timesteps, total rewards 3.0, mean loss 0.01603352058952915\n",
      "Episode 5008 finished after 123 timesteps, total rewards 4.0, mean loss 0.014837734408797772\n",
      "Episode 5009 finished after 237 timesteps, total rewards 2.0, mean loss 0.015455783563882176\n",
      "Episode 5010 finished after 165 timesteps, total rewards 7.0, mean loss 0.01679433578145549\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 5011 finished after 199 timesteps, total rewards 8.0, mean loss 0.01446601287719218\n",
      "Episode 5012 finished after 255 timesteps, total rewards 9.0, mean loss 0.013925482384750948\n",
      "Episode 5013 finished after 199 timesteps, total rewards 0.0, mean loss 0.015032484207826493\n",
      "Episode 5014 finished after 135 timesteps, total rewards 7.0, mean loss 0.016064750548900553\n",
      "Episode 5015 finished after 254 timesteps, total rewards 4.0, mean loss 0.015566964446764997\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 121.666667\n",
      "---------------------------------------\n",
      "Episode 5016 finished after 284 timesteps, total rewards 5.0, mean loss 0.01943731928427874\n",
      "Episode 5017 finished after 260 timesteps, total rewards 7.0, mean loss 0.019172068113962618\n",
      "Episode 5018 finished after 174 timesteps, total rewards 4.0, mean loss 0.02015775282621041\n",
      "Episode 5019 finished after 205 timesteps, total rewards 6.0, mean loss 0.01915211288099427\n",
      "Episode 5020 finished after 250 timesteps, total rewards 2.0, mean loss 0.018114037713967263\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 5021 finished after 135 timesteps, total rewards 1.0, mean loss 0.019925633961058876\n",
      "Episode 5022 finished after 193 timesteps, total rewards 4.0, mean loss 0.01777586232450998\n",
      "Episode 5023 finished after 227 timesteps, total rewards 3.0, mean loss 0.01905291377451878\n",
      "Episode 5024 finished after 141 timesteps, total rewards 3.0, mean loss 0.02050083929983278\n",
      "Episode 5025 finished after 174 timesteps, total rewards 0.0, mean loss 0.01666494633202259\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 71.666667\n",
      "---------------------------------------\n",
      "Episode 5026 finished after 141 timesteps, total rewards 3.0, mean loss 0.017354323541917276\n",
      "Episode 5027 finished after 178 timesteps, total rewards 3.0, mean loss 0.017307501867036806\n",
      "Episode 5028 finished after 131 timesteps, total rewards 1.0, mean loss 0.01746538310766732\n",
      "Episode 5029 finished after 403 timesteps, total rewards 9.0, mean loss 0.017554373993415837\n",
      "Episode 5030 finished after 239 timesteps, total rewards 7.0, mean loss 0.018642350669880105\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 5031 finished after 149 timesteps, total rewards 4.0, mean loss 0.017782408222181086\n",
      "Episode 5032 finished after 218 timesteps, total rewards 4.0, mean loss 0.02039715260910212\n",
      "Episode 5033 finished after 171 timesteps, total rewards 6.0, mean loss 0.01852558189403941\n",
      "Episode 5034 finished after 248 timesteps, total rewards 3.0, mean loss 0.0177869510258578\n",
      "Episode 5035 finished after 163 timesteps, total rewards 5.0, mean loss 0.017877451202723604\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 5036 finished after 308 timesteps, total rewards 9.0, mean loss 0.0169372130328995\n",
      "Episode 5037 finished after 168 timesteps, total rewards 4.0, mean loss 0.01775732008363342\n",
      "Episode 5038 finished after 245 timesteps, total rewards 9.0, mean loss 0.01902728363995117\n",
      "Episode 5039 finished after 179 timesteps, total rewards 4.0, mean loss 0.018860566191760273\n",
      "Episode 5040 finished after 191 timesteps, total rewards 2.0, mean loss 0.016584351386982187\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 105.000000\n",
      "---------------------------------------\n",
      "Episode 5041 finished after 167 timesteps, total rewards 3.0, mean loss 0.017943933692587885\n",
      "Episode 5042 finished after 101 timesteps, total rewards 0.0, mean loss 0.01754787760685281\n",
      "Episode 5043 finished after 197 timesteps, total rewards 7.0, mean loss 0.01708728758154938\n",
      "Episode 5044 finished after 101 timesteps, total rewards 0.0, mean loss 0.016936009311771923\n",
      "Episode 5045 finished after 173 timesteps, total rewards 7.0, mean loss 0.016398141390746298\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5046 finished after 193 timesteps, total rewards 3.0, mean loss 0.019200181731486143\n",
      "Episode 5047 finished after 370 timesteps, total rewards 7.0, mean loss 0.017470786301104504\n",
      "Episode 5048 finished after 135 timesteps, total rewards 1.0, mean loss 0.02029559889083935\n",
      "Episode 5049 finished after 92 timesteps, total rewards 3.0, mean loss 0.012730904389172792\n",
      "Episode 5050 finished after 123 timesteps, total rewards 2.0, mean loss 0.016053336242789297\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 5051 finished after 271 timesteps, total rewards 7.0, mean loss 0.015571273838400125\n",
      "Episode 5052 finished after 313 timesteps, total rewards 10.0, mean loss 0.017420448753357933\n",
      "Episode 5053 finished after 161 timesteps, total rewards 2.0, mean loss 0.018914294388631115\n",
      "Episode 5054 finished after 164 timesteps, total rewards 6.0, mean loss 0.01722785024534593\n",
      "Episode 5055 finished after 153 timesteps, total rewards 2.0, mean loss 0.017826550666035876\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 5056 finished after 264 timesteps, total rewards 5.0, mean loss 0.018287789737135456\n",
      "Episode 5057 finished after 179 timesteps, total rewards 3.0, mean loss 0.01676070402953878\n",
      "Episode 5058 finished after 130 timesteps, total rewards 1.0, mean loss 0.01886894936637523\n",
      "Episode 5059 finished after 226 timesteps, total rewards 11.0, mean loss 0.018486512102705913\n",
      "Episode 5060 finished after 122 timesteps, total rewards 3.0, mean loss 0.01794399011529005\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 5061 finished after 123 timesteps, total rewards 0.0, mean loss 0.017139466091369588\n",
      "Episode 5062 finished after 182 timesteps, total rewards 4.0, mean loss 0.019899835164024198\n",
      "Episode 5063 finished after 159 timesteps, total rewards 3.0, mean loss 0.016526223150076076\n",
      "Episode 5064 finished after 230 timesteps, total rewards 1.0, mean loss 0.01648029418581206\n",
      "Episode 5065 finished after 130 timesteps, total rewards 1.0, mean loss 0.016862330440646753\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 5066 finished after 220 timesteps, total rewards 5.0, mean loss 0.017009523840070786\n",
      "Episode 5067 finished after 192 timesteps, total rewards 6.0, mean loss 0.015473732539248886\n",
      "Episode 5068 finished after 208 timesteps, total rewards 5.0, mean loss 0.01676490048703272\n",
      "Episode 5069 finished after 112 timesteps, total rewards 0.0, mean loss 0.015804563900538988\n",
      "Episode 5070 finished after 326 timesteps, total rewards 12.0, mean loss 0.016497260826979793\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 58.333333\n",
      "---------------------------------------\n",
      "Episode 5071 finished after 291 timesteps, total rewards 11.0, mean loss 0.017904053787825648\n",
      "Episode 5072 finished after 125 timesteps, total rewards 3.0, mean loss 0.014775695012882352\n",
      "Episode 5073 finished after 260 timesteps, total rewards 6.0, mean loss 0.01625991401155121\n",
      "Episode 5074 finished after 126 timesteps, total rewards 3.0, mean loss 0.01638618670034385\n",
      "Episode 5075 finished after 306 timesteps, total rewards 6.0, mean loss 0.016185118782553163\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 5076 finished after 90 timesteps, total rewards 2.0, mean loss 0.020000071122518018\n",
      "Episode 5077 finished after 125 timesteps, total rewards 2.0, mean loss 0.01580875744111836\n",
      "Episode 5078 finished after 255 timesteps, total rewards 4.0, mean loss 0.015878951761360262\n",
      "Episode 5079 finished after 84 timesteps, total rewards 2.0, mean loss 0.02042771922701615\n",
      "Episode 5080 finished after 199 timesteps, total rewards 1.0, mean loss 0.016314018356857175\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 5081 finished after 246 timesteps, total rewards 3.0, mean loss 0.017732767511265186\n",
      "Episode 5082 finished after 144 timesteps, total rewards 4.0, mean loss 0.016188810135645326\n",
      "Episode 5083 finished after 159 timesteps, total rewards 2.0, mean loss 0.015478344161192295\n",
      "Episode 5084 finished after 139 timesteps, total rewards 1.0, mean loss 0.01623047771361127\n",
      "Episode 5085 finished after 97 timesteps, total rewards 4.0, mean loss 0.014941935029034455\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 5086 finished after 204 timesteps, total rewards 3.0, mean loss 0.017315075517056838\n",
      "Episode 5087 finished after 191 timesteps, total rewards 4.0, mean loss 0.01649796523041633\n",
      "Episode 5088 finished after 132 timesteps, total rewards 1.0, mean loss 0.013357097831568823\n",
      "Episode 5089 finished after 203 timesteps, total rewards 7.0, mean loss 0.015839977000651907\n",
      "Episode 5090 finished after 237 timesteps, total rewards 5.0, mean loss 0.01615436735340956\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 85.000000\n",
      "---------------------------------------\n",
      "Episode 5091 finished after 172 timesteps, total rewards 3.0, mean loss 0.015416950879477745\n",
      "Episode 5092 finished after 207 timesteps, total rewards 3.0, mean loss 0.017586291014050372\n",
      "Episode 5093 finished after 313 timesteps, total rewards 8.0, mean loss 0.016628164673051515\n",
      "Episode 5094 finished after 135 timesteps, total rewards 1.0, mean loss 0.016805178371982442\n",
      "Episode 5095 finished after 96 timesteps, total rewards 1.0, mean loss 0.01894743369727318\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 25.000000\n",
      "---------------------------------------\n",
      "Episode 5096 finished after 236 timesteps, total rewards 3.0, mean loss 0.0163738615613557\n",
      "Episode 5097 finished after 98 timesteps, total rewards 2.0, mean loss 0.0171969466158474\n",
      "Episode 5098 finished after 119 timesteps, total rewards 2.0, mean loss 0.015666817682756094\n",
      "Episode 5099 finished after 177 timesteps, total rewards 6.0, mean loss 0.015367500603693606\n",
      "Episode 5100 finished after 91 timesteps, total rewards 0.0, mean loss 0.015618512829110682\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 5101 finished after 161 timesteps, total rewards 5.0, mean loss 0.0168668612814653\n",
      "Episode 5102 finished after 224 timesteps, total rewards 5.0, mean loss 0.01706572685361607\n",
      "Episode 5103 finished after 193 timesteps, total rewards 2.0, mean loss 0.017007391882934875\n",
      "Episode 5104 finished after 181 timesteps, total rewards 3.0, mean loss 0.015062967069621985\n",
      "Episode 5105 finished after 166 timesteps, total rewards 1.0, mean loss 0.018538613427789456\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 5106 finished after 239 timesteps, total rewards 3.0, mean loss 0.015868111806785275\n",
      "Episode 5107 finished after 217 timesteps, total rewards 2.0, mean loss 0.01569503464699564\n",
      "Episode 5108 finished after 90 timesteps, total rewards 1.0, mean loss 0.018094348697923125\n",
      "Episode 5109 finished after 105 timesteps, total rewards 0.0, mean loss 0.016177166980646904\n",
      "Episode 5110 finished after 266 timesteps, total rewards 5.0, mean loss 0.01682129027969659\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 5111 finished after 224 timesteps, total rewards 2.0, mean loss 0.013760077054030262\n",
      "Episode 5112 finished after 144 timesteps, total rewards 1.0, mean loss 0.015365149054559879\n",
      "Episode 5113 finished after 156 timesteps, total rewards 3.0, mean loss 0.015592095004812552\n",
      "Episode 5114 finished after 184 timesteps, total rewards 5.0, mean loss 0.01749131740096187\n",
      "Episode 5115 finished after 162 timesteps, total rewards 0.0, mean loss 0.01740812115014795\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5116 finished after 136 timesteps, total rewards 6.0, mean loss 0.017847983894171193\n",
      "Episode 5117 finished after 103 timesteps, total rewards 2.0, mean loss 0.019326253450538927\n",
      "Episode 5118 finished after 197 timesteps, total rewards 3.0, mean loss 0.014813714392417049\n",
      "Episode 5119 finished after 263 timesteps, total rewards 6.0, mean loss 0.017156283787235674\n",
      "Episode 5120 finished after 162 timesteps, total rewards 0.0, mean loss 0.01526718268768839\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 5121 finished after 263 timesteps, total rewards 4.0, mean loss 0.016973127645507683\n",
      "Episode 5122 finished after 300 timesteps, total rewards 5.0, mean loss 0.016034793007808426\n",
      "Episode 5123 finished after 178 timesteps, total rewards 7.0, mean loss 0.014978541388945507\n",
      "Episode 5124 finished after 263 timesteps, total rewards 5.0, mean loss 0.015370779612345955\n",
      "Episode 5125 finished after 303 timesteps, total rewards 6.0, mean loss 0.01681733160685416\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 5126 finished after 85 timesteps, total rewards 2.0, mean loss 0.015282651902559926\n",
      "Episode 5127 finished after 102 timesteps, total rewards 0.0, mean loss 0.015967034333038563\n",
      "Episode 5128 finished after 311 timesteps, total rewards 5.0, mean loss 0.015885328443517303\n",
      "Episode 5129 finished after 272 timesteps, total rewards 9.0, mean loss 0.01734039695806774\n",
      "Episode 5130 finished after 189 timesteps, total rewards 0.0, mean loss 0.015574082621944842\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 5131 finished after 165 timesteps, total rewards 3.0, mean loss 0.018627495988921234\n",
      "Episode 5132 finished after 209 timesteps, total rewards 4.0, mean loss 0.015795059009726966\n",
      "Episode 5133 finished after 214 timesteps, total rewards 7.0, mean loss 0.017900164157509037\n",
      "Episode 5134 finished after 181 timesteps, total rewards 4.0, mean loss 0.015163583108558427\n",
      "Episode 5135 finished after 256 timesteps, total rewards 5.0, mean loss 0.015574558432490448\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 5136 finished after 233 timesteps, total rewards 3.0, mean loss 0.016827450258769253\n",
      "Episode 5137 finished after 294 timesteps, total rewards 2.0, mean loss 0.015760188790427863\n",
      "Episode 5138 finished after 235 timesteps, total rewards 7.0, mean loss 0.017263466156424677\n",
      "Episode 5139 finished after 127 timesteps, total rewards 1.0, mean loss 0.017319562866931825\n",
      "Episode 5140 finished after 176 timesteps, total rewards 5.0, mean loss 0.017051792819984257\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 5141 finished after 245 timesteps, total rewards 4.0, mean loss 0.01719539272788988\n",
      "Episode 5142 finished after 243 timesteps, total rewards 2.0, mean loss 0.01788189836563121\n",
      "Episode 5143 finished after 98 timesteps, total rewards 1.0, mean loss 0.015796441294975122\n",
      "Episode 5144 finished after 264 timesteps, total rewards 2.0, mean loss 0.016222285341372655\n",
      "Episode 5145 finished after 317 timesteps, total rewards 5.0, mean loss 0.015469282268390812\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 5146 finished after 114 timesteps, total rewards 1.0, mean loss 0.017515929767110368\n",
      "Episode 5147 finished after 178 timesteps, total rewards 2.0, mean loss 0.015803701674638924\n",
      "Episode 5148 finished after 203 timesteps, total rewards 2.0, mean loss 0.015371145119688708\n",
      "Episode 5149 finished after 86 timesteps, total rewards 1.0, mean loss 0.01877120785891663\n",
      "Episode 5150 finished after 134 timesteps, total rewards 1.0, mean loss 0.014586648377311875\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 5151 finished after 171 timesteps, total rewards 2.0, mean loss 0.017687259355630747\n",
      "Episode 5152 finished after 165 timesteps, total rewards 5.0, mean loss 0.015619174643617235\n",
      "Episode 5153 finished after 249 timesteps, total rewards 5.0, mean loss 0.017363400072285748\n",
      "Episode 5154 finished after 166 timesteps, total rewards 3.0, mean loss 0.015337404938055629\n",
      "Episode 5155 finished after 284 timesteps, total rewards 7.0, mean loss 0.01656923878611669\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 5156 finished after 314 timesteps, total rewards 6.0, mean loss 0.01736867605192456\n",
      "Episode 5157 finished after 237 timesteps, total rewards 5.0, mean loss 0.017000037385936716\n",
      "Episode 5158 finished after 179 timesteps, total rewards 5.0, mean loss 0.018721270654610666\n",
      "Episode 5159 finished after 225 timesteps, total rewards 10.0, mean loss 0.014899556310847401\n",
      "Episode 5160 finished after 179 timesteps, total rewards 7.0, mean loss 0.01735221330003615\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 5161 finished after 234 timesteps, total rewards 4.0, mean loss 0.015608404226736445\n",
      "Episode 5162 finished after 126 timesteps, total rewards 0.0, mean loss 0.015430077143560445\n",
      "Episode 5163 finished after 95 timesteps, total rewards 1.0, mean loss 0.011954519039902248\n",
      "Episode 5164 finished after 173 timesteps, total rewards 3.0, mean loss 0.014497779095760745\n",
      "Episode 5165 finished after 175 timesteps, total rewards 4.0, mean loss 0.017225154602367964\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 5166 finished after 250 timesteps, total rewards 2.0, mean loss 0.017483070800080894\n",
      "Episode 5167 finished after 138 timesteps, total rewards 1.0, mean loss 0.015185716447244951\n",
      "Episode 5168 finished after 149 timesteps, total rewards 3.0, mean loss 0.016712937354822738\n",
      "Episode 5169 finished after 107 timesteps, total rewards 1.0, mean loss 0.01610887887295788\n",
      "Episode 5170 finished after 164 timesteps, total rewards 2.0, mean loss 0.013920399906942847\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 5171 finished after 86 timesteps, total rewards 1.0, mean loss 0.014698838058393449\n",
      "Episode 5172 finished after 93 timesteps, total rewards 2.0, mean loss 0.014575358637939058\n",
      "Episode 5173 finished after 178 timesteps, total rewards 4.0, mean loss 0.014404173861937911\n",
      "Episode 5174 finished after 175 timesteps, total rewards 2.0, mean loss 0.015833754896053247\n",
      "Episode 5175 finished after 111 timesteps, total rewards 4.0, mean loss 0.01698604687998021\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 101.666667\n",
      "---------------------------------------\n",
      "Episode 5176 finished after 225 timesteps, total rewards 1.0, mean loss 0.016443487010482285\n",
      "Episode 5177 finished after 305 timesteps, total rewards 8.0, mean loss 0.015650841248313303\n",
      "Episode 5178 finished after 123 timesteps, total rewards 1.0, mean loss 0.016983886808724848\n",
      "Episode 5179 finished after 93 timesteps, total rewards 1.0, mean loss 0.015656380809002345\n",
      "Episode 5180 finished after 214 timesteps, total rewards 8.0, mean loss 0.015796804205497082\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 5181 finished after 164 timesteps, total rewards 2.0, mean loss 0.015486189266439618\n",
      "Episode 5182 finished after 163 timesteps, total rewards 3.0, mean loss 0.015443008193072\n",
      "Episode 5183 finished after 273 timesteps, total rewards 5.0, mean loss 0.014998726240587812\n",
      "Episode 5184 finished after 128 timesteps, total rewards 1.0, mean loss 0.016469337280796026\n",
      "Episode 5185 finished after 182 timesteps, total rewards 4.0, mean loss 0.01572984933679166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 60.000000\n",
      "---------------------------------------\n",
      "Episode 5186 finished after 169 timesteps, total rewards 2.0, mean loss 0.01519174576522066\n",
      "Episode 5187 finished after 137 timesteps, total rewards 1.0, mean loss 0.014751079590841584\n",
      "Episode 5188 finished after 279 timesteps, total rewards 6.0, mean loss 0.015946180827992443\n",
      "Episode 5189 finished after 218 timesteps, total rewards 6.0, mean loss 0.015392307437734183\n",
      "Episode 5190 finished after 177 timesteps, total rewards 1.0, mean loss 0.018216950359812144\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 5191 finished after 104 timesteps, total rewards 7.0, mean loss 0.012589319391946237\n",
      "Episode 5192 finished after 180 timesteps, total rewards 4.0, mean loss 0.013191891063211694\n",
      "Episode 5193 finished after 132 timesteps, total rewards 2.0, mean loss 0.014814906314396385\n",
      "Episode 5194 finished after 183 timesteps, total rewards 8.0, mean loss 0.01567409159564744\n",
      "Episode 5195 finished after 124 timesteps, total rewards 3.0, mean loss 0.015986729402004952\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 5196 finished after 212 timesteps, total rewards 1.0, mean loss 0.01504184129836811\n",
      "Episode 5197 finished after 165 timesteps, total rewards 6.0, mean loss 0.015790882367979396\n",
      "Episode 5198 finished after 216 timesteps, total rewards 2.0, mean loss 0.01568214233783591\n",
      "Episode 5199 finished after 201 timesteps, total rewards 4.0, mean loss 0.016650040893449417\n",
      "Episode 5200 finished after 199 timesteps, total rewards 2.0, mean loss 0.018656619657158625\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 75.000000\n",
      "---------------------------------------\n",
      "Episode 5201 finished after 145 timesteps, total rewards 5.0, mean loss 0.016018406210210303\n",
      "Episode 5202 finished after 139 timesteps, total rewards 4.0, mean loss 0.012766060551708872\n",
      "Episode 5203 finished after 126 timesteps, total rewards 3.0, mean loss 0.016509778882628157\n",
      "Episode 5204 finished after 295 timesteps, total rewards 3.0, mean loss 0.015336037842335842\n",
      "Episode 5205 finished after 147 timesteps, total rewards 2.0, mean loss 0.013940083940012926\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 5206 finished after 203 timesteps, total rewards 6.0, mean loss 0.01649247846690011\n",
      "Episode 5207 finished after 154 timesteps, total rewards 2.0, mean loss 0.016243560812677946\n",
      "Episode 5208 finished after 91 timesteps, total rewards 1.0, mean loss 0.015187880413217858\n",
      "Episode 5209 finished after 182 timesteps, total rewards 3.0, mean loss 0.014635936144718921\n",
      "Episode 5210 finished after 169 timesteps, total rewards 0.0, mean loss 0.015562595051476323\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 5211 finished after 247 timesteps, total rewards 2.0, mean loss 0.014568002073957701\n",
      "Episode 5212 finished after 403 timesteps, total rewards 10.0, mean loss 0.017042346546257716\n",
      "Episode 5213 finished after 228 timesteps, total rewards 0.0, mean loss 0.015010689953015301\n",
      "Episode 5214 finished after 103 timesteps, total rewards 3.0, mean loss 0.016039029012425144\n",
      "Episode 5215 finished after 152 timesteps, total rewards 1.0, mean loss 0.015074248693724113\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 143.333333\n",
      "---------------------------------------\n",
      "Episode 5216 finished after 263 timesteps, total rewards 11.0, mean loss 0.01573404164173899\n",
      "Episode 5217 finished after 177 timesteps, total rewards 3.0, mean loss 0.01395295433421214\n",
      "Episode 5218 finished after 138 timesteps, total rewards 0.0, mean loss 0.01579272283150045\n",
      "Episode 5219 finished after 207 timesteps, total rewards 4.0, mean loss 0.017479077052405995\n",
      "Episode 5220 finished after 191 timesteps, total rewards 6.0, mean loss 0.014443144000486437\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 5221 finished after 223 timesteps, total rewards 7.0, mean loss 0.015070437425496934\n",
      "Episode 5222 finished after 206 timesteps, total rewards 5.0, mean loss 0.016799643041240504\n",
      "Episode 5223 finished after 229 timesteps, total rewards 6.0, mean loss 0.014604704682288212\n",
      "Episode 5224 finished after 209 timesteps, total rewards 4.0, mean loss 0.015871962072309908\n",
      "Episode 5225 finished after 127 timesteps, total rewards 4.0, mean loss 0.015765888341445856\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 5226 finished after 226 timesteps, total rewards 7.0, mean loss 0.017697450449377036\n",
      "Episode 5227 finished after 262 timesteps, total rewards 7.0, mean loss 0.015099796826439334\n",
      "Episode 5228 finished after 133 timesteps, total rewards 2.0, mean loss 0.018296399699958197\n",
      "Episode 5229 finished after 168 timesteps, total rewards 1.0, mean loss 0.015230214663980795\n",
      "Episode 5230 finished after 97 timesteps, total rewards 2.0, mean loss 0.013924617694748431\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 5231 finished after 115 timesteps, total rewards 6.0, mean loss 0.019657967329713637\n",
      "Episode 5232 finished after 198 timesteps, total rewards 5.0, mean loss 0.017070755841107003\n",
      "Episode 5233 finished after 107 timesteps, total rewards 2.0, mean loss 0.015233636748863855\n",
      "Episode 5234 finished after 123 timesteps, total rewards 3.0, mean loss 0.015522999480760438\n",
      "Episode 5235 finished after 158 timesteps, total rewards 1.0, mean loss 0.016399526995123377\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 5236 finished after 231 timesteps, total rewards 10.0, mean loss 0.015958187061470824\n",
      "Episode 5237 finished after 156 timesteps, total rewards 2.0, mean loss 0.017289916744443756\n",
      "Episode 5238 finished after 212 timesteps, total rewards 7.0, mean loss 0.014870793953480831\n",
      "Episode 5239 finished after 178 timesteps, total rewards 5.0, mean loss 0.01499319897723918\n",
      "Episode 5240 finished after 177 timesteps, total rewards 2.0, mean loss 0.015534381030948233\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.000000\n",
      "---------------------------------------\n",
      "Episode 5241 finished after 182 timesteps, total rewards 2.0, mean loss 0.018661784875503445\n",
      "Episode 5242 finished after 195 timesteps, total rewards 4.0, mean loss 0.015490180284429628\n",
      "Episode 5243 finished after 227 timesteps, total rewards 0.0, mean loss 0.015404082992676203\n",
      "Episode 5244 finished after 101 timesteps, total rewards 1.0, mean loss 0.016335049413587197\n",
      "Episode 5245 finished after 173 timesteps, total rewards 3.0, mean loss 0.015015146110309126\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 5246 finished after 177 timesteps, total rewards 2.0, mean loss 0.01426244830241753\n",
      "Episode 5247 finished after 92 timesteps, total rewards 2.0, mean loss 0.014354153497033227\n",
      "Episode 5248 finished after 126 timesteps, total rewards 1.0, mean loss 0.015097526127531652\n",
      "Episode 5249 finished after 271 timesteps, total rewards 9.0, mean loss 0.015584277495143873\n",
      "Episode 5250 finished after 254 timesteps, total rewards 2.0, mean loss 0.015645940174847314\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 5251 finished after 294 timesteps, total rewards 3.0, mean loss 0.015702631238208083\n",
      "Episode 5252 finished after 221 timesteps, total rewards 6.0, mean loss 0.016559086008865503\n",
      "Episode 5253 finished after 98 timesteps, total rewards 4.0, mean loss 0.01777654402290604\n",
      "Episode 5254 finished after 158 timesteps, total rewards 2.0, mean loss 0.014314740109316345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5255 finished after 96 timesteps, total rewards 0.0, mean loss 0.015663362661143765\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 5256 finished after 256 timesteps, total rewards 4.0, mean loss 0.01596427095000763\n",
      "Episode 5257 finished after 197 timesteps, total rewards 6.0, mean loss 0.016273998858662427\n",
      "Episode 5258 finished after 142 timesteps, total rewards 1.0, mean loss 0.016831763252668396\n",
      "Episode 5259 finished after 136 timesteps, total rewards 5.0, mean loss 0.017681070003377767\n",
      "Episode 5260 finished after 179 timesteps, total rewards 4.0, mean loss 0.01712494684440505\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 5261 finished after 150 timesteps, total rewards 1.0, mean loss 0.01645876087869207\n",
      "Episode 5262 finished after 229 timesteps, total rewards 6.0, mean loss 0.016306629574219147\n",
      "Episode 5263 finished after 173 timesteps, total rewards 2.0, mean loss 0.01494885399679228\n",
      "Episode 5264 finished after 264 timesteps, total rewards 8.0, mean loss 0.013480654758027275\n",
      "Episode 5265 finished after 163 timesteps, total rewards 1.0, mean loss 0.017067831610986517\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 128.333333\n",
      "---------------------------------------\n",
      "Episode 5266 finished after 304 timesteps, total rewards 6.0, mean loss 0.016949077092392958\n",
      "Episode 5267 finished after 88 timesteps, total rewards 2.0, mean loss 0.015235178918704729\n",
      "Episode 5268 finished after 175 timesteps, total rewards 2.0, mean loss 0.013974788121080824\n",
      "Episode 5269 finished after 167 timesteps, total rewards 6.0, mean loss 0.014862151590627646\n",
      "Episode 5270 finished after 217 timesteps, total rewards 2.0, mean loss 0.015980393436419764\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 35.000000\n",
      "---------------------------------------\n",
      "Episode 5271 finished after 271 timesteps, total rewards 7.0, mean loss 0.01551940187646894\n",
      "Episode 5272 finished after 158 timesteps, total rewards 6.0, mean loss 0.01622639973485795\n",
      "Episode 5273 finished after 180 timesteps, total rewards 3.0, mean loss 0.016920428958514498\n",
      "Episode 5274 finished after 176 timesteps, total rewards 1.0, mean loss 0.015536061342572793\n",
      "Episode 5275 finished after 223 timesteps, total rewards 4.0, mean loss 0.014951120765520586\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 5276 finished after 93 timesteps, total rewards 1.0, mean loss 0.013865554908050163\n",
      "Episode 5277 finished after 253 timesteps, total rewards 1.0, mean loss 0.01759918176996054\n",
      "Episode 5278 finished after 263 timesteps, total rewards 4.0, mean loss 0.01731445460877746\n",
      "Episode 5279 finished after 245 timesteps, total rewards 8.0, mean loss 0.016533041502140005\n",
      "Episode 5280 finished after 88 timesteps, total rewards 2.0, mean loss 0.013083997227526694\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 20.000000\n",
      "---------------------------------------\n",
      "Episode 5281 finished after 164 timesteps, total rewards 6.0, mean loss 0.015069080053164237\n",
      "Episode 5282 finished after 239 timesteps, total rewards 2.0, mean loss 0.01593994083288262\n",
      "Episode 5283 finished after 89 timesteps, total rewards 1.0, mean loss 0.015326878069010511\n",
      "Episode 5284 finished after 137 timesteps, total rewards 6.0, mean loss 0.016862933399889916\n",
      "Episode 5285 finished after 185 timesteps, total rewards 4.0, mean loss 0.014778685211078137\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 73.333333\n",
      "---------------------------------------\n",
      "Episode 5286 finished after 161 timesteps, total rewards 3.0, mean loss 0.014920984186602305\n",
      "Episode 5287 finished after 184 timesteps, total rewards 6.0, mean loss 0.014831593060238369\n",
      "Episode 5288 finished after 134 timesteps, total rewards 4.0, mean loss 0.015863915729864654\n",
      "Episode 5289 finished after 106 timesteps, total rewards 1.0, mean loss 0.01639620207342372\n",
      "Episode 5290 finished after 273 timesteps, total rewards 7.0, mean loss 0.015847922673776417\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 106.666667\n",
      "---------------------------------------\n",
      "Episode 5291 finished after 308 timesteps, total rewards 3.0, mean loss 0.015679012713409574\n",
      "Episode 5292 finished after 113 timesteps, total rewards 4.0, mean loss 0.016776155179729108\n",
      "Episode 5293 finished after 157 timesteps, total rewards 0.0, mean loss 0.014668313623999191\n",
      "Episode 5294 finished after 254 timesteps, total rewards 3.0, mean loss 0.015610263436321374\n",
      "Episode 5295 finished after 168 timesteps, total rewards 4.0, mean loss 0.015487694555693972\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 5296 finished after 127 timesteps, total rewards 0.0, mean loss 0.016036615177153015\n",
      "Episode 5297 finished after 133 timesteps, total rewards 1.0, mean loss 0.015749038309138968\n",
      "Episode 5298 finished after 206 timesteps, total rewards 3.0, mean loss 0.016292336174214567\n",
      "Episode 5299 finished after 161 timesteps, total rewards 1.0, mean loss 0.016666026245108486\n",
      "Episode 5300 finished after 194 timesteps, total rewards 3.0, mean loss 0.015575242746074098\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 5301 finished after 199 timesteps, total rewards 7.0, mean loss 0.016704900241714327\n",
      "Episode 5302 finished after 150 timesteps, total rewards 2.0, mean loss 0.012799488677022358\n",
      "Episode 5303 finished after 204 timesteps, total rewards 4.0, mean loss 0.014767384060713299\n",
      "Episode 5304 finished after 143 timesteps, total rewards 3.0, mean loss 0.017617694944000013\n",
      "Episode 5305 finished after 91 timesteps, total rewards 0.0, mean loss 0.014112417427515919\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 5306 finished after 170 timesteps, total rewards 6.0, mean loss 0.013852950060904465\n",
      "Episode 5307 finished after 295 timesteps, total rewards 7.0, mean loss 0.016115324034245086\n",
      "Episode 5308 finished after 231 timesteps, total rewards 3.0, mean loss 0.016974994532755895\n",
      "Episode 5309 finished after 123 timesteps, total rewards 3.0, mean loss 0.013576932361243459\n",
      "Episode 5310 finished after 131 timesteps, total rewards 5.0, mean loss 0.01735571783235055\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 61.666667\n",
      "---------------------------------------\n",
      "Episode 5311 finished after 166 timesteps, total rewards 1.0, mean loss 0.016987258393783122\n",
      "Episode 5312 finished after 237 timesteps, total rewards 4.0, mean loss 0.01652816498999873\n",
      "Episode 5313 finished after 247 timesteps, total rewards 3.0, mean loss 0.01568373905585302\n",
      "Episode 5314 finished after 170 timesteps, total rewards 1.0, mean loss 0.016595003093757173\n",
      "Episode 5315 finished after 162 timesteps, total rewards 4.0, mean loss 0.016097636097168296\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 15.000000\n",
      "---------------------------------------\n",
      "Episode 5316 finished after 203 timesteps, total rewards 5.0, mean loss 0.01637961925924962\n",
      "Episode 5317 finished after 291 timesteps, total rewards 5.0, mean loss 0.016920966394261626\n",
      "Episode 5318 finished after 193 timesteps, total rewards 5.0, mean loss 0.016614659740034624\n",
      "Episode 5319 finished after 269 timesteps, total rewards 4.0, mean loss 0.015617739327847404\n",
      "Episode 5320 finished after 119 timesteps, total rewards 1.0, mean loss 0.015334901689481335\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 5321 finished after 184 timesteps, total rewards 4.0, mean loss 0.017358535305981564\n",
      "Episode 5322 finished after 128 timesteps, total rewards 5.0, mean loss 0.0160378094369662\n",
      "Episode 5323 finished after 233 timesteps, total rewards 6.0, mean loss 0.015661229759198914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5324 finished after 144 timesteps, total rewards 3.0, mean loss 0.018601993276711762\n",
      "Episode 5325 finished after 108 timesteps, total rewards 2.0, mean loss 0.013575133312433199\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 5326 finished after 173 timesteps, total rewards 3.0, mean loss 0.016733919740035443\n",
      "Episode 5327 finished after 275 timesteps, total rewards 5.0, mean loss 0.014989761852405288\n",
      "Episode 5328 finished after 268 timesteps, total rewards 3.0, mean loss 0.01548552637443697\n",
      "Episode 5329 finished after 247 timesteps, total rewards 3.0, mean loss 0.01505463927284803\n",
      "Episode 5330 finished after 216 timesteps, total rewards 5.0, mean loss 0.015393659256881586\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 80.000000\n",
      "---------------------------------------\n",
      "Episode 5331 finished after 201 timesteps, total rewards 2.0, mean loss 0.016551752299051824\n",
      "Episode 5332 finished after 160 timesteps, total rewards 3.0, mean loss 0.014826384551997761\n",
      "Episode 5333 finished after 348 timesteps, total rewards 3.0, mean loss 0.01533127492012713\n",
      "Episode 5334 finished after 249 timesteps, total rewards 4.0, mean loss 0.017118742674529314\n",
      "Episode 5335 finished after 188 timesteps, total rewards 5.0, mean loss 0.01617830872109675\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 5336 finished after 128 timesteps, total rewards 1.0, mean loss 0.015158927226366359\n",
      "Episode 5337 finished after 232 timesteps, total rewards 3.0, mean loss 0.016309363139263386\n",
      "Episode 5338 finished after 240 timesteps, total rewards 3.0, mean loss 0.016232325055655868\n",
      "Episode 5339 finished after 205 timesteps, total rewards 2.0, mean loss 0.018038737907914854\n",
      "Episode 5340 finished after 151 timesteps, total rewards 1.0, mean loss 0.015538357875143356\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 5341 finished after 177 timesteps, total rewards 1.0, mean loss 0.016315231498299262\n",
      "Episode 5342 finished after 212 timesteps, total rewards 3.0, mean loss 0.016386606485609245\n",
      "Episode 5343 finished after 269 timesteps, total rewards 5.0, mean loss 0.014815425390349943\n",
      "Episode 5344 finished after 381 timesteps, total rewards 5.0, mean loss 0.015872838189507642\n",
      "Episode 5345 finished after 172 timesteps, total rewards 2.0, mean loss 0.016769784273915424\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 5346 finished after 128 timesteps, total rewards 3.0, mean loss 0.01449221880920959\n",
      "Episode 5347 finished after 273 timesteps, total rewards 4.0, mean loss 0.015987402554806115\n",
      "Episode 5348 finished after 272 timesteps, total rewards 6.0, mean loss 0.016112663105184978\n",
      "Episode 5349 finished after 282 timesteps, total rewards 3.0, mean loss 0.01566660359950987\n",
      "Episode 5350 finished after 167 timesteps, total rewards 3.0, mean loss 0.01662304065285061\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 5351 finished after 241 timesteps, total rewards 9.0, mean loss 0.01581273871595697\n",
      "Episode 5352 finished after 173 timesteps, total rewards 3.0, mean loss 0.014638654839898853\n",
      "Episode 5353 finished after 167 timesteps, total rewards 9.0, mean loss 0.016181291915437285\n",
      "Episode 5354 finished after 208 timesteps, total rewards 4.0, mean loss 0.01562019344307303\n",
      "Episode 5355 finished after 185 timesteps, total rewards 5.0, mean loss 0.015542706799366184\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 51.666667\n",
      "---------------------------------------\n",
      "Episode 5356 finished after 131 timesteps, total rewards 1.0, mean loss 0.017480859602268303\n",
      "Episode 5357 finished after 190 timesteps, total rewards 6.0, mean loss 0.014350754556883323\n",
      "Episode 5358 finished after 127 timesteps, total rewards 1.0, mean loss 0.016014518307917936\n",
      "Episode 5359 finished after 171 timesteps, total rewards 4.0, mean loss 0.015609048970032766\n",
      "Episode 5360 finished after 188 timesteps, total rewards 3.0, mean loss 0.014713387183988704\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 41.666667\n",
      "---------------------------------------\n",
      "Episode 5361 finished after 174 timesteps, total rewards 2.0, mean loss 0.01647795061848458\n",
      "Episode 5362 finished after 255 timesteps, total rewards 6.0, mean loss 0.014755896036056619\n",
      "Episode 5363 finished after 243 timesteps, total rewards 2.0, mean loss 0.01461300968696505\n",
      "Episode 5364 finished after 233 timesteps, total rewards 3.0, mean loss 0.013571738875511026\n",
      "Episode 5365 finished after 116 timesteps, total rewards 2.0, mean loss 0.015088627036612737\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 5366 finished after 200 timesteps, total rewards 4.0, mean loss 0.015870730289025234\n",
      "Episode 5367 finished after 178 timesteps, total rewards 2.0, mean loss 0.015924508298166475\n",
      "Episode 5368 finished after 122 timesteps, total rewards 0.0, mean loss 0.018039376306019296\n",
      "Episode 5369 finished after 173 timesteps, total rewards 5.0, mean loss 0.014649373211364957\n",
      "Episode 5370 finished after 177 timesteps, total rewards 2.0, mean loss 0.01552724911854997\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 5371 finished after 158 timesteps, total rewards 1.0, mean loss 0.015741533614530026\n",
      "Episode 5372 finished after 326 timesteps, total rewards 10.0, mean loss 0.015312341937118215\n",
      "Episode 5373 finished after 260 timesteps, total rewards 6.0, mean loss 0.016121343984447707\n",
      "Episode 5374 finished after 196 timesteps, total rewards 7.0, mean loss 0.013941750306949705\n",
      "Episode 5375 finished after 161 timesteps, total rewards 3.0, mean loss 0.01772066345438361\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.666667\n",
      "---------------------------------------\n",
      "Episode 5376 finished after 149 timesteps, total rewards 3.0, mean loss 0.014372159548778862\n",
      "Episode 5377 finished after 313 timesteps, total rewards 8.0, mean loss 0.01624444710684363\n",
      "Episode 5378 finished after 111 timesteps, total rewards 1.0, mean loss 0.015522537161470265\n",
      "Episode 5379 finished after 100 timesteps, total rewards 2.0, mean loss 0.015581682769116013\n",
      "Episode 5380 finished after 273 timesteps, total rewards 1.0, mean loss 0.01625294980956344\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 5381 finished after 182 timesteps, total rewards 3.0, mean loss 0.01735561141350235\n",
      "Episode 5382 finished after 88 timesteps, total rewards 1.0, mean loss 0.013679529781008816\n",
      "Episode 5383 finished after 135 timesteps, total rewards 2.0, mean loss 0.01632482004696848\n",
      "Episode 5384 finished after 127 timesteps, total rewards 5.0, mean loss 0.016672477454284396\n",
      "Episode 5385 finished after 228 timesteps, total rewards 3.0, mean loss 0.016819100853529546\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 5386 finished after 155 timesteps, total rewards 3.0, mean loss 0.015524359903628788\n",
      "Episode 5387 finished after 219 timesteps, total rewards 5.0, mean loss 0.015770798487393142\n",
      "Episode 5388 finished after 236 timesteps, total rewards 4.0, mean loss 0.016176007820739236\n",
      "Episode 5389 finished after 236 timesteps, total rewards 6.0, mean loss 0.016573094947350417\n",
      "Episode 5390 finished after 301 timesteps, total rewards 6.0, mean loss 0.015480147769049652\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 28.333333\n",
      "---------------------------------------\n",
      "Episode 5391 finished after 169 timesteps, total rewards 4.0, mean loss 0.016004291120585958\n",
      "Episode 5392 finished after 198 timesteps, total rewards 4.0, mean loss 0.015916971923110798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5393 finished after 214 timesteps, total rewards 3.0, mean loss 0.013590439877725232\n",
      "Episode 5394 finished after 184 timesteps, total rewards 5.0, mean loss 0.015641181285584182\n",
      "Episode 5395 finished after 169 timesteps, total rewards 5.0, mean loss 0.015752205244243454\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 5396 finished after 227 timesteps, total rewards 4.0, mean loss 0.01559083657503325\n",
      "Episode 5397 finished after 200 timesteps, total rewards 1.0, mean loss 0.01672826225287281\n",
      "Episode 5398 finished after 263 timesteps, total rewards 5.0, mean loss 0.015920305104055104\n",
      "Episode 5399 finished after 162 timesteps, total rewards 1.0, mean loss 0.013670794342437552\n",
      "Episode 5400 finished after 204 timesteps, total rewards 5.0, mean loss 0.0151546806548996\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 21.666667\n",
      "---------------------------------------\n",
      "Episode 5401 finished after 314 timesteps, total rewards 9.0, mean loss 0.016096017957045135\n",
      "Episode 5402 finished after 162 timesteps, total rewards 5.0, mean loss 0.01722686107552116\n",
      "Episode 5403 finished after 138 timesteps, total rewards 2.0, mean loss 0.015502026412582052\n",
      "Episode 5404 finished after 134 timesteps, total rewards 5.0, mean loss 0.016993132779555425\n",
      "Episode 5405 finished after 150 timesteps, total rewards 6.0, mean loss 0.016564491493627428\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 98.333333\n",
      "---------------------------------------\n",
      "Episode 5406 finished after 205 timesteps, total rewards 2.0, mean loss 0.016265845319210756\n",
      "Episode 5407 finished after 159 timesteps, total rewards 6.0, mean loss 0.016063707391585007\n",
      "Episode 5408 finished after 294 timesteps, total rewards 1.0, mean loss 0.016461863161102697\n",
      "Episode 5409 finished after 122 timesteps, total rewards 1.0, mean loss 0.015595791422471892\n",
      "Episode 5410 finished after 246 timesteps, total rewards 3.0, mean loss 0.015584060935515577\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 53.333333\n",
      "---------------------------------------\n",
      "Episode 5411 finished after 140 timesteps, total rewards 3.0, mean loss 0.015580066115528876\n",
      "Episode 5412 finished after 200 timesteps, total rewards 1.0, mean loss 0.015712053597671912\n",
      "Episode 5413 finished after 156 timesteps, total rewards 3.0, mean loss 0.015772458292746868\n",
      "Episode 5414 finished after 135 timesteps, total rewards 2.0, mean loss 0.015514856100047904\n",
      "Episode 5415 finished after 142 timesteps, total rewards 2.0, mean loss 0.017983930159202764\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 26.666667\n",
      "---------------------------------------\n",
      "Episode 5416 finished after 189 timesteps, total rewards 5.0, mean loss 0.016166358790729963\n",
      "Episode 5417 finished after 244 timesteps, total rewards 5.0, mean loss 0.01488989176320248\n",
      "Episode 5418 finished after 255 timesteps, total rewards 8.0, mean loss 0.016061336530701204\n",
      "Episode 5419 finished after 283 timesteps, total rewards 3.0, mean loss 0.017257865153865185\n",
      "Episode 5420 finished after 244 timesteps, total rewards 3.0, mean loss 0.015774364480615182\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 5421 finished after 159 timesteps, total rewards 6.0, mean loss 0.0165409891318483\n",
      "Episode 5422 finished after 130 timesteps, total rewards 3.0, mean loss 0.016776968117874977\n",
      "Episode 5423 finished after 180 timesteps, total rewards 1.0, mean loss 0.017853998264763506\n",
      "Episode 5424 finished after 296 timesteps, total rewards 4.0, mean loss 0.015929075103485957\n",
      "Episode 5425 finished after 244 timesteps, total rewards 3.0, mean loss 0.017152866608173143\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 40.000000\n",
      "---------------------------------------\n",
      "Episode 5426 finished after 111 timesteps, total rewards 3.0, mean loss 0.015860454282127658\n",
      "Episode 5427 finished after 254 timesteps, total rewards 2.0, mean loss 0.016768950695817277\n",
      "Episode 5428 finished after 220 timesteps, total rewards 5.0, mean loss 0.01635612464636903\n",
      "Episode 5429 finished after 190 timesteps, total rewards 5.0, mean loss 0.015483989862504563\n",
      "Episode 5430 finished after 123 timesteps, total rewards 3.0, mean loss 0.017146969645670276\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 70.000000\n",
      "---------------------------------------\n",
      "Episode 5431 finished after 193 timesteps, total rewards 1.0, mean loss 0.016895264387130737\n",
      "Episode 5432 finished after 145 timesteps, total rewards 4.0, mean loss 0.01480966741219163\n",
      "Episode 5433 finished after 147 timesteps, total rewards 4.0, mean loss 0.014598707023843312\n",
      "Episode 5434 finished after 146 timesteps, total rewards 2.0, mean loss 0.014169713877753853\n",
      "Episode 5435 finished after 94 timesteps, total rewards 3.0, mean loss 0.015039976210670268\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 81.666667\n",
      "---------------------------------------\n",
      "Episode 5436 finished after 155 timesteps, total rewards 0.0, mean loss 0.01683292959275986\n",
      "Episode 5437 finished after 270 timesteps, total rewards 6.0, mean loss 0.016501631593780108\n",
      "Episode 5438 finished after 232 timesteps, total rewards 4.0, mean loss 0.014582727460194667\n",
      "Episode 5439 finished after 103 timesteps, total rewards 2.0, mean loss 0.01323298753343197\n",
      "Episode 5440 finished after 184 timesteps, total rewards 7.0, mean loss 0.016423898461811325\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 5441 finished after 89 timesteps, total rewards 1.0, mean loss 0.015446640176991567\n",
      "Episode 5442 finished after 245 timesteps, total rewards 3.0, mean loss 0.01606922122953954\n",
      "Episode 5443 finished after 253 timesteps, total rewards 3.0, mean loss 0.016395514498726123\n",
      "Episode 5444 finished after 298 timesteps, total rewards 3.0, mean loss 0.01598837760733378\n",
      "Episode 5445 finished after 165 timesteps, total rewards 2.0, mean loss 0.015181830061148063\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 5446 finished after 235 timesteps, total rewards 4.0, mean loss 0.017814490600350372\n",
      "Episode 5447 finished after 94 timesteps, total rewards 1.0, mean loss 0.015480993808484934\n",
      "Episode 5448 finished after 168 timesteps, total rewards 3.0, mean loss 0.014457015161280565\n",
      "Episode 5449 finished after 134 timesteps, total rewards 3.0, mean loss 0.015910945188101214\n",
      "Episode 5450 finished after 219 timesteps, total rewards 8.0, mean loss 0.016915809026107054\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 5451 finished after 293 timesteps, total rewards 6.0, mean loss 0.01513700191894667\n",
      "Episode 5452 finished after 157 timesteps, total rewards 1.0, mean loss 0.015546603076409097\n",
      "Episode 5453 finished after 262 timesteps, total rewards 2.0, mean loss 0.0159346193563058\n",
      "Episode 5454 finished after 124 timesteps, total rewards 4.0, mean loss 0.014992984079365288\n",
      "Episode 5455 finished after 269 timesteps, total rewards 5.0, mean loss 0.016143556000477875\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 48.333333\n",
      "---------------------------------------\n",
      "Episode 5456 finished after 242 timesteps, total rewards 5.0, mean loss 0.015606267450854552\n",
      "Episode 5457 finished after 249 timesteps, total rewards 5.0, mean loss 0.01574808520911987\n",
      "Episode 5458 finished after 277 timesteps, total rewards 5.0, mean loss 0.015480581446574321\n",
      "Episode 5459 finished after 111 timesteps, total rewards 3.0, mean loss 0.016517628160481516\n",
      "Episode 5460 finished after 112 timesteps, total rewards 1.0, mean loss 0.015349973151127674\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 43.333333\n",
      "---------------------------------------\n",
      "Episode 5461 finished after 289 timesteps, total rewards 1.0, mean loss 0.01571200443850669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5462 finished after 207 timesteps, total rewards 2.0, mean loss 0.016241121527644387\n",
      "Episode 5463 finished after 228 timesteps, total rewards 5.0, mean loss 0.016157143939237454\n",
      "Episode 5464 finished after 185 timesteps, total rewards 2.0, mean loss 0.016058458206621377\n",
      "Episode 5465 finished after 159 timesteps, total rewards 3.0, mean loss 0.016732419675520662\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 5466 finished after 183 timesteps, total rewards 1.0, mean loss 0.01719008046685835\n",
      "Episode 5467 finished after 343 timesteps, total rewards 5.0, mean loss 0.0154725940960511\n",
      "Episode 5468 finished after 114 timesteps, total rewards 5.0, mean loss 0.014562199339515677\n",
      "Episode 5469 finished after 209 timesteps, total rewards 3.0, mean loss 0.016051010549531883\n",
      "Episode 5470 finished after 163 timesteps, total rewards 0.0, mean loss 0.014995308465787147\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 68.333333\n",
      "---------------------------------------\n",
      "Episode 5471 finished after 337 timesteps, total rewards 4.0, mean loss 0.014781851922880404\n",
      "Episode 5472 finished after 225 timesteps, total rewards 3.0, mean loss 0.01570955337646107\n",
      "Episode 5473 finished after 156 timesteps, total rewards 2.0, mean loss 0.014980677180201149\n",
      "Episode 5474 finished after 144 timesteps, total rewards 1.0, mean loss 0.016082460552247033\n",
      "Episode 5475 finished after 130 timesteps, total rewards 0.0, mean loss 0.013619647576258732\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 46.666667\n",
      "---------------------------------------\n",
      "Episode 5476 finished after 256 timesteps, total rewards 5.0, mean loss 0.015858297797421983\n",
      "Episode 5477 finished after 208 timesteps, total rewards 8.0, mean loss 0.01602421778191526\n",
      "Episode 5478 finished after 232 timesteps, total rewards 4.0, mean loss 0.015313376229986998\n",
      "Episode 5479 finished after 122 timesteps, total rewards 4.0, mean loss 0.015326010698803747\n",
      "Episode 5480 finished after 240 timesteps, total rewards 7.0, mean loss 0.014600179511277626\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.333333\n",
      "---------------------------------------\n",
      "Episode 5481 finished after 248 timesteps, total rewards 2.0, mean loss 0.01590145484829742\n",
      "Episode 5482 finished after 129 timesteps, total rewards 4.0, mean loss 0.01482730752124285\n",
      "Episode 5483 finished after 181 timesteps, total rewards 1.0, mean loss 0.013989033921009433\n",
      "Episode 5484 finished after 163 timesteps, total rewards 1.0, mean loss 0.01497427797790594\n",
      "Episode 5485 finished after 222 timesteps, total rewards 3.0, mean loss 0.014812689582368376\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 55.000000\n",
      "---------------------------------------\n",
      "Episode 5486 finished after 125 timesteps, total rewards 3.0, mean loss 0.017299535317346453\n",
      "Episode 5487 finished after 167 timesteps, total rewards 6.0, mean loss 0.015105574232844\n",
      "Episode 5488 finished after 100 timesteps, total rewards 2.0, mean loss 0.016380004952661693\n",
      "Episode 5489 finished after 200 timesteps, total rewards 4.0, mean loss 0.016537255528382956\n",
      "Episode 5490 finished after 109 timesteps, total rewards 2.0, mean loss 0.01828394980178377\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 88.333333\n",
      "---------------------------------------\n",
      "Episode 5491 finished after 175 timesteps, total rewards 5.0, mean loss 0.01684146403734173\n",
      "Episode 5492 finished after 301 timesteps, total rewards 2.0, mean loss 0.016285906797213066\n",
      "Episode 5493 finished after 204 timesteps, total rewards 3.0, mean loss 0.016187668080651658\n",
      "Episode 5494 finished after 170 timesteps, total rewards 5.0, mean loss 0.015730516729629874\n",
      "Episode 5495 finished after 139 timesteps, total rewards 6.0, mean loss 0.01622255581678997\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 38.333333\n",
      "---------------------------------------\n",
      "Episode 5496 finished after 175 timesteps, total rewards 3.0, mean loss 0.01576665831929339\n",
      "Episode 5497 finished after 286 timesteps, total rewards 5.0, mean loss 0.01598362869117409\n",
      "Episode 5498 finished after 95 timesteps, total rewards 2.0, mean loss 0.016532889922688667\n",
      "Episode 5499 finished after 93 timesteps, total rewards 0.0, mean loss 0.01713501135267878\n",
      "Episode 5500 finished after 160 timesteps, total rewards 4.0, mean loss 0.018301534371857997\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 5501 finished after 179 timesteps, total rewards 4.0, mean loss 0.016499533375215265\n",
      "Episode 5502 finished after 131 timesteps, total rewards 2.0, mean loss 0.016629994875317765\n",
      "Episode 5503 finished after 164 timesteps, total rewards 1.0, mean loss 0.015896852747196467\n",
      "Episode 5504 finished after 92 timesteps, total rewards 2.0, mean loss 0.017259331306179418\n",
      "Episode 5505 finished after 245 timesteps, total rewards 5.0, mean loss 0.014699600577088339\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 50.000000\n",
      "---------------------------------------\n",
      "Episode 5506 finished after 158 timesteps, total rewards 3.0, mean loss 0.01672582789560002\n",
      "Episode 5507 finished after 121 timesteps, total rewards 1.0, mean loss 0.016814130901028054\n",
      "Episode 5508 finished after 160 timesteps, total rewards 4.0, mean loss 0.017548958233965094\n",
      "Episode 5509 finished after 161 timesteps, total rewards 4.0, mean loss 0.015817598929998945\n",
      "Episode 5510 finished after 178 timesteps, total rewards 3.0, mean loss 0.014613705852037568\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 36.666667\n",
      "---------------------------------------\n",
      "Episode 5511 finished after 167 timesteps, total rewards 4.0, mean loss 0.01665482591436116\n",
      "Episode 5512 finished after 218 timesteps, total rewards 2.0, mean loss 0.015020132806032486\n",
      "Episode 5513 finished after 165 timesteps, total rewards 3.0, mean loss 0.013860570168066206\n",
      "Episode 5514 finished after 148 timesteps, total rewards 1.0, mean loss 0.017578008639739472\n",
      "Episode 5515 finished after 156 timesteps, total rewards 4.0, mean loss 0.01641866473110918\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n",
      "Episode 5516 finished after 204 timesteps, total rewards 2.0, mean loss 0.0149607215985181\n",
      "Episode 5517 finished after 109 timesteps, total rewards 3.0, mean loss 0.01624157014110664\n",
      "Episode 5518 finished after 259 timesteps, total rewards 3.0, mean loss 0.016490426623805437\n",
      "Episode 5519 finished after 244 timesteps, total rewards 6.0, mean loss 0.01569800064841774\n",
      "Episode 5520 finished after 246 timesteps, total rewards 5.0, mean loss 0.015845353114115817\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 65.000000\n",
      "---------------------------------------\n",
      "Episode 5521 finished after 113 timesteps, total rewards 1.0, mean loss 0.014983490785270665\n",
      "Episode 5522 finished after 169 timesteps, total rewards 4.0, mean loss 0.01620438327246825\n",
      "Episode 5523 finished after 165 timesteps, total rewards 1.0, mean loss 0.016884943386398708\n",
      "Episode 5524 finished after 116 timesteps, total rewards 0.0, mean loss 0.014380975313291982\n",
      "Episode 5525 finished after 102 timesteps, total rewards 1.0, mean loss 0.013509147697785759\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 71.666667\n",
      "---------------------------------------\n",
      "Episode 5526 finished after 224 timesteps, total rewards 5.0, mean loss 0.01768321116001711\n",
      "Episode 5527 finished after 105 timesteps, total rewards 2.0, mean loss 0.014187607488461903\n",
      "Episode 5528 finished after 210 timesteps, total rewards 2.0, mean loss 0.01302066959296575\n",
      "Episode 5529 finished after 249 timesteps, total rewards 5.0, mean loss 0.01551658950323411\n",
      "Episode 5530 finished after 168 timesteps, total rewards 4.0, mean loss 0.016253954776662534\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 45.000000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5531 finished after 154 timesteps, total rewards 3.0, mean loss 0.015205993868906486\n",
      "Episode 5532 finished after 128 timesteps, total rewards 0.0, mean loss 0.014082483052334283\n",
      "Episode 5533 finished after 93 timesteps, total rewards 0.0, mean loss 0.014766043865732768\n",
      "Episode 5534 finished after 131 timesteps, total rewards 0.0, mean loss 0.0164965238948246\n",
      "Episode 5535 finished after 244 timesteps, total rewards 9.0, mean loss 0.013972411042396132\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 33.333333\n",
      "---------------------------------------\n",
      "Episode 5536 finished after 172 timesteps, total rewards 2.0, mean loss 0.014792572899835271\n",
      "Episode 5537 finished after 136 timesteps, total rewards 3.0, mean loss 0.017349234504816943\n",
      "Episode 5538 finished after 214 timesteps, total rewards 4.0, mean loss 0.01611085724472895\n",
      "Episode 5539 finished after 162 timesteps, total rewards 2.0, mean loss 0.016540751308145255\n",
      "Episode 5540 finished after 200 timesteps, total rewards 5.0, mean loss 0.016575120149645954\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 31.666667\n",
      "---------------------------------------\n",
      "Episode 5541 finished after 250 timesteps, total rewards 9.0, mean loss 0.014806444293819368\n",
      "Episode 5542 finished after 171 timesteps, total rewards 1.0, mean loss 0.015255523187908949\n",
      "Episode 5543 finished after 90 timesteps, total rewards 0.0, mean loss 0.01750167515128851\n",
      "Episode 5544 finished after 220 timesteps, total rewards 8.0, mean loss 0.014981042031749067\n",
      "Episode 5545 finished after 155 timesteps, total rewards 1.0, mean loss 0.015584661565240352\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 18.333333\n",
      "---------------------------------------\n",
      "Episode 5546 finished after 241 timesteps, total rewards 4.0, mean loss 0.015355628198283218\n",
      "Episode 5547 finished after 136 timesteps, total rewards 3.0, mean loss 0.01530246261630536\n",
      "Episode 5548 finished after 294 timesteps, total rewards 5.0, mean loss 0.017078261658134333\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'directory_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-48b2ddec8c6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0meval_Q_value_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_Q_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"./{directory_name}_pytorch_models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'directory_name' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "decay_step = 0\n",
    "state = None\n",
    "stacked_frame = None\n",
    "# 練 max_episode 個 episodes\n",
    "while True:\n",
    "    \n",
    "     # 練 max_episode 個 episodes\n",
    "    if isuseepisode and current_episode > max_episode:\n",
    "        break\n",
    "    # 練 max_timestep 個 timestep\n",
    "    if not isuseepisode and current_timestep > max_timestep:\n",
    "        break\n",
    "        \n",
    "    # 初始每次 episode 的參數\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    episode_loss = 0.0\n",
    "    loss_step = 0\n",
    "    frame = env.reset()\n",
    "    del stacked_frame\n",
    "    stacked_frame = deque([np.zeros((84, 84)) for i in range(4)],maxlen=4)\n",
    "    \n",
    "    # 前4張 action 都做 NOP\n",
    "    stacked_frame.append(preprocess(frame))\n",
    "    for _ in range(3):\n",
    "        frame, _, _, _ = env.step(0)\n",
    "        stacked_frame.append(preprocess(frame))\n",
    "    # 初始化 state\n",
    "    del state\n",
    "    state = np.array(stacked_frame)\n",
    "    \n",
    "    # 進入 episode\n",
    "    while True:\n",
    "        # episode 中的 timestep 更新\n",
    "        t += 1\n",
    "        \n",
    "        # epsilon\n",
    "        training_epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-decay_rate * decay_step)\n",
    "        # epsilon-greedy\n",
    "        if np.random.uniform() < training_epsilon: \n",
    "            action = np.random.randint(0, action_dim)# 隨機\n",
    "        else:\n",
    "            # stacked_frame -> deque(t1, t2, t3, t4)\n",
    "            action, _ = agent.select_action(state)# agent 挑\n",
    "            \n",
    "        # play with env frame skipping(一次刷新4次frames)\n",
    "        for _ in range(4):\n",
    "            next_frame, reward, done, _ = env.step(action)\n",
    "            stacked_frame.append(preprocess(next_frame))\n",
    "            if done: break\n",
    "        \n",
    "        next_state = np.array(stacked_frame)\n",
    "        \n",
    "        # reward 限制在 [-1,1]\n",
    "        reward = np.clip(reward,-1,1) if perform_clip else reward\n",
    "\n",
    "        # 存進 experience replay buffer\n",
    "        replaybuffer.add((state, next_state, action, reward, done))\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "        \n",
    "        # 足夠大的經驗就開始學習\n",
    "        if len(replaybuffer) >= memory_capacity:\n",
    "            \n",
    "            episode_loss += agent.train(replaybuffer, batch_size, discount, policy_freq)\n",
    "            loss_step += 1\n",
    "            # epsilon-greedy 的 epsilon 線性下降\n",
    "            decay_step += 1\n",
    "            # training_epsilon -= ((epsilon_start - epsilon_end))\n",
    "        \n",
    "        # episode 結束\n",
    "        if done:\n",
    "            if len(replaybuffer) >= memory_capacity:\n",
    "                episode_loss /= loss_step\n",
    "                total_loss.append(episode_loss)\n",
    "                print('Episode {} finished after {} timesteps, total rewards {}, mean loss {}'.format(current_episode+1, t, rewards, episode_loss))\n",
    "            else:\n",
    "                print(f'Accumulating Experience Play..../replaybuffer:{len(replaybuffer)}')\n",
    "            break\n",
    "        \n",
    "        # 進入下一個 state\n",
    "        del state\n",
    "        state = next_state\n",
    "            \n",
    "    # 每當一個 episode 結束，存檔 agent 的 weight\n",
    "    if save_models:\n",
    "        agent.save(filename, directory=f\"./{filename}_pytorch_models\")\n",
    "    \n",
    "    # 進入下一個 episode \n",
    "    if len(replaybuffer) >= memory_capacity:\n",
    "        current_episode += 1\n",
    "        current_timestep += t\n",
    "    \n",
    "    # 每經過 eval_freq 個 episodes 就做一次 evaluation\n",
    "    if len(replaybuffer) >= memory_capacity and current_episode % eval_freq == 0:\n",
    "        eval_reward, eval_Q_value = evaluate(agent, 3)\n",
    "        eval_reward_record.append(eval_reward)\n",
    "        eval_Q_value_record.append(eval_Q_value)\n",
    "tn = time.time()\n",
    "agent.save(filename, directory=f\"./{filename}_pytorch_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26d49bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(filename, directory=f\"./{filename}_pytorch_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3921d683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replaybuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "895b903e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000227"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "632eccd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5548"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85805f96",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "953d77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value(value, title, filename, directory, color='red', x='episode'):\n",
    "    total_episode = len(value)\n",
    "    x_1 = range(total_episode)\n",
    "    \n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x_1, value, c=f'tab:{color}', label=title)\n",
    "    \n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(title)\n",
    "    \n",
    "    plt.title(f'{title} through {x}s')\n",
    "    plt.legend()\n",
    "    plt.savefig(directory+'/'+filename+f'.{title}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81899c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_value(total_loss, 'loss', filename, f\"./{filename}_results\", 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f4bd3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_value(eval_reward_record,'reward', filename, f\"./{filename}_results\", x = 'every five episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62944a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_value(eval_Q_value_record,'Q value', filename, f\"./{filename}_results\", 'blue', 'every five episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3ba07",
   "metadata": {},
   "source": [
    "## Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1a49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7879ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'SpaceInvaders-v0'\n",
    "seed = 42\n",
    "lr = 0.00025\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5df36d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 742738649]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b17dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83e2125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim, 4, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fcd7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(filename, directory=f\"./{filename}_pytorch_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "571dfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_action = {\n",
    "    0:'NOP',\n",
    "    1:'fight',\n",
    "    2:'right',\n",
    "    3:'left',\n",
    "    4:'right and fight',\n",
    "    5:'left and fight'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7686aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "state = deque([np.zeros((84, 84)) for i in range(4)],maxlen=4)\n",
    "state.append(preprocess(obs))\n",
    "for _ in range(3):\n",
    "    obs, _, _, _ = env.step(0)\n",
    "    state.append(preprocess(obs))\n",
    "action = 0\n",
    "done = False\n",
    "t = 0\n",
    "r = 0\n",
    "while not done:\n",
    "    t += 1\n",
    "    \n",
    "    action, _ = agent.select_action(np.array(state))\n",
    "    #action = env.action_space.sample()\n",
    "    for _ in range(4):\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        state.append(preprocess(obs))\n",
    "        r += reward\n",
    "        show_state(obs,t, f'reward: {r} \\n action: {idx_to_action[action]}')\n",
    "        if done: break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead4e22",
   "metadata": {},
   "source": [
    "## Problem\n",
    "* 不管 state 為何 model 都預測一樣的 \n",
    "    - solution: reward 打錯字\n",
    "* replaybuffer 有一堆 0 action (left)\n",
    "    - solution: 調整 exploration\n",
    "* reward 一直停在 0, 270, 285 原地射擊\n",
    "    - solution: agent 落入 local maxima，調整超參數\n",
    "* frame skipping 問題(實作錯誤) \n",
    "    - 錯誤: state(t1, t2, t3, t4) -> next state(t2, t3, t4, t5)\n",
    "    - 正確: state(t1, t2, t3, t4) -> next state(t5, t6, t7, t8)\n",
    "* OOM 問題:\n",
    "    - solution: env playing時要把用完後的每個 timestep 的 state 和 stacked_frame 刪掉以減少 memory 負擔\n",
    "* 不管甚麼 state, agent 都輸出一樣的 Q value\n",
    "    - solution: 訓練次數太少，且落入 local maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddfed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
